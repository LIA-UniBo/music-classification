{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "4CspUKbHyK7A"
      },
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pmtu1rHfxusY"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "oZfijJ2kxusc"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "from src.dataset import *\n",
        "from src.train import *\n",
        "from src.utils import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "mFZtkDipxusg"
      },
      "outputs": [],
      "source": [
        "RES_DIR_PATH = \"res\"\n",
        "AUDIOS_DIR_PATH = os.path.join(RES_DIR_PATH, \"mp3_data\")\n",
        "MODELS_DIR_PATH = os.path.join(RES_DIR_PATH, \"models\")\n",
        "DATASETS_DIR_PATH = os.path.join(RES_DIR_PATH, \"datasets\")\n",
        "\n",
        "CSV_PATH = os.path.join(RES_DIR_PATH, \"samples.csv\")\n",
        "\n",
        "FEATURES_CONFIG = {\n",
        "    \"genre\": {\"top_n\": 3, \"samples\": 1000}\n",
        "}\n",
        "\n",
        "VALID_SIZE = 0.1\n",
        "TEST_SIZE = 0.1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "siGNvZU5xusi"
      },
      "source": [
        "## Backbones"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "oW-T64DCxusk"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V6G03uSuxusl"
      },
      "source": [
        "## Fine-tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XIqT4RUdxuso"
      },
      "source": [
        "Thanks to the HuggingFace library, it would be enough to"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hDoCTT27xusp"
      },
      "source": [
        "## Classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aEzI2ksqxusr"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yo8fX54ixust"
      },
      "source": [
        "## Multi-task"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eL1fVvwkxusv"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PxFCxU7Oxusw"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "ZOEp2fHKxusw"
      },
      "outputs": [],
      "source": [
        "TRAINING_CONFIG = {\n",
        "    \"epochs\": 20,\n",
        "    \"learning_rate\": 5e-5,\n",
        "    \"warmup\": 0.0,\n",
        "    \"train_batch_size\": 16,\n",
        "    \"eval_batch_size\": 32,\n",
        "    \"feature_encoder\": None,\n",
        "    \"freeze_encoder\": None,\n",
        "    \"classifier_layers\": None, \n",
        "    \"classifier_dropout\": None,\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "LXUDuhuuxusy"
      },
      "outputs": [],
      "source": [
        "# filtered_csv_path = get_csv_name(FEATURES_CONFIG, CSV_PATH)\n",
        "# encoder_ds = {}\n",
        "\n",
        "# for encoder in [\"wav2vec2\", \"whisper\"]:\n",
        "#     encoded_dataset_path = os.path.join(DATASETS_DIR_PATH, f\"ds-{encoder}-full-encoded\")\n",
        "\n",
        "#     df = pd.read_csv(filtered_csv_path)\n",
        "#     ds = datasets.load_from_disk(encoded_dataset_path)\n",
        "#     ds = add_audio_column(ds, audios_dir_path=AUDIOS_DIR_PATH, training_config={\"feature_encoder\": encoder})\n",
        "#     encoder_ds[encoder] = prepare_ds(ds, df, FEATURES_CONFIG, 0.2, fixed_mapping=None, save=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7MWfQIuVxus0"
      },
      "source": [
        "## Baseline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "wSFCEv3T4Q-m"
      },
      "outputs": [],
      "source": [
        "TRAINING_CONFIG[\"feature_encoder\"] = \"wav2vec2\"\n",
        "TRAINING_CONFIG[\"freeze_encoder\"] = True\n",
        "TRAINING_CONFIG[\"classifier_layers\"] = [256]\n",
        "TRAINING_CONFIG[\"classifier_dropout\"] = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140,
          "referenced_widgets": [
            "35622cbe24684a7ea5382f1f536bd9ea",
            "0fee81913e764382bfd0946d1661be8d",
            "c9e2fff6b66e434e9806d869417f4eff",
            "e3207490c4ba470f8e7cf61b97d84d54",
            "2febbdf5605149f494d9ac510883f262",
            "570e7fe98c364883a87014e220e5baf5",
            "5d46f1f40ae24380a6b6d3304d1d1d36",
            "4849127988f64e1e9558a88f220ad1de",
            "76877461d03e4018b8e5c5bc1721d2cb",
            "a70ff0866d564c96ae19d5d6aa91a727",
            "7df7757d71ea46d5a4616f431066aef1"
          ]
        },
        "id": "znoTtUqX4SAD",
        "outputId": "8a6b4048-1620-4231-d60c-51330e63d3cb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading /content/drive/MyDrive/Projects/music-classification/res/samples_genre3s1000.csv\n",
            "999 examples in DataFrame\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/transformers/configuration_utils.py:375: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "35622cbe24684a7ea5382f1f536bd9ea",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Casting the dataset:   0%|          | 0/1 [00:00<?, ?ba/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "filtered_csv_path = get_csv_name(FEATURES_CONFIG, CSV_PATH)\n",
        "encoded_dataset_path = os.path.join(DATASETS_DIR_PATH, f\"ds-{TRAINING_CONFIG['feature_encoder']}-full-encoded\")\n",
        "\n",
        "\n",
        "filtered_csv_path = get_csv_name(FEATURES_CONFIG, CSV_PATH)\n",
        "\n",
        "if os.path.exists(filtered_csv_path):\n",
        "    print(f\"Loading {filtered_csv_path}\")\n",
        "    df = pd.read_csv(filtered_csv_path)\n",
        "else:\n",
        "    df = pd.read_csv(CSV_PATH)\n",
        "    df = filter_df(\n",
        "        df, \n",
        "        remove_nones=False,\n",
        "        features_config=FEATURES_CONFIG, \n",
        "    )\n",
        "    df.to_csv(filtered_csv_path, index=False)\n",
        "\n",
        "print(f\"{len(df)} examples in DataFrame\")\n",
        "\n",
        "ds = datasets.load_from_disk(encoded_dataset_path)\n",
        "ds = add_audio_column(ds, audios_dir_path=AUDIOS_DIR_PATH, training_config={\"feature_encoder\": TRAINING_CONFIG['feature_encoder']})\n",
        "ds = prepare_ds(ds, df, FEATURES_CONFIG, 0.2, fixed_mapping=None, save=False)\n",
        "ds = ds.rename_column(\"label_genre\", \"label\")\n",
        "\n",
        "# ds = encoder_ds[TRAINING_CONFIG[\"feature_encoder\"]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "m-AHRUIK2KPL",
        "outputId": "34b6f338-882c-4ebb-8ce0-b4508b66d254"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at facebook/wav2vec2-base were not used when initializing Wav2Vec2ForSequenceClassification: ['project_q.weight', 'quantizer.weight_proj.bias', 'project_q.bias', 'quantizer.weight_proj.weight', 'quantizer.codevectors', 'project_hid.bias', 'project_hid.weight']\n",
            "- This IS expected if you are initializing Wav2Vec2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing Wav2Vec2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of Wav2Vec2ForSequenceClassification were not initialized from the model checkpoint at facebook/wav2vec2-base and are newly initialized: ['projector.weight', 'classifier.bias', 'projector.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "ERROR:wandb.jupyter:Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mastockman\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.13.10"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/music-classification/wandb/run-20230222_105059-pr5oib7p</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/astockman/music-classification-aii/runs/pr5oib7p' target=\"_blank\">baseline</a></strong> to <a href='https://wandb.ai/astockman/music-classification-aii' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/astockman/music-classification-aii' target=\"_blank\">https://wandb.ai/astockman/music-classification-aii</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/astockman/music-classification-aii/runs/pr5oib7p' target=\"_blank\">https://wandb.ai/astockman/music-classification-aii/runs/pr5oib7p</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The following columns in the training set don't have a corresponding argument in `Wav2Vec2ForSequenceClassification.forward` and have been ignored: id, duration. If id, duration are not expected by `Wav2Vec2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "***** Running training *****\n",
            "  Num examples = 608\n",
            "  Num Epochs = 20\n",
            "  Instantaneous batch size per device = 16\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 760\n",
            "  Number of trainable parameters = 94569347\n",
            "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='760' max='760' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [760/760 53:43, Epoch 20/20]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.046600</td>\n",
              "      <td>1.020314</td>\n",
              "      <td>0.549020</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.975800</td>\n",
              "      <td>0.917156</td>\n",
              "      <td>0.614379</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.911300</td>\n",
              "      <td>0.848616</td>\n",
              "      <td>0.614379</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.825900</td>\n",
              "      <td>0.842709</td>\n",
              "      <td>0.620915</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.705200</td>\n",
              "      <td>0.862440</td>\n",
              "      <td>0.653595</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.663200</td>\n",
              "      <td>0.781739</td>\n",
              "      <td>0.705882</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.511500</td>\n",
              "      <td>0.701397</td>\n",
              "      <td>0.732026</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.476700</td>\n",
              "      <td>0.804047</td>\n",
              "      <td>0.725490</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.396500</td>\n",
              "      <td>0.931263</td>\n",
              "      <td>0.673203</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.332300</td>\n",
              "      <td>0.993699</td>\n",
              "      <td>0.692810</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>0.278900</td>\n",
              "      <td>0.988242</td>\n",
              "      <td>0.705882</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>0.266800</td>\n",
              "      <td>1.043295</td>\n",
              "      <td>0.745098</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>0.246200</td>\n",
              "      <td>1.076963</td>\n",
              "      <td>0.745098</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>0.162100</td>\n",
              "      <td>1.228294</td>\n",
              "      <td>0.718954</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>0.094600</td>\n",
              "      <td>1.298543</td>\n",
              "      <td>0.725490</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>0.080400</td>\n",
              "      <td>1.266580</td>\n",
              "      <td>0.738562</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>0.063300</td>\n",
              "      <td>1.182788</td>\n",
              "      <td>0.764706</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>0.044400</td>\n",
              "      <td>1.260643</td>\n",
              "      <td>0.751634</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>0.035200</td>\n",
              "      <td>1.271021</td>\n",
              "      <td>0.745098</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>0.036100</td>\n",
              "      <td>1.247396</td>\n",
              "      <td>0.751634</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSequenceClassification.forward` and have been ignored: id, duration. If id, duration are not expected by `Wav2Vec2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 153\n",
            "  Batch size = 32\n",
            "Saving model checkpoint to out/checkpoint-38\n",
            "Configuration saved in out/checkpoint-38/config.json\n",
            "Model weights saved in out/checkpoint-38/pytorch_model.bin\n",
            "Feature extractor saved in out/checkpoint-38/preprocessor_config.json\n",
            "Deleting older checkpoint [out/checkpoint-418] due to args.save_total_limit\n",
            "The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSequenceClassification.forward` and have been ignored: id, duration. If id, duration are not expected by `Wav2Vec2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 153\n",
            "  Batch size = 32\n",
            "Saving model checkpoint to out/checkpoint-76\n",
            "Configuration saved in out/checkpoint-76/config.json\n",
            "Model weights saved in out/checkpoint-76/pytorch_model.bin\n",
            "Feature extractor saved in out/checkpoint-76/preprocessor_config.json\n",
            "Deleting older checkpoint [out/checkpoint-722] due to args.save_total_limit\n",
            "The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSequenceClassification.forward` and have been ignored: id, duration. If id, duration are not expected by `Wav2Vec2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 153\n",
            "  Batch size = 32\n",
            "Saving model checkpoint to out/checkpoint-114\n",
            "Configuration saved in out/checkpoint-114/config.json\n",
            "Model weights saved in out/checkpoint-114/pytorch_model.bin\n",
            "Feature extractor saved in out/checkpoint-114/preprocessor_config.json\n",
            "Deleting older checkpoint [out/checkpoint-760] due to args.save_total_limit\n",
            "The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSequenceClassification.forward` and have been ignored: id, duration. If id, duration are not expected by `Wav2Vec2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 153\n",
            "  Batch size = 32\n",
            "Saving model checkpoint to out/checkpoint-152\n",
            "Configuration saved in out/checkpoint-152/config.json\n",
            "Model weights saved in out/checkpoint-152/pytorch_model.bin\n",
            "Feature extractor saved in out/checkpoint-152/preprocessor_config.json\n",
            "Deleting older checkpoint [out/checkpoint-38] due to args.save_total_limit\n",
            "The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSequenceClassification.forward` and have been ignored: id, duration. If id, duration are not expected by `Wav2Vec2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 153\n",
            "  Batch size = 32\n",
            "Saving model checkpoint to out/checkpoint-190\n",
            "Configuration saved in out/checkpoint-190/config.json\n",
            "Model weights saved in out/checkpoint-190/pytorch_model.bin\n",
            "Feature extractor saved in out/checkpoint-190/preprocessor_config.json\n",
            "Deleting older checkpoint [out/checkpoint-76] due to args.save_total_limit\n",
            "The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSequenceClassification.forward` and have been ignored: id, duration. If id, duration are not expected by `Wav2Vec2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 153\n",
            "  Batch size = 32\n",
            "Saving model checkpoint to out/checkpoint-228\n",
            "Configuration saved in out/checkpoint-228/config.json\n",
            "Model weights saved in out/checkpoint-228/pytorch_model.bin\n",
            "Feature extractor saved in out/checkpoint-228/preprocessor_config.json\n",
            "Deleting older checkpoint [out/checkpoint-114] due to args.save_total_limit\n",
            "The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSequenceClassification.forward` and have been ignored: id, duration. If id, duration are not expected by `Wav2Vec2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 153\n",
            "  Batch size = 32\n",
            "Saving model checkpoint to out/checkpoint-266\n",
            "Configuration saved in out/checkpoint-266/config.json\n",
            "Model weights saved in out/checkpoint-266/pytorch_model.bin\n",
            "Feature extractor saved in out/checkpoint-266/preprocessor_config.json\n",
            "Deleting older checkpoint [out/checkpoint-152] due to args.save_total_limit\n",
            "The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSequenceClassification.forward` and have been ignored: id, duration. If id, duration are not expected by `Wav2Vec2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 153\n",
            "  Batch size = 32\n",
            "Saving model checkpoint to out/checkpoint-304\n",
            "Configuration saved in out/checkpoint-304/config.json\n",
            "Model weights saved in out/checkpoint-304/pytorch_model.bin\n",
            "Feature extractor saved in out/checkpoint-304/preprocessor_config.json\n",
            "Deleting older checkpoint [out/checkpoint-190] due to args.save_total_limit\n",
            "The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSequenceClassification.forward` and have been ignored: id, duration. If id, duration are not expected by `Wav2Vec2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 153\n",
            "  Batch size = 32\n",
            "Saving model checkpoint to out/checkpoint-342\n",
            "Configuration saved in out/checkpoint-342/config.json\n",
            "Model weights saved in out/checkpoint-342/pytorch_model.bin\n",
            "Feature extractor saved in out/checkpoint-342/preprocessor_config.json\n",
            "Deleting older checkpoint [out/checkpoint-228] due to args.save_total_limit\n",
            "The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSequenceClassification.forward` and have been ignored: id, duration. If id, duration are not expected by `Wav2Vec2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 153\n",
            "  Batch size = 32\n",
            "Saving model checkpoint to out/checkpoint-380\n",
            "Configuration saved in out/checkpoint-380/config.json\n",
            "Model weights saved in out/checkpoint-380/pytorch_model.bin\n",
            "Feature extractor saved in out/checkpoint-380/preprocessor_config.json\n",
            "Deleting older checkpoint [out/checkpoint-304] due to args.save_total_limit\n",
            "The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSequenceClassification.forward` and have been ignored: id, duration. If id, duration are not expected by `Wav2Vec2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 153\n",
            "  Batch size = 32\n",
            "Saving model checkpoint to out/checkpoint-418\n",
            "Configuration saved in out/checkpoint-418/config.json\n",
            "Model weights saved in out/checkpoint-418/pytorch_model.bin\n",
            "Feature extractor saved in out/checkpoint-418/preprocessor_config.json\n",
            "Deleting older checkpoint [out/checkpoint-342] due to args.save_total_limit\n",
            "The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSequenceClassification.forward` and have been ignored: id, duration. If id, duration are not expected by `Wav2Vec2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 153\n",
            "  Batch size = 32\n",
            "Saving model checkpoint to out/checkpoint-456\n",
            "Configuration saved in out/checkpoint-456/config.json\n",
            "Model weights saved in out/checkpoint-456/pytorch_model.bin\n",
            "Feature extractor saved in out/checkpoint-456/preprocessor_config.json\n",
            "Deleting older checkpoint [out/checkpoint-266] due to args.save_total_limit\n",
            "The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSequenceClassification.forward` and have been ignored: id, duration. If id, duration are not expected by `Wav2Vec2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 153\n",
            "  Batch size = 32\n",
            "Saving model checkpoint to out/checkpoint-494\n",
            "Configuration saved in out/checkpoint-494/config.json\n",
            "Model weights saved in out/checkpoint-494/pytorch_model.bin\n",
            "Feature extractor saved in out/checkpoint-494/preprocessor_config.json\n",
            "Deleting older checkpoint [out/checkpoint-380] due to args.save_total_limit\n",
            "The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSequenceClassification.forward` and have been ignored: id, duration. If id, duration are not expected by `Wav2Vec2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 153\n",
            "  Batch size = 32\n",
            "Saving model checkpoint to out/checkpoint-532\n",
            "Configuration saved in out/checkpoint-532/config.json\n",
            "Model weights saved in out/checkpoint-532/pytorch_model.bin\n",
            "Feature extractor saved in out/checkpoint-532/preprocessor_config.json\n",
            "Deleting older checkpoint [out/checkpoint-418] due to args.save_total_limit\n",
            "The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSequenceClassification.forward` and have been ignored: id, duration. If id, duration are not expected by `Wav2Vec2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 153\n",
            "  Batch size = 32\n",
            "Saving model checkpoint to out/checkpoint-570\n",
            "Configuration saved in out/checkpoint-570/config.json\n",
            "Model weights saved in out/checkpoint-570/pytorch_model.bin\n",
            "Feature extractor saved in out/checkpoint-570/preprocessor_config.json\n",
            "Deleting older checkpoint [out/checkpoint-494] due to args.save_total_limit\n",
            "The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSequenceClassification.forward` and have been ignored: id, duration. If id, duration are not expected by `Wav2Vec2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 153\n",
            "  Batch size = 32\n",
            "Saving model checkpoint to out/checkpoint-608\n",
            "Configuration saved in out/checkpoint-608/config.json\n",
            "Model weights saved in out/checkpoint-608/pytorch_model.bin\n",
            "Feature extractor saved in out/checkpoint-608/preprocessor_config.json\n",
            "Deleting older checkpoint [out/checkpoint-532] due to args.save_total_limit\n",
            "The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSequenceClassification.forward` and have been ignored: id, duration. If id, duration are not expected by `Wav2Vec2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 153\n",
            "  Batch size = 32\n",
            "Saving model checkpoint to out/checkpoint-646\n",
            "Configuration saved in out/checkpoint-646/config.json\n",
            "Model weights saved in out/checkpoint-646/pytorch_model.bin\n",
            "Feature extractor saved in out/checkpoint-646/preprocessor_config.json\n",
            "Deleting older checkpoint [out/checkpoint-456] due to args.save_total_limit\n",
            "The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSequenceClassification.forward` and have been ignored: id, duration. If id, duration are not expected by `Wav2Vec2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 153\n",
            "  Batch size = 32\n",
            "Saving model checkpoint to out/checkpoint-684\n",
            "Configuration saved in out/checkpoint-684/config.json\n",
            "Model weights saved in out/checkpoint-684/pytorch_model.bin\n",
            "Feature extractor saved in out/checkpoint-684/preprocessor_config.json\n",
            "Deleting older checkpoint [out/checkpoint-570] due to args.save_total_limit\n",
            "The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSequenceClassification.forward` and have been ignored: id, duration. If id, duration are not expected by `Wav2Vec2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 153\n",
            "  Batch size = 32\n",
            "Saving model checkpoint to out/checkpoint-722\n",
            "Configuration saved in out/checkpoint-722/config.json\n",
            "Model weights saved in out/checkpoint-722/pytorch_model.bin\n",
            "Feature extractor saved in out/checkpoint-722/preprocessor_config.json\n",
            "Deleting older checkpoint [out/checkpoint-608] due to args.save_total_limit\n",
            "The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSequenceClassification.forward` and have been ignored: id, duration. If id, duration are not expected by `Wav2Vec2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 153\n",
            "  Batch size = 32\n",
            "Saving model checkpoint to out/checkpoint-760\n",
            "Configuration saved in out/checkpoint-760/config.json\n",
            "Model weights saved in out/checkpoint-760/pytorch_model.bin\n",
            "Feature extractor saved in out/checkpoint-760/preprocessor_config.json\n",
            "Deleting older checkpoint [out/checkpoint-684] due to args.save_total_limit\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "Loading best model from out/checkpoint-646 (score: 0.7647058823529411).\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>▁▃▃▃▄▆▇▇▅▆▆▇▇▇▇▇██▇█</td></tr><tr><td>eval/loss</td><td>▅▄▃▃▃▂▁▂▄▄▄▅▅▇██▇██▇</td></tr><tr><td>eval/runtime</td><td>▄▁▅▂▄█▁▃▃▃▂▃▂▃▃█▄▅▂▃</td></tr><tr><td>eval/samples_per_second</td><td>▅█▄▇▅▁█▆▆▆▇▅▇▆▆▁▅▄▇▆</td></tr><tr><td>eval/steps_per_second</td><td>▅█▄▇▅▁█▆▆▆▇▅▇▆▆▁▅▄▇▆</td></tr><tr><td>train/epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>train/global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>train/learning_rate</td><td>██▇▇▇▆▆▆▅▅▅▄▄▃▃▃▂▂▂▁▁</td></tr><tr><td>train/loss</td><td>██▇▇▆▅▅▄▄▃▃▃▃▂▂▁▁▁▁▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>0.75163</td></tr><tr><td>eval/loss</td><td>1.2474</td></tr><tr><td>eval/runtime</td><td>13.6322</td></tr><tr><td>eval/samples_per_second</td><td>11.223</td></tr><tr><td>eval/steps_per_second</td><td>0.367</td></tr><tr><td>train/epoch</td><td>20.0</td></tr><tr><td>train/global_step</td><td>760</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.0361</td></tr><tr><td>train/total_flos</td><td>9.981901915841618e+17</td></tr><tr><td>train/train_loss</td><td>0.40772</td></tr><tr><td>train/train_runtime</td><td>3231.6884</td></tr><tr><td>train/train_samples_per_second</td><td>3.763</td></tr><tr><td>train/train_steps_per_second</td><td>0.235</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">baseline</strong> at: <a href='https://wandb.ai/astockman/music-classification-aii/runs/pr5oib7p' target=\"_blank\">https://wandb.ai/astockman/music-classification-aii/runs/pr5oib7p</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20230222_105059-pr5oib7p/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to /content/drive/MyDrive/Projects/music-classification/res/models/baseline\n",
            "Configuration saved in /content/drive/MyDrive/Projects/music-classification/res/models/baseline/config.json\n",
            "Model weights saved in /content/drive/MyDrive/Projects/music-classification/res/models/baseline/pytorch_model.bin\n",
            "Feature extractor saved in /content/drive/MyDrive/Projects/music-classification/res/models/baseline/preprocessor_config.json\n"
          ]
        }
      ],
      "source": [
        "TRAINING_CONFIG[\"warmup\"] = 0.0\n",
        "TRAINING_CONFIG[\"freeze_encoder\"] = False\n",
        "\n",
        "run_name = \"baseline\"\n",
        "model = get_model(TRAINING_CONFIG, ds[\"train\"])\n",
        "\n",
        "trainer = get_trainer(\n",
        "    run_name=run_name,\n",
        "    model=model,\n",
        "    train_ds=ds[\"train\"],\n",
        "    eval_ds=ds[\"test\"],\n",
        "    training_config=TRAINING_CONFIG,\n",
        "    feature_extractor=None,\n",
        "    output_dir=\"out\",\n",
        "    debug=True,\n",
        "    env=NOTEBOOK_ENV,\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "end_training(run_name, trainer, MODELS_DIR_PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4UkUlHVixus1"
      },
      "source": [
        "## Wav2Vec2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "2EZnDRWtxus2"
      },
      "outputs": [],
      "source": [
        "permutations = {\n",
        "    \"freeze_encoder\": [True, False],\n",
        "    \"classifier_layers\": [[256], [256, 256]],\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "yQziqkRexus3",
        "outputId": "62da9f33-69b2-4e7e-ed29-c7a2a8dfe1b6"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-baa088e8928f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mconfig\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpermutation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"feature_encoder\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mrun_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_run_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'permutation' is not defined"
          ]
        }
      ],
      "source": [
        "for config in permutation:\n",
        "    ds = datasets[config[\"feature_encoder\"]]\n",
        "    run_name = get_run_name(config)\n",
        "    model = get_model(config, ds)\n",
        "\n",
        "    trainer = get_trainer(\n",
        "        run_name=run_name,\n",
        "        model=model,\n",
        "        train_ds=ds[\"train\"],\n",
        "        eval_ds=ds[\"valid\"],\n",
        "        training_config=config,\n",
        "        feature_extractor=None,\n",
        "        output_dir=\"out\",\n",
        "        debug=True,\n",
        "        env=NOTEBOOK_ENV,\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "    end_training(run_name, trainer, MODELS_DIR_PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Z6ixINFyTA7"
      },
      "source": [
        "## Whisper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3HbmFGfWyStR"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "interpreter": {
      "hash": "729786e4ac820b34608f27d07ec4fb597776bcc1a9d4d98bff8b4375359b1d17"
    },
    "kernelspec": {
      "display_name": "Python 3.10.9 ('music-classification-89IuhjTZ')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0fee81913e764382bfd0946d1661be8d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_570e7fe98c364883a87014e220e5baf5",
            "placeholder": "​",
            "style": "IPY_MODEL_5d46f1f40ae24380a6b6d3304d1d1d36",
            "value": "Casting the dataset: 100%"
          }
        },
        "2febbdf5605149f494d9ac510883f262": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "35622cbe24684a7ea5382f1f536bd9ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0fee81913e764382bfd0946d1661be8d",
              "IPY_MODEL_c9e2fff6b66e434e9806d869417f4eff",
              "IPY_MODEL_e3207490c4ba470f8e7cf61b97d84d54"
            ],
            "layout": "IPY_MODEL_2febbdf5605149f494d9ac510883f262"
          }
        },
        "4849127988f64e1e9558a88f220ad1de": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "570e7fe98c364883a87014e220e5baf5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5d46f1f40ae24380a6b6d3304d1d1d36": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "76877461d03e4018b8e5c5bc1721d2cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7df7757d71ea46d5a4616f431066aef1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a70ff0866d564c96ae19d5d6aa91a727": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c9e2fff6b66e434e9806d869417f4eff": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4849127988f64e1e9558a88f220ad1de",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_76877461d03e4018b8e5c5bc1721d2cb",
            "value": 1
          }
        },
        "e3207490c4ba470f8e7cf61b97d84d54": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a70ff0866d564c96ae19d5d6aa91a727",
            "placeholder": "​",
            "style": "IPY_MODEL_7df7757d71ea46d5a4616f431066aef1",
            "value": " 1/1 [00:12&lt;00:00, 12.42s/ba]"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
