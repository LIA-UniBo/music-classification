{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "4CspUKbHyK7A"
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pmtu1rHfxusY"
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "oZfijJ2kxusc"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import datasets\n",
    "import pandas as pd\n",
    "\n",
    "from src.dataset import add_audio_column, filter_df, prepare_ds, split_df\n",
    "from src.train import end_training, get_model, get_trainer\n",
    "from src.utils import get_csv_name, get_run_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "mFZtkDipxusg"
   },
   "outputs": [],
   "source": [
    "RES_DIR_PATH = \"res\"\n",
    "NOTEBOOK_ENV = \"jupyter\"\n",
    "\n",
    "AUDIOS_DIR_PATH = os.path.join(RES_DIR_PATH, \"mp3_data\")\n",
    "MODELS_DIR_PATH = os.path.join(RES_DIR_PATH, \"models\")\n",
    "DATASETS_DIR_PATH = os.path.join(RES_DIR_PATH, \"datasets\")\n",
    "\n",
    "CSV_PATH = os.path.join(RES_DIR_PATH, \"samples_clustered.csv\")\n",
    "\n",
    "TOP_N_GENRES = 6\n",
    "TOP_N_FEATURES = 9\n",
    "\n",
    "\n",
    "FEATURES_CONFIG_SUBSET = {\"genre\": {\"top_n\": 3, \"samples\": 1000}}\n",
    "FEATURES_CONFIG_GEN = {\"genre\": {\"top_n\": TOP_N_GENRES, \"samples\": None}}\n",
    "FEATURES_CONFIG_CAT = {\"category\": {\"top_n\": TOP_N_FEATURES, \"samples\": None}}\n",
    "FEATURES_CONFIG_MULTI = {\n",
    "    \"genre\": {\"top_n\": TOP_N_GENRES, \"samples\": None},\n",
    "    \"category\": {\"top_n\": TOP_N_FEATURES, \"samples\": None},\n",
    "}\n",
    "\n",
    "VALID_SIZE = 0.1\n",
    "TEST_SIZE = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "siGNvZU5xusi"
   },
   "source": [
    "## Backbones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two considered backbones are [Wav2Vec2](https://arxiv.org/abs/2006.11477) and [Whisper](https://cdn.openai.com/papers/whisper.pdf).\n",
    "\n",
    "Both models are used through the [Hugging Face Transformers](https://huggingface.co/docs/transformers) library.\n",
    "\n",
    "The implementation of the **Wav2Vec2** classifier follows the one in the [Wav2Vec2ForSequenceClassification](https://huggingface.co/docs/transformers/model_doc/wav2vec2#transformers.Wav2Vec2ForSequenceClassification) class, adding the support for a custom classification head.\n",
    "\n",
    "Regarding **Whisper**, I took the outputs from the [WhisperEncoder](https://huggingface.co/docs/transformers/model_doc/whisper) class and used them right away."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V6G03uSuxusl"
   },
   "source": [
    "## Fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XIqT4RUdxuso"
   },
   "source": [
    "For both of the backbones, when freezing them, the gradient computation of the entire encoder was disabled."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hDoCTT27xusp"
   },
   "source": [
    "## Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aEzI2ksqxusr"
   },
   "source": [
    "The classifier is implemented through an MLP, with variable layer size and hidden dimensions.\n",
    "Each layer is followed by an optional Dropout layer and a ReLU activation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yo8fX54ixust"
   },
   "source": [
    "## Multi-task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eL1fVvwkxusv"
   },
   "source": [
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PxFCxU7Oxusw"
   },
   "source": [
    "# Training 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "ZOEp2fHKxusw"
   },
   "outputs": [],
   "source": [
    "TRAINING_CONFIG = {\n",
    "    \"epochs\": 20,\n",
    "    \"learning_rate\": 5e-5,\n",
    "    \"warmup\": 0.0,\n",
    "    \"train_batch_size\": 8,\n",
    "    \"eval_batch_size\": 16,\n",
    "    \"feature_encoder\": None,\n",
    "    \"freeze_encoder\": None,\n",
    "    \"classifier_layers\": None, \n",
    "    \"classifier_dropout\": None,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_or_load_df(features_config):\n",
    "    filtered_csv_path = get_csv_name(features_config, CSV_PATH)\n",
    "\n",
    "    # If the subset is already in the filesystem, load it directly\n",
    "    if os.path.exists(filtered_csv_path):\n",
    "        print(f\"Loading {filtered_csv_path}\")\n",
    "        df = pd.read_csv(filtered_csv_path)\n",
    "    else:\n",
    "        df = pd.read_csv(CSV_PATH)\n",
    "        # Filter the dataset according to the given configuration and remove rows containing null values\n",
    "        df = filter_df(\n",
    "            df, \n",
    "            remove_nones=True,\n",
    "            features_config=features_config, \n",
    "        )\n",
    "        df.to_csv(filtered_csv_path, index=False)\n",
    "\n",
    "    print(f\"{len(df)} examples in DataFrame\")\n",
    "    # If the split column is not in the dataset, split the dataset into three partisions using \n",
    "    # `TEST_SIZE` and `VALID_SIZE` and save the result\n",
    "\n",
    "    if \"split\" not in df.columns:\n",
    "        df = split_df(df, validation_size=VALID_SIZE, test_size=TEST_SIZE)\n",
    "        df.to_csv(filtered_csv_path, index=False)\n",
    "\n",
    "    print(df.value_counts(\"split\"))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function for loading the dataset for the requested model\n",
    "\n",
    "def load_and_prepare_ds(training_config, feature_config, df, clustered=True):\n",
    "    encoded_dataset_path = os.path.join(DATASETS_DIR_PATH, f\"ds-{training_config['feature_encoder']}-full-encoded\")\n",
    "    ds = datasets.load_from_disk(encoded_dataset_path)\n",
    "    ds = add_audio_column(ds, audios_dir_path=AUDIOS_DIR_PATH, training_config={\"feature_encoder\": training_config['feature_encoder']})\n",
    "    return prepare_ds(ds, df, feature_config, clustered=clustered, fixed_mapping=None, save=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading res/samples_clustered_genre3s1000.csv\n",
      "999 examples in DataFrame\n",
      "split\n",
      "train    799\n",
      "test     100\n",
      "valid    100\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Build the filename indicating the subset of the whole dataset with the specific configurations\n",
    "df = create_or_load_df(FEATURES_CONFIG_SUBSET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7MWfQIuVxus0"
   },
   "source": [
    "## Wav2Vec2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "wSFCEv3T4Q-m"
   },
   "outputs": [],
   "source": [
    "TRAINING_CONFIG[\"feature_encoder\"] = \"wav2vec2\"\n",
    "TRAINING_CONFIG[\"freeze_encoder\"] = True\n",
    "TRAINING_CONFIG[\"classifier_layers\"] = [256]\n",
    "TRAINING_CONFIG[\"classifier_dropout\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alesssandros/.local/share/virtualenvs/aii-EtAbm6lw/lib/python3.10/site-packages/transformers/configuration_utils.py:375: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "Loading cached processed dataset at /home/alesssandros/dev/FCN_Newspaper/aii/res/datasets/ds-wav2vec2-full-encoded/cache-b5264b5b2644adf3.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing extra columns from dataset\n",
      "Mapping features clusters\n",
      "Extracting train split\n",
      "Extracting valid split\n",
      "Extracting test split\n",
      "Create `ClassLabels` for target classes\n",
      "{'genre': ClassLabel(names=['Electronic', 'Rock/Blues', 'World/Ethnic'], id=None)}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "658de955cee34a338fb87dd1b051e627",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting the dataset:   0%|          | 0/799 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ec044cdb60e4dfa87424e7276078a10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting the dataset:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf75f0a64c9e469a9edb32df23cd90f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting the dataset:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['label', 'id', 'duration', 'input_values'],\n",
       "        num_rows: 799\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['label', 'id', 'duration', 'input_values'],\n",
       "        num_rows: 100\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['label', 'id', 'duration', 'input_values'],\n",
       "        num_rows: 100\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prepared_ds = load_and_prepare_ds(TRAINING_CONFIG, FEATURES_CONFIG_SUBSET, df)\n",
    "\n",
    "prepared_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "m-AHRUIK2KPL",
    "outputId": "34b6f338-882c-4ebb-8ce0-b4508b66d254"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/wav2vec2-base were not used when initializing Wav2Vec2ForSequenceMultiClassification: ['quantizer.weight_proj.bias', 'project_hid.bias', 'project_hid.weight', 'project_q.weight', 'quantizer.weight_proj.weight', 'quantizer.codevectors', 'project_q.bias']\n",
      "- This IS expected if you are initializing Wav2Vec2ForSequenceMultiClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2ForSequenceMultiClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2ForSequenceMultiClassification were not initialized from the model checkpoint at facebook/wav2vec2-base and are newly initialized: ['classifier.weight', 'head.layers.0.bias', 'projector.weight', 'head.layers.0.weight', 'projector.bias', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mastockman\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/alesssandros/dev/FCN_Newspaper/aii/wandb/run-20230223_205425-hwhr4y6e</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/astockman/music-classification-aii/runs/hwhr4y6e' target=\"_blank\">wav2vec2-frz-c256-d0-20230223-205423</a></strong> to <a href='https://wandb.ai/astockman/music-classification-aii' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/astockman/music-classification-aii' target=\"_blank\">https://wandb.ai/astockman/music-classification-aii</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/astockman/music-classification-aii/runs/hwhr4y6e' target=\"_blank\">https://wandb.ai/astockman/music-classification-aii/runs/hwhr4y6e</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `Wav2Vec2ForSequenceMultiClassification.forward` and have been ignored: duration, id. If duration, id are not expected by `Wav2Vec2ForSequenceMultiClassification.forward`,  you can safely ignore this message.\n",
      "/home/alesssandros/.local/share/virtualenvs/aii-EtAbm6lw/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 799\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 2000\n",
      "  Number of trainable parameters = 394499\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2000' max='2000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2000/2000 11:51, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.086300</td>\n",
       "      <td>1.071130</td>\n",
       "      <td>0.460000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.062800</td>\n",
       "      <td>1.046994</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.043500</td>\n",
       "      <td>1.017666</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.017200</td>\n",
       "      <td>0.996653</td>\n",
       "      <td>0.490000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.997600</td>\n",
       "      <td>0.983945</td>\n",
       "      <td>0.520000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.982400</td>\n",
       "      <td>0.967910</td>\n",
       "      <td>0.530000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.981800</td>\n",
       "      <td>0.962945</td>\n",
       "      <td>0.510000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.962700</td>\n",
       "      <td>0.956101</td>\n",
       "      <td>0.530000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.942700</td>\n",
       "      <td>0.947107</td>\n",
       "      <td>0.550000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.942000</td>\n",
       "      <td>0.946416</td>\n",
       "      <td>0.560000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.954300</td>\n",
       "      <td>0.942338</td>\n",
       "      <td>0.550000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.942400</td>\n",
       "      <td>0.941398</td>\n",
       "      <td>0.530000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.917800</td>\n",
       "      <td>0.935344</td>\n",
       "      <td>0.550000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.917100</td>\n",
       "      <td>0.937705</td>\n",
       "      <td>0.540000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.906000</td>\n",
       "      <td>0.937021</td>\n",
       "      <td>0.540000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.902100</td>\n",
       "      <td>0.933175</td>\n",
       "      <td>0.560000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.913800</td>\n",
       "      <td>0.930955</td>\n",
       "      <td>0.580000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.907600</td>\n",
       "      <td>0.932101</td>\n",
       "      <td>0.560000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.895500</td>\n",
       "      <td>0.931521</td>\n",
       "      <td>0.570000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.911000</td>\n",
       "      <td>0.932199</td>\n",
       "      <td>0.560000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSequenceMultiClassification.forward` and have been ignored: duration, id. If duration, id are not expected by `Wav2Vec2ForSequenceMultiClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to out/checkpoint-100\n",
      "Configuration saved in out/checkpoint-100/config.json\n",
      "Model weights saved in out/checkpoint-100/pytorch_model.bin\n",
      "Feature extractor saved in out/checkpoint-100/preprocessor_config.json\n",
      "Deleting older checkpoint [out/checkpoint-200] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSequenceMultiClassification.forward` and have been ignored: duration, id. If duration, id are not expected by `Wav2Vec2ForSequenceMultiClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to out/checkpoint-200\n",
      "Configuration saved in out/checkpoint-200/config.json\n",
      "Model weights saved in out/checkpoint-200/pytorch_model.bin\n",
      "Feature extractor saved in out/checkpoint-200/preprocessor_config.json\n",
      "Deleting older checkpoint [out/checkpoint-1900] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSequenceMultiClassification.forward` and have been ignored: duration, id. If duration, id are not expected by `Wav2Vec2ForSequenceMultiClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to out/checkpoint-300\n",
      "Configuration saved in out/checkpoint-300/config.json\n",
      "Model weights saved in out/checkpoint-300/pytorch_model.bin\n",
      "Feature extractor saved in out/checkpoint-300/preprocessor_config.json\n",
      "Deleting older checkpoint [out/checkpoint-2000] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSequenceMultiClassification.forward` and have been ignored: duration, id. If duration, id are not expected by `Wav2Vec2ForSequenceMultiClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to out/checkpoint-400\n",
      "Configuration saved in out/checkpoint-400/config.json\n",
      "Model weights saved in out/checkpoint-400/pytorch_model.bin\n",
      "Feature extractor saved in out/checkpoint-400/preprocessor_config.json\n",
      "Deleting older checkpoint [out/checkpoint-100] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSequenceMultiClassification.forward` and have been ignored: duration, id. If duration, id are not expected by `Wav2Vec2ForSequenceMultiClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to out/checkpoint-500\n",
      "Configuration saved in out/checkpoint-500/config.json\n",
      "Model weights saved in out/checkpoint-500/pytorch_model.bin\n",
      "Feature extractor saved in out/checkpoint-500/preprocessor_config.json\n",
      "Deleting older checkpoint [out/checkpoint-200] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSequenceMultiClassification.forward` and have been ignored: duration, id. If duration, id are not expected by `Wav2Vec2ForSequenceMultiClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to out/checkpoint-600\n",
      "Configuration saved in out/checkpoint-600/config.json\n",
      "Model weights saved in out/checkpoint-600/pytorch_model.bin\n",
      "Feature extractor saved in out/checkpoint-600/preprocessor_config.json\n",
      "Deleting older checkpoint [out/checkpoint-300] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSequenceMultiClassification.forward` and have been ignored: duration, id. If duration, id are not expected by `Wav2Vec2ForSequenceMultiClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to out/checkpoint-700\n",
      "Configuration saved in out/checkpoint-700/config.json\n",
      "Model weights saved in out/checkpoint-700/pytorch_model.bin\n",
      "Feature extractor saved in out/checkpoint-700/preprocessor_config.json\n",
      "Deleting older checkpoint [out/checkpoint-400] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSequenceMultiClassification.forward` and have been ignored: duration, id. If duration, id are not expected by `Wav2Vec2ForSequenceMultiClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to out/checkpoint-800\n",
      "Configuration saved in out/checkpoint-800/config.json\n",
      "Model weights saved in out/checkpoint-800/pytorch_model.bin\n",
      "Feature extractor saved in out/checkpoint-800/preprocessor_config.json\n",
      "Deleting older checkpoint [out/checkpoint-500] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSequenceMultiClassification.forward` and have been ignored: duration, id. If duration, id are not expected by `Wav2Vec2ForSequenceMultiClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to out/checkpoint-900\n",
      "Configuration saved in out/checkpoint-900/config.json\n",
      "Model weights saved in out/checkpoint-900/pytorch_model.bin\n",
      "Feature extractor saved in out/checkpoint-900/preprocessor_config.json\n",
      "Deleting older checkpoint [out/checkpoint-600] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSequenceMultiClassification.forward` and have been ignored: duration, id. If duration, id are not expected by `Wav2Vec2ForSequenceMultiClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to out/checkpoint-1000\n",
      "Configuration saved in out/checkpoint-1000/config.json\n",
      "Model weights saved in out/checkpoint-1000/pytorch_model.bin\n",
      "Feature extractor saved in out/checkpoint-1000/preprocessor_config.json\n",
      "Deleting older checkpoint [out/checkpoint-700] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSequenceMultiClassification.forward` and have been ignored: duration, id. If duration, id are not expected by `Wav2Vec2ForSequenceMultiClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to out/checkpoint-1100\n",
      "Configuration saved in out/checkpoint-1100/config.json\n",
      "Model weights saved in out/checkpoint-1100/pytorch_model.bin\n",
      "Feature extractor saved in out/checkpoint-1100/preprocessor_config.json\n",
      "Deleting older checkpoint [out/checkpoint-800] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSequenceMultiClassification.forward` and have been ignored: duration, id. If duration, id are not expected by `Wav2Vec2ForSequenceMultiClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to out/checkpoint-1200\n",
      "Configuration saved in out/checkpoint-1200/config.json\n",
      "Model weights saved in out/checkpoint-1200/pytorch_model.bin\n",
      "Feature extractor saved in out/checkpoint-1200/preprocessor_config.json\n",
      "Deleting older checkpoint [out/checkpoint-900] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSequenceMultiClassification.forward` and have been ignored: duration, id. If duration, id are not expected by `Wav2Vec2ForSequenceMultiClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to out/checkpoint-1300\n",
      "Configuration saved in out/checkpoint-1300/config.json\n",
      "Model weights saved in out/checkpoint-1300/pytorch_model.bin\n",
      "Feature extractor saved in out/checkpoint-1300/preprocessor_config.json\n",
      "Deleting older checkpoint [out/checkpoint-1100] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSequenceMultiClassification.forward` and have been ignored: duration, id. If duration, id are not expected by `Wav2Vec2ForSequenceMultiClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to out/checkpoint-1400\n",
      "Configuration saved in out/checkpoint-1400/config.json\n",
      "Model weights saved in out/checkpoint-1400/pytorch_model.bin\n",
      "Feature extractor saved in out/checkpoint-1400/preprocessor_config.json\n",
      "Deleting older checkpoint [out/checkpoint-1200] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSequenceMultiClassification.forward` and have been ignored: duration, id. If duration, id are not expected by `Wav2Vec2ForSequenceMultiClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to out/checkpoint-1500\n",
      "Configuration saved in out/checkpoint-1500/config.json\n",
      "Model weights saved in out/checkpoint-1500/pytorch_model.bin\n",
      "Feature extractor saved in out/checkpoint-1500/preprocessor_config.json\n",
      "Deleting older checkpoint [out/checkpoint-1300] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSequenceMultiClassification.forward` and have been ignored: duration, id. If duration, id are not expected by `Wav2Vec2ForSequenceMultiClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to out/checkpoint-1600\n",
      "Configuration saved in out/checkpoint-1600/config.json\n",
      "Model weights saved in out/checkpoint-1600/pytorch_model.bin\n",
      "Feature extractor saved in out/checkpoint-1600/preprocessor_config.json\n",
      "Deleting older checkpoint [out/checkpoint-1400] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSequenceMultiClassification.forward` and have been ignored: duration, id. If duration, id are not expected by `Wav2Vec2ForSequenceMultiClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to out/checkpoint-1700\n",
      "Configuration saved in out/checkpoint-1700/config.json\n",
      "Model weights saved in out/checkpoint-1700/pytorch_model.bin\n",
      "Feature extractor saved in out/checkpoint-1700/preprocessor_config.json\n",
      "Deleting older checkpoint [out/checkpoint-1000] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSequenceMultiClassification.forward` and have been ignored: duration, id. If duration, id are not expected by `Wav2Vec2ForSequenceMultiClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to out/checkpoint-1800\n",
      "Configuration saved in out/checkpoint-1800/config.json\n",
      "Model weights saved in out/checkpoint-1800/pytorch_model.bin\n",
      "Feature extractor saved in out/checkpoint-1800/preprocessor_config.json\n",
      "Deleting older checkpoint [out/checkpoint-1500] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSequenceMultiClassification.forward` and have been ignored: duration, id. If duration, id are not expected by `Wav2Vec2ForSequenceMultiClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to out/checkpoint-1900\n",
      "Configuration saved in out/checkpoint-1900/config.json\n",
      "Model weights saved in out/checkpoint-1900/pytorch_model.bin\n",
      "Feature extractor saved in out/checkpoint-1900/preprocessor_config.json\n",
      "Deleting older checkpoint [out/checkpoint-1600] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSequenceMultiClassification.forward` and have been ignored: duration, id. If duration, id are not expected by `Wav2Vec2ForSequenceMultiClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to out/checkpoint-2000\n",
      "Configuration saved in out/checkpoint-2000/config.json\n",
      "Model weights saved in out/checkpoint-2000/pytorch_model.bin\n",
      "Feature extractor saved in out/checkpoint-2000/preprocessor_config.json\n",
      "Deleting older checkpoint [out/checkpoint-1800] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from out/checkpoint-1700 (score: 0.58).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>▁▃▃▃▄▅▄▅▆▇▆▅▆▆▆▇█▇▇▇</td></tr><tr><td>eval/loss</td><td>█▇▅▄▄▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>eval/runtime</td><td>▁▁▁▁▁▁▁▁▁▁█▁▁▁▁███▁▁</td></tr><tr><td>eval/samples_per_second</td><td>▇█████████▁████▁▁▁██</td></tr><tr><td>eval/steps_per_second</td><td>▇█████████▁████▁▁▁██</td></tr><tr><td>train/epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>train/global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>train/learning_rate</td><td>███▇▇▇▇▇▇▆▆▆▆▆▆▅▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▁▁</td></tr><tr><td>train/loss</td><td>███▇▇▇▆▆▅▅▅▄▄▄▄▄▄▃▃▃▃▂▃▂▃▂▂▂▂▂▂▂▁▁▂▁▂▂▁▂</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>0.56</td></tr><tr><td>eval/loss</td><td>0.9322</td></tr><tr><td>eval/runtime</td><td>2.4511</td></tr><tr><td>eval/samples_per_second</td><td>40.799</td></tr><tr><td>eval/steps_per_second</td><td>2.856</td></tr><tr><td>train/epoch</td><td>20.0</td></tr><tr><td>train/global_step</td><td>2000</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.911</td></tr><tr><td>train/total_flos</td><td>1.245292480666616e+18</td></tr><tr><td>train/train_loss</td><td>0.95915</td></tr><tr><td>train/train_runtime</td><td>712.5984</td></tr><tr><td>train/train_samples_per_second</td><td>22.425</td></tr><tr><td>train/train_steps_per_second</td><td>2.807</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">wav2vec2-frz-c256-d0-20230223-205423</strong> at: <a href='https://wandb.ai/astockman/music-classification-aii/runs/hwhr4y6e' target=\"_blank\">https://wandb.ai/astockman/music-classification-aii/runs/hwhr4y6e</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230223_205425-hwhr4y6e/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to res/models/wav2vec2-frz-c256-d0-20230223-205423\n",
      "Configuration saved in res/models/wav2vec2-frz-c256-d0-20230223-205423/config.json\n",
      "Model weights saved in res/models/wav2vec2-frz-c256-d0-20230223-205423/pytorch_model.bin\n",
      "Feature extractor saved in res/models/wav2vec2-frz-c256-d0-20230223-205423/preprocessor_config.json\n"
     ]
    }
   ],
   "source": [
    "run_name = get_run_name(TRAINING_CONFIG)\n",
    "model = get_model(TRAINING_CONFIG, prepared_ds[\"train\"])\n",
    "\n",
    "trainer = get_trainer(\n",
    "    run_name=run_name,\n",
    "    model=model,\n",
    "    train_ds=prepared_ds[\"train\"],\n",
    "    eval_ds=prepared_ds[\"valid\"],\n",
    "    training_config=TRAINING_CONFIG,\n",
    "    output_dir=\"out\",\n",
    "    debug=False,\n",
    "    env=NOTEBOOK_ENV,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "end_training(run_name, trainer, MODELS_DIR_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_CONFIG[\"freeze_encoder\"] = False "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/alesssandros/.cache/huggingface/hub/models--facebook--wav2vec2-base/snapshots/0b5b8e868dd84f03fd87d01f9c4ff0f080fecfe8/config.json\n",
      "/home/alesssandros/.local/share/virtualenvs/aii-EtAbm6lw/lib/python3.10/site-packages/transformers/configuration_utils.py:375: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "Model config Wav2Vec2Config {\n",
      "  \"_name_or_path\": \"facebook/wav2vec2-base\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"adapter_kernel_size\": 3,\n",
      "  \"adapter_stride\": 2,\n",
      "  \"add_adapter\": false,\n",
      "  \"apply_spec_augment\": true,\n",
      "  \"architectures\": [\n",
      "    \"Wav2Vec2ForPreTraining\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"classifier_proj_size\": 256,\n",
      "  \"codevector_dim\": 256,\n",
      "  \"contrastive_logits_temperature\": 0.1,\n",
      "  \"conv_bias\": false,\n",
      "  \"conv_dim\": [\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512\n",
      "  ],\n",
      "  \"conv_kernel\": [\n",
      "    10,\n",
      "    3,\n",
      "    3,\n",
      "    3,\n",
      "    3,\n",
      "    2,\n",
      "    2\n",
      "  ],\n",
      "  \"conv_stride\": [\n",
      "    5,\n",
      "    2,\n",
      "    2,\n",
      "    2,\n",
      "    2,\n",
      "    2,\n",
      "    2\n",
      "  ],\n",
      "  \"ctc_loss_reduction\": \"sum\",\n",
      "  \"ctc_zero_infinity\": false,\n",
      "  \"diversity_loss_weight\": 0.1,\n",
      "  \"do_stable_layer_norm\": false,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"feat_extract_activation\": \"gelu\",\n",
      "  \"feat_extract_norm\": \"group\",\n",
      "  \"feat_proj_dropout\": 0.1,\n",
      "  \"feat_quantizer_dropout\": 0.0,\n",
      "  \"final_dropout\": 0.0,\n",
      "  \"freeze_feat_extract_train\": true,\n",
      "  \"gradient_checkpointing\": true,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"layerdrop\": 0.0,\n",
      "  \"mask_channel_length\": 10,\n",
      "  \"mask_channel_min_space\": 1,\n",
      "  \"mask_channel_other\": 0.0,\n",
      "  \"mask_channel_prob\": 0.0,\n",
      "  \"mask_channel_selection\": \"static\",\n",
      "  \"mask_feature_length\": 10,\n",
      "  \"mask_feature_min_masks\": 0,\n",
      "  \"mask_feature_prob\": 0.0,\n",
      "  \"mask_time_length\": 10,\n",
      "  \"mask_time_min_masks\": 2,\n",
      "  \"mask_time_min_space\": 1,\n",
      "  \"mask_time_other\": 0.0,\n",
      "  \"mask_time_prob\": 0.05,\n",
      "  \"mask_time_selection\": \"static\",\n",
      "  \"model_type\": \"wav2vec2\",\n",
      "  \"no_mask_channel_overlap\": false,\n",
      "  \"no_mask_time_overlap\": false,\n",
      "  \"num_adapter_layers\": 3,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_codevector_groups\": 2,\n",
      "  \"num_codevectors_per_group\": 320,\n",
      "  \"num_conv_pos_embedding_groups\": 16,\n",
      "  \"num_conv_pos_embeddings\": 128,\n",
      "  \"num_feat_extract_layers\": 7,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_negatives\": 100,\n",
      "  \"output_hidden_size\": 768,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"proj_codevector_dim\": 256,\n",
      "  \"tdnn_dilation\": [\n",
      "    1,\n",
      "    2,\n",
      "    3,\n",
      "    1,\n",
      "    1\n",
      "  ],\n",
      "  \"tdnn_dim\": [\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    1500\n",
      "  ],\n",
      "  \"tdnn_kernel\": [\n",
      "    5,\n",
      "    3,\n",
      "    3,\n",
      "    1,\n",
      "    1\n",
      "  ],\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"use_weighted_layer_sum\": false,\n",
      "  \"vocab_size\": 32,\n",
      "  \"xvector_output_dim\": 512\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/alesssandros/.cache/huggingface/hub/models--facebook--wav2vec2-base/snapshots/0b5b8e868dd84f03fd87d01f9c4ff0f080fecfe8/pytorch_model.bin\n",
      "Some weights of the model checkpoint at facebook/wav2vec2-base were not used when initializing Wav2Vec2ForSequenceMultiClassification: ['quantizer.weight_proj.bias', 'project_hid.bias', 'project_hid.weight', 'project_q.weight', 'quantizer.weight_proj.weight', 'quantizer.codevectors', 'project_q.bias']\n",
      "- This IS expected if you are initializing Wav2Vec2ForSequenceMultiClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2ForSequenceMultiClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2ForSequenceMultiClassification were not initialized from the model checkpoint at facebook/wav2vec2-base and are newly initialized: ['classifier.weight', 'head.layers.0.bias', 'projector.weight', 'head.layers.0.weight', 'projector.bias', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96908999a3674b4d91bbc3be93bde43b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016668569513907036, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/alesssandros/dev/FCN_Newspaper/aii/wandb/run-20230223_210655-91c7rz12</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/astockman/music-classification-aii/runs/91c7rz12' target=\"_blank\">wav2vec2-fnt-c256-d0-20230223-210654</a></strong> to <a href='https://wandb.ai/astockman/music-classification-aii' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/astockman/music-classification-aii' target=\"_blank\">https://wandb.ai/astockman/music-classification-aii</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/astockman/music-classification-aii/runs/91c7rz12' target=\"_blank\">https://wandb.ai/astockman/music-classification-aii/runs/91c7rz12</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The following columns in the training set don't have a corresponding argument in `Wav2Vec2ForSequenceMultiClassification.forward` and have been ignored: duration, id. If duration, id are not expected by `Wav2Vec2ForSequenceMultiClassification.forward`,  you can safely ignore this message.\n",
      "/home/alesssandros/.local/share/virtualenvs/aii-EtAbm6lw/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 799\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 2000\n",
      "  Number of trainable parameters = 94766211\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2000' max='2000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2000/2000 18:42, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.016700</td>\n",
       "      <td>0.858208</td>\n",
       "      <td>0.660000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.829100</td>\n",
       "      <td>0.780925</td>\n",
       "      <td>0.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.763700</td>\n",
       "      <td>0.748313</td>\n",
       "      <td>0.670000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.654400</td>\n",
       "      <td>0.809231</td>\n",
       "      <td>0.630000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.568900</td>\n",
       "      <td>0.624179</td>\n",
       "      <td>0.770000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.509300</td>\n",
       "      <td>0.785466</td>\n",
       "      <td>0.740000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.439900</td>\n",
       "      <td>0.622021</td>\n",
       "      <td>0.820000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.290900</td>\n",
       "      <td>0.577539</td>\n",
       "      <td>0.860000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.299600</td>\n",
       "      <td>0.682577</td>\n",
       "      <td>0.830000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.280700</td>\n",
       "      <td>0.620278</td>\n",
       "      <td>0.830000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.241800</td>\n",
       "      <td>1.150820</td>\n",
       "      <td>0.770000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.143700</td>\n",
       "      <td>0.824184</td>\n",
       "      <td>0.810000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.133400</td>\n",
       "      <td>0.870542</td>\n",
       "      <td>0.810000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.088900</td>\n",
       "      <td>0.664543</td>\n",
       "      <td>0.840000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.076500</td>\n",
       "      <td>0.661830</td>\n",
       "      <td>0.860000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.023100</td>\n",
       "      <td>0.880786</td>\n",
       "      <td>0.840000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.031500</td>\n",
       "      <td>0.841625</td>\n",
       "      <td>0.840000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.029600</td>\n",
       "      <td>0.873401</td>\n",
       "      <td>0.850000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.018100</td>\n",
       "      <td>0.850367</td>\n",
       "      <td>0.850000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.016400</td>\n",
       "      <td>0.841859</td>\n",
       "      <td>0.840000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSequenceMultiClassification.forward` and have been ignored: duration, id. If duration, id are not expected by `Wav2Vec2ForSequenceMultiClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to out/checkpoint-100\n",
      "Configuration saved in out/checkpoint-100/config.json\n",
      "Model weights saved in out/checkpoint-100/pytorch_model.bin\n",
      "Feature extractor saved in out/checkpoint-100/preprocessor_config.json\n",
      "Deleting older checkpoint [out/checkpoint-1700] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSequenceMultiClassification.forward` and have been ignored: duration, id. If duration, id are not expected by `Wav2Vec2ForSequenceMultiClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to out/checkpoint-200\n",
      "Configuration saved in out/checkpoint-200/config.json\n",
      "Model weights saved in out/checkpoint-200/pytorch_model.bin\n",
      "Feature extractor saved in out/checkpoint-200/preprocessor_config.json\n",
      "Deleting older checkpoint [out/checkpoint-1900] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSequenceMultiClassification.forward` and have been ignored: duration, id. If duration, id are not expected by `Wav2Vec2ForSequenceMultiClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to out/checkpoint-300\n",
      "Configuration saved in out/checkpoint-300/config.json\n",
      "Model weights saved in out/checkpoint-300/pytorch_model.bin\n",
      "Feature extractor saved in out/checkpoint-300/preprocessor_config.json\n",
      "Deleting older checkpoint [out/checkpoint-2000] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSequenceMultiClassification.forward` and have been ignored: duration, id. If duration, id are not expected by `Wav2Vec2ForSequenceMultiClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to out/checkpoint-400\n",
      "Configuration saved in out/checkpoint-400/config.json\n",
      "Model weights saved in out/checkpoint-400/pytorch_model.bin\n",
      "Feature extractor saved in out/checkpoint-400/preprocessor_config.json\n",
      "Deleting older checkpoint [out/checkpoint-100] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSequenceMultiClassification.forward` and have been ignored: duration, id. If duration, id are not expected by `Wav2Vec2ForSequenceMultiClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to out/checkpoint-500\n",
      "Configuration saved in out/checkpoint-500/config.json\n",
      "Model weights saved in out/checkpoint-500/pytorch_model.bin\n",
      "Feature extractor saved in out/checkpoint-500/preprocessor_config.json\n",
      "Deleting older checkpoint [out/checkpoint-200] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSequenceMultiClassification.forward` and have been ignored: duration, id. If duration, id are not expected by `Wav2Vec2ForSequenceMultiClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to out/checkpoint-600\n",
      "Configuration saved in out/checkpoint-600/config.json\n",
      "Model weights saved in out/checkpoint-600/pytorch_model.bin\n",
      "Feature extractor saved in out/checkpoint-600/preprocessor_config.json\n",
      "Deleting older checkpoint [out/checkpoint-300] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSequenceMultiClassification.forward` and have been ignored: duration, id. If duration, id are not expected by `Wav2Vec2ForSequenceMultiClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to out/checkpoint-700\n",
      "Configuration saved in out/checkpoint-700/config.json\n",
      "Model weights saved in out/checkpoint-700/pytorch_model.bin\n",
      "Feature extractor saved in out/checkpoint-700/preprocessor_config.json\n",
      "Deleting older checkpoint [out/checkpoint-400] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSequenceMultiClassification.forward` and have been ignored: duration, id. If duration, id are not expected by `Wav2Vec2ForSequenceMultiClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to out/checkpoint-800\n",
      "Configuration saved in out/checkpoint-800/config.json\n",
      "Model weights saved in out/checkpoint-800/pytorch_model.bin\n",
      "Feature extractor saved in out/checkpoint-800/preprocessor_config.json\n",
      "Deleting older checkpoint [out/checkpoint-500] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSequenceMultiClassification.forward` and have been ignored: duration, id. If duration, id are not expected by `Wav2Vec2ForSequenceMultiClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to out/checkpoint-900\n",
      "Configuration saved in out/checkpoint-900/config.json\n",
      "Model weights saved in out/checkpoint-900/pytorch_model.bin\n",
      "Feature extractor saved in out/checkpoint-900/preprocessor_config.json\n",
      "Deleting older checkpoint [out/checkpoint-600] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSequenceMultiClassification.forward` and have been ignored: duration, id. If duration, id are not expected by `Wav2Vec2ForSequenceMultiClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to out/checkpoint-1000\n",
      "Configuration saved in out/checkpoint-1000/config.json\n",
      "Model weights saved in out/checkpoint-1000/pytorch_model.bin\n",
      "Feature extractor saved in out/checkpoint-1000/preprocessor_config.json\n",
      "Deleting older checkpoint [out/checkpoint-700] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSequenceMultiClassification.forward` and have been ignored: duration, id. If duration, id are not expected by `Wav2Vec2ForSequenceMultiClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to out/checkpoint-1100\n",
      "Configuration saved in out/checkpoint-1100/config.json\n",
      "Model weights saved in out/checkpoint-1100/pytorch_model.bin\n",
      "Feature extractor saved in out/checkpoint-1100/preprocessor_config.json\n",
      "Deleting older checkpoint [out/checkpoint-900] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSequenceMultiClassification.forward` and have been ignored: duration, id. If duration, id are not expected by `Wav2Vec2ForSequenceMultiClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to out/checkpoint-1200\n",
      "Configuration saved in out/checkpoint-1200/config.json\n",
      "Model weights saved in out/checkpoint-1200/pytorch_model.bin\n",
      "Feature extractor saved in out/checkpoint-1200/preprocessor_config.json\n",
      "Deleting older checkpoint [out/checkpoint-1000] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSequenceMultiClassification.forward` and have been ignored: duration, id. If duration, id are not expected by `Wav2Vec2ForSequenceMultiClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to out/checkpoint-1300\n",
      "Configuration saved in out/checkpoint-1300/config.json\n",
      "Model weights saved in out/checkpoint-1300/pytorch_model.bin\n",
      "Feature extractor saved in out/checkpoint-1300/preprocessor_config.json\n",
      "Deleting older checkpoint [out/checkpoint-1100] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSequenceMultiClassification.forward` and have been ignored: duration, id. If duration, id are not expected by `Wav2Vec2ForSequenceMultiClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to out/checkpoint-1400\n",
      "Configuration saved in out/checkpoint-1400/config.json\n",
      "Model weights saved in out/checkpoint-1400/pytorch_model.bin\n",
      "Feature extractor saved in out/checkpoint-1400/preprocessor_config.json\n",
      "Deleting older checkpoint [out/checkpoint-1200] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSequenceMultiClassification.forward` and have been ignored: duration, id. If duration, id are not expected by `Wav2Vec2ForSequenceMultiClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to out/checkpoint-1500\n",
      "Configuration saved in out/checkpoint-1500/config.json\n",
      "Model weights saved in out/checkpoint-1500/pytorch_model.bin\n",
      "Feature extractor saved in out/checkpoint-1500/preprocessor_config.json\n",
      "Deleting older checkpoint [out/checkpoint-1300] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSequenceMultiClassification.forward` and have been ignored: duration, id. If duration, id are not expected by `Wav2Vec2ForSequenceMultiClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to out/checkpoint-1600\n",
      "Configuration saved in out/checkpoint-1600/config.json\n",
      "Model weights saved in out/checkpoint-1600/pytorch_model.bin\n",
      "Feature extractor saved in out/checkpoint-1600/preprocessor_config.json\n",
      "Deleting older checkpoint [out/checkpoint-1400] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSequenceMultiClassification.forward` and have been ignored: duration, id. If duration, id are not expected by `Wav2Vec2ForSequenceMultiClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to out/checkpoint-1700\n",
      "Configuration saved in out/checkpoint-1700/config.json\n",
      "Model weights saved in out/checkpoint-1700/pytorch_model.bin\n",
      "Feature extractor saved in out/checkpoint-1700/preprocessor_config.json\n",
      "Deleting older checkpoint [out/checkpoint-1500] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSequenceMultiClassification.forward` and have been ignored: duration, id. If duration, id are not expected by `Wav2Vec2ForSequenceMultiClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to out/checkpoint-1800\n",
      "Configuration saved in out/checkpoint-1800/config.json\n",
      "Model weights saved in out/checkpoint-1800/pytorch_model.bin\n",
      "Feature extractor saved in out/checkpoint-1800/preprocessor_config.json\n",
      "Deleting older checkpoint [out/checkpoint-1600] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSequenceMultiClassification.forward` and have been ignored: duration, id. If duration, id are not expected by `Wav2Vec2ForSequenceMultiClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to out/checkpoint-1900\n",
      "Configuration saved in out/checkpoint-1900/config.json\n",
      "Model weights saved in out/checkpoint-1900/pytorch_model.bin\n",
      "Feature extractor saved in out/checkpoint-1900/preprocessor_config.json\n",
      "Deleting older checkpoint [out/checkpoint-1700] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSequenceMultiClassification.forward` and have been ignored: duration, id. If duration, id are not expected by `Wav2Vec2ForSequenceMultiClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to out/checkpoint-2000\n",
      "Configuration saved in out/checkpoint-2000/config.json\n",
      "Model weights saved in out/checkpoint-2000/pytorch_model.bin\n",
      "Feature extractor saved in out/checkpoint-2000/preprocessor_config.json\n",
      "Deleting older checkpoint [out/checkpoint-1800] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from out/checkpoint-800 (score: 0.86).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bc7eb7a660146b189a2a5c666b1d876",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.002 MB of 0.030 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=0.079292…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>▂▃▂▁▅▄▇█▇▇▅▆▆▇█▇▇██▇</td></tr><tr><td>eval/loss</td><td>▄▃▃▄▂▄▂▁▂▂█▄▅▂▂▅▄▅▄▄</td></tr><tr><td>eval/runtime</td><td>█▁▁█▁▁▁▁▁▂▁█████████</td></tr><tr><td>eval/samples_per_second</td><td>▁██▁█████▆█▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval/steps_per_second</td><td>▁██▁█████▆█▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>train/global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>train/learning_rate</td><td>███▇▇▇▇▇▇▆▆▆▆▆▆▅▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▁▁</td></tr><tr><td>train/loss</td><td>███▇▆▆▆▆▅▅▅▄▄▄▄▃▃▂▃▂▃▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>0.84</td></tr><tr><td>eval/loss</td><td>0.84186</td></tr><tr><td>eval/runtime</td><td>5.9526</td></tr><tr><td>eval/samples_per_second</td><td>16.799</td></tr><tr><td>eval/steps_per_second</td><td>1.176</td></tr><tr><td>train/epoch</td><td>20.0</td></tr><tr><td>train/global_step</td><td>2000</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.0164</td></tr><tr><td>train/total_flos</td><td>1.245292480666616e+18</td></tr><tr><td>train/train_loss</td><td>0.32393</td></tr><tr><td>train/train_runtime</td><td>1123.2244</td></tr><tr><td>train/train_samples_per_second</td><td>14.227</td></tr><tr><td>train/train_steps_per_second</td><td>1.781</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">wav2vec2-fnt-c256-d0-20230223-210654</strong> at: <a href='https://wandb.ai/astockman/music-classification-aii/runs/91c7rz12' target=\"_blank\">https://wandb.ai/astockman/music-classification-aii/runs/91c7rz12</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230223_210655-91c7rz12/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to res/models/wav2vec2-fnt-c256-d0-20230223-210654\n",
      "Configuration saved in res/models/wav2vec2-fnt-c256-d0-20230223-210654/config.json\n",
      "Model weights saved in res/models/wav2vec2-fnt-c256-d0-20230223-210654/pytorch_model.bin\n",
      "Feature extractor saved in res/models/wav2vec2-fnt-c256-d0-20230223-210654/preprocessor_config.json\n"
     ]
    }
   ],
   "source": [
    "run_name = get_run_name(TRAINING_CONFIG)\n",
    "model = get_model(TRAINING_CONFIG, prepared_ds[\"train\"])\n",
    "\n",
    "trainer = get_trainer(\n",
    "    run_name=run_name,\n",
    "    model=model,\n",
    "    train_ds=prepared_ds[\"train\"],\n",
    "    eval_ds=prepared_ds[\"valid\"],\n",
    "    training_config=TRAINING_CONFIG,\n",
    "    output_dir=\"out\",\n",
    "    debug=False,\n",
    "    env=NOTEBOOK_ENV,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "end_training(run_name, trainer, MODELS_DIR_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_CONFIG[\"freeze_encoder\"] = False\n",
    "TRAINING_CONFIG[\"classifier_layers\"] = [256, 256]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/alesssandros/.cache/huggingface/hub/models--facebook--wav2vec2-base/snapshots/0b5b8e868dd84f03fd87d01f9c4ff0f080fecfe8/config.json\n",
      "/home/alesssandros/.local/share/virtualenvs/aii-EtAbm6lw/lib/python3.10/site-packages/transformers/configuration_utils.py:375: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "Model config Wav2Vec2Config {\n",
      "  \"_name_or_path\": \"facebook/wav2vec2-base\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"adapter_kernel_size\": 3,\n",
      "  \"adapter_stride\": 2,\n",
      "  \"add_adapter\": false,\n",
      "  \"apply_spec_augment\": true,\n",
      "  \"architectures\": [\n",
      "    \"Wav2Vec2ForPreTraining\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"classifier_proj_size\": 256,\n",
      "  \"codevector_dim\": 256,\n",
      "  \"contrastive_logits_temperature\": 0.1,\n",
      "  \"conv_bias\": false,\n",
      "  \"conv_dim\": [\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512\n",
      "  ],\n",
      "  \"conv_kernel\": [\n",
      "    10,\n",
      "    3,\n",
      "    3,\n",
      "    3,\n",
      "    3,\n",
      "    2,\n",
      "    2\n",
      "  ],\n",
      "  \"conv_stride\": [\n",
      "    5,\n",
      "    2,\n",
      "    2,\n",
      "    2,\n",
      "    2,\n",
      "    2,\n",
      "    2\n",
      "  ],\n",
      "  \"ctc_loss_reduction\": \"sum\",\n",
      "  \"ctc_zero_infinity\": false,\n",
      "  \"diversity_loss_weight\": 0.1,\n",
      "  \"do_stable_layer_norm\": false,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"feat_extract_activation\": \"gelu\",\n",
      "  \"feat_extract_norm\": \"group\",\n",
      "  \"feat_proj_dropout\": 0.1,\n",
      "  \"feat_quantizer_dropout\": 0.0,\n",
      "  \"final_dropout\": 0.0,\n",
      "  \"freeze_feat_extract_train\": true,\n",
      "  \"gradient_checkpointing\": true,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"layerdrop\": 0.0,\n",
      "  \"mask_channel_length\": 10,\n",
      "  \"mask_channel_min_space\": 1,\n",
      "  \"mask_channel_other\": 0.0,\n",
      "  \"mask_channel_prob\": 0.0,\n",
      "  \"mask_channel_selection\": \"static\",\n",
      "  \"mask_feature_length\": 10,\n",
      "  \"mask_feature_min_masks\": 0,\n",
      "  \"mask_feature_prob\": 0.0,\n",
      "  \"mask_time_length\": 10,\n",
      "  \"mask_time_min_masks\": 2,\n",
      "  \"mask_time_min_space\": 1,\n",
      "  \"mask_time_other\": 0.0,\n",
      "  \"mask_time_prob\": 0.05,\n",
      "  \"mask_time_selection\": \"static\",\n",
      "  \"model_type\": \"wav2vec2\",\n",
      "  \"no_mask_channel_overlap\": false,\n",
      "  \"no_mask_time_overlap\": false,\n",
      "  \"num_adapter_layers\": 3,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_codevector_groups\": 2,\n",
      "  \"num_codevectors_per_group\": 320,\n",
      "  \"num_conv_pos_embedding_groups\": 16,\n",
      "  \"num_conv_pos_embeddings\": 128,\n",
      "  \"num_feat_extract_layers\": 7,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_negatives\": 100,\n",
      "  \"output_hidden_size\": 768,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"proj_codevector_dim\": 256,\n",
      "  \"tdnn_dilation\": [\n",
      "    1,\n",
      "    2,\n",
      "    3,\n",
      "    1,\n",
      "    1\n",
      "  ],\n",
      "  \"tdnn_dim\": [\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    1500\n",
      "  ],\n",
      "  \"tdnn_kernel\": [\n",
      "    5,\n",
      "    3,\n",
      "    3,\n",
      "    1,\n",
      "    1\n",
      "  ],\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"use_weighted_layer_sum\": false,\n",
      "  \"vocab_size\": 32,\n",
      "  \"xvector_output_dim\": 512\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/alesssandros/.cache/huggingface/hub/models--facebook--wav2vec2-base/snapshots/0b5b8e868dd84f03fd87d01f9c4ff0f080fecfe8/pytorch_model.bin\n",
      "Some weights of the model checkpoint at facebook/wav2vec2-base were not used when initializing Wav2Vec2ForSequenceMultiClassification: ['quantizer.weight_proj.bias', 'project_hid.bias', 'project_hid.weight', 'project_q.weight', 'quantizer.weight_proj.weight', 'quantizer.codevectors', 'project_q.bias']\n",
      "- This IS expected if you are initializing Wav2Vec2ForSequenceMultiClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2ForSequenceMultiClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2ForSequenceMultiClassification were not initialized from the model checkpoint at facebook/wav2vec2-base and are newly initialized: ['head.layers.1.bias', 'classifier.weight', 'head.layers.0.bias', 'projector.weight', 'head.layers.1.weight', 'head.layers.0.weight', 'projector.bias', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "loading configuration file preprocessor_config.json from cache at /home/alesssandros/.cache/huggingface/hub/models--facebook--wav2vec2-base/snapshots/0b5b8e868dd84f03fd87d01f9c4ff0f080fecfe8/preprocessor_config.json\n",
      "loading configuration file config.json from cache at /home/alesssandros/.cache/huggingface/hub/models--facebook--wav2vec2-base/snapshots/0b5b8e868dd84f03fd87d01f9c4ff0f080fecfe8/config.json\n",
      "Model config Wav2Vec2Config {\n",
      "  \"_name_or_path\": \"facebook/wav2vec2-base\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"adapter_kernel_size\": 3,\n",
      "  \"adapter_stride\": 2,\n",
      "  \"add_adapter\": false,\n",
      "  \"apply_spec_augment\": true,\n",
      "  \"architectures\": [\n",
      "    \"Wav2Vec2ForPreTraining\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"classifier_proj_size\": 256,\n",
      "  \"codevector_dim\": 256,\n",
      "  \"contrastive_logits_temperature\": 0.1,\n",
      "  \"conv_bias\": false,\n",
      "  \"conv_dim\": [\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512\n",
      "  ],\n",
      "  \"conv_kernel\": [\n",
      "    10,\n",
      "    3,\n",
      "    3,\n",
      "    3,\n",
      "    3,\n",
      "    2,\n",
      "    2\n",
      "  ],\n",
      "  \"conv_stride\": [\n",
      "    5,\n",
      "    2,\n",
      "    2,\n",
      "    2,\n",
      "    2,\n",
      "    2,\n",
      "    2\n",
      "  ],\n",
      "  \"ctc_loss_reduction\": \"sum\",\n",
      "  \"ctc_zero_infinity\": false,\n",
      "  \"diversity_loss_weight\": 0.1,\n",
      "  \"do_stable_layer_norm\": false,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"feat_extract_activation\": \"gelu\",\n",
      "  \"feat_extract_norm\": \"group\",\n",
      "  \"feat_proj_dropout\": 0.1,\n",
      "  \"feat_quantizer_dropout\": 0.0,\n",
      "  \"final_dropout\": 0.0,\n",
      "  \"freeze_feat_extract_train\": true,\n",
      "  \"gradient_checkpointing\": true,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"layerdrop\": 0.0,\n",
      "  \"mask_channel_length\": 10,\n",
      "  \"mask_channel_min_space\": 1,\n",
      "  \"mask_channel_other\": 0.0,\n",
      "  \"mask_channel_prob\": 0.0,\n",
      "  \"mask_channel_selection\": \"static\",\n",
      "  \"mask_feature_length\": 10,\n",
      "  \"mask_feature_min_masks\": 0,\n",
      "  \"mask_feature_prob\": 0.0,\n",
      "  \"mask_time_length\": 10,\n",
      "  \"mask_time_min_masks\": 2,\n",
      "  \"mask_time_min_space\": 1,\n",
      "  \"mask_time_other\": 0.0,\n",
      "  \"mask_time_prob\": 0.05,\n",
      "  \"mask_time_selection\": \"static\",\n",
      "  \"model_type\": \"wav2vec2\",\n",
      "  \"no_mask_channel_overlap\": false,\n",
      "  \"no_mask_time_overlap\": false,\n",
      "  \"num_adapter_layers\": 3,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_codevector_groups\": 2,\n",
      "  \"num_codevectors_per_group\": 320,\n",
      "  \"num_conv_pos_embedding_groups\": 16,\n",
      "  \"num_conv_pos_embeddings\": 128,\n",
      "  \"num_feat_extract_layers\": 7,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_negatives\": 100,\n",
      "  \"output_hidden_size\": 768,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"proj_codevector_dim\": 256,\n",
      "  \"tdnn_dilation\": [\n",
      "    1,\n",
      "    2,\n",
      "    3,\n",
      "    1,\n",
      "    1\n",
      "  ],\n",
      "  \"tdnn_dim\": [\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    1500\n",
      "  ],\n",
      "  \"tdnn_kernel\": [\n",
      "    5,\n",
      "    3,\n",
      "    3,\n",
      "    1,\n",
      "    1\n",
      "  ],\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"use_weighted_layer_sum\": false,\n",
      "  \"vocab_size\": 32,\n",
      "  \"xvector_output_dim\": 512\n",
      "}\n",
      "\n",
      "Feature extractor Wav2Vec2FeatureExtractor {\n",
      "  \"do_normalize\": true,\n",
      "  \"feature_extractor_type\": \"Wav2Vec2FeatureExtractor\",\n",
      "  \"feature_size\": 1,\n",
      "  \"padding_side\": \"right\",\n",
      "  \"padding_value\": 0.0,\n",
      "  \"return_attention_mask\": false,\n",
      "  \"sampling_rate\": 16000\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/alesssandros/dev/FCN_Newspaper/aii/wandb/run-20230223_212554-qx4d2zow</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/astockman/music-classification-aii/runs/qx4d2zow' target=\"_blank\">wav2vec2-fnt-c256_256-d0-20230223-212551</a></strong> to <a href='https://wandb.ai/astockman/music-classification-aii' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/astockman/music-classification-aii' target=\"_blank\">https://wandb.ai/astockman/music-classification-aii</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/astockman/music-classification-aii/runs/qx4d2zow' target=\"_blank\">https://wandb.ai/astockman/music-classification-aii/runs/qx4d2zow</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The following columns in the training set don't have a corresponding argument in `Wav2Vec2ForSequenceMultiClassification.forward` and have been ignored: duration, id. If duration, id are not expected by `Wav2Vec2ForSequenceMultiClassification.forward`,  you can safely ignore this message.\n",
      "/home/alesssandros/.local/share/virtualenvs/aii-EtAbm6lw/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 799\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 2000\n",
      "  Number of trainable parameters = 94832003\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2000' max='2000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2000/2000 17:02, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.082300</td>\n",
       "      <td>1.037547</td>\n",
       "      <td>0.530000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.062000</td>\n",
       "      <td>1.112393</td>\n",
       "      <td>0.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.075800</td>\n",
       "      <td>1.069584</td>\n",
       "      <td>0.370000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.091000</td>\n",
       "      <td>1.084402</td>\n",
       "      <td>0.320000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.094600</td>\n",
       "      <td>1.089886</td>\n",
       "      <td>0.310000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.086100</td>\n",
       "      <td>1.060857</td>\n",
       "      <td>0.360000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.046200</td>\n",
       "      <td>1.003772</td>\n",
       "      <td>0.530000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.054500</td>\n",
       "      <td>0.990443</td>\n",
       "      <td>0.550000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.037300</td>\n",
       "      <td>0.988915</td>\n",
       "      <td>0.550000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.003500</td>\n",
       "      <td>1.078376</td>\n",
       "      <td>0.410000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>1.066800</td>\n",
       "      <td>1.044170</td>\n",
       "      <td>0.430000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>1.041100</td>\n",
       "      <td>0.986541</td>\n",
       "      <td>0.480000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>1.017100</td>\n",
       "      <td>0.911391</td>\n",
       "      <td>0.560000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>1.029400</td>\n",
       "      <td>1.003259</td>\n",
       "      <td>0.520000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1.059000</td>\n",
       "      <td>1.065440</td>\n",
       "      <td>0.450000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>1.038100</td>\n",
       "      <td>1.032187</td>\n",
       "      <td>0.480000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>1.082100</td>\n",
       "      <td>1.021938</td>\n",
       "      <td>0.490000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>1.079500</td>\n",
       "      <td>1.002644</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>1.079200</td>\n",
       "      <td>0.973916</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.068300</td>\n",
       "      <td>0.972488</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSequenceMultiClassification.forward` and have been ignored: duration, id. If duration, id are not expected by `Wav2Vec2ForSequenceMultiClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to out/checkpoint-100\n",
      "Configuration saved in out/checkpoint-100/config.json\n",
      "Model weights saved in out/checkpoint-100/pytorch_model.bin\n",
      "Feature extractor saved in out/checkpoint-100/preprocessor_config.json\n",
      "Deleting older checkpoint [out/checkpoint-800] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSequenceMultiClassification.forward` and have been ignored: duration, id. If duration, id are not expected by `Wav2Vec2ForSequenceMultiClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to out/checkpoint-200\n",
      "Configuration saved in out/checkpoint-200/config.json\n",
      "Model weights saved in out/checkpoint-200/pytorch_model.bin\n",
      "Feature extractor saved in out/checkpoint-200/preprocessor_config.json\n",
      "Deleting older checkpoint [out/checkpoint-1900] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSequenceMultiClassification.forward` and have been ignored: duration, id. If duration, id are not expected by `Wav2Vec2ForSequenceMultiClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to out/checkpoint-300\n",
      "Configuration saved in out/checkpoint-300/config.json\n",
      "Model weights saved in out/checkpoint-300/pytorch_model.bin\n",
      "Feature extractor saved in out/checkpoint-300/preprocessor_config.json\n",
      "Deleting older checkpoint [out/checkpoint-2000] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSequenceMultiClassification.forward` and have been ignored: duration, id. If duration, id are not expected by `Wav2Vec2ForSequenceMultiClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to out/checkpoint-400\n",
      "Configuration saved in out/checkpoint-400/config.json\n",
      "Model weights saved in out/checkpoint-400/pytorch_model.bin\n",
      "Feature extractor saved in out/checkpoint-400/preprocessor_config.json\n",
      "Deleting older checkpoint [out/checkpoint-200] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSequenceMultiClassification.forward` and have been ignored: duration, id. If duration, id are not expected by `Wav2Vec2ForSequenceMultiClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to out/checkpoint-500\n",
      "Configuration saved in out/checkpoint-500/config.json\n",
      "Model weights saved in out/checkpoint-500/pytorch_model.bin\n",
      "Feature extractor saved in out/checkpoint-500/preprocessor_config.json\n",
      "Deleting older checkpoint [out/checkpoint-300] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSequenceMultiClassification.forward` and have been ignored: duration, id. If duration, id are not expected by `Wav2Vec2ForSequenceMultiClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to out/checkpoint-600\n",
      "Configuration saved in out/checkpoint-600/config.json\n",
      "Model weights saved in out/checkpoint-600/pytorch_model.bin\n",
      "Feature extractor saved in out/checkpoint-600/preprocessor_config.json\n",
      "Deleting older checkpoint [out/checkpoint-400] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSequenceMultiClassification.forward` and have been ignored: duration, id. If duration, id are not expected by `Wav2Vec2ForSequenceMultiClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to out/checkpoint-700\n",
      "Configuration saved in out/checkpoint-700/config.json\n",
      "Model weights saved in out/checkpoint-700/pytorch_model.bin\n",
      "Feature extractor saved in out/checkpoint-700/preprocessor_config.json\n",
      "Deleting older checkpoint [out/checkpoint-500] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSequenceMultiClassification.forward` and have been ignored: duration, id. If duration, id are not expected by `Wav2Vec2ForSequenceMultiClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to out/checkpoint-800\n",
      "Configuration saved in out/checkpoint-800/config.json\n",
      "Model weights saved in out/checkpoint-800/pytorch_model.bin\n",
      "Feature extractor saved in out/checkpoint-800/preprocessor_config.json\n",
      "Deleting older checkpoint [out/checkpoint-100] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSequenceMultiClassification.forward` and have been ignored: duration, id. If duration, id are not expected by `Wav2Vec2ForSequenceMultiClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to out/checkpoint-900\n",
      "Configuration saved in out/checkpoint-900/config.json\n",
      "Model weights saved in out/checkpoint-900/pytorch_model.bin\n",
      "Feature extractor saved in out/checkpoint-900/preprocessor_config.json\n",
      "Deleting older checkpoint [out/checkpoint-600] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSequenceMultiClassification.forward` and have been ignored: duration, id. If duration, id are not expected by `Wav2Vec2ForSequenceMultiClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to out/checkpoint-1000\n",
      "Configuration saved in out/checkpoint-1000/config.json\n",
      "Model weights saved in out/checkpoint-1000/pytorch_model.bin\n",
      "Feature extractor saved in out/checkpoint-1000/preprocessor_config.json\n",
      "Deleting older checkpoint [out/checkpoint-700] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSequenceMultiClassification.forward` and have been ignored: duration, id. If duration, id are not expected by `Wav2Vec2ForSequenceMultiClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to out/checkpoint-1100\n",
      "Configuration saved in out/checkpoint-1100/config.json\n",
      "Model weights saved in out/checkpoint-1100/pytorch_model.bin\n",
      "Feature extractor saved in out/checkpoint-1100/preprocessor_config.json\n",
      "Deleting older checkpoint [out/checkpoint-900] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSequenceMultiClassification.forward` and have been ignored: duration, id. If duration, id are not expected by `Wav2Vec2ForSequenceMultiClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to out/checkpoint-1200\n",
      "Configuration saved in out/checkpoint-1200/config.json\n",
      "Model weights saved in out/checkpoint-1200/pytorch_model.bin\n",
      "Feature extractor saved in out/checkpoint-1200/preprocessor_config.json\n",
      "Deleting older checkpoint [out/checkpoint-1000] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSequenceMultiClassification.forward` and have been ignored: duration, id. If duration, id are not expected by `Wav2Vec2ForSequenceMultiClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to out/checkpoint-1300\n",
      "Configuration saved in out/checkpoint-1300/config.json\n",
      "Model weights saved in out/checkpoint-1300/pytorch_model.bin\n",
      "Feature extractor saved in out/checkpoint-1300/preprocessor_config.json\n",
      "Deleting older checkpoint [out/checkpoint-800] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSequenceMultiClassification.forward` and have been ignored: duration, id. If duration, id are not expected by `Wav2Vec2ForSequenceMultiClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to out/checkpoint-1400\n",
      "Configuration saved in out/checkpoint-1400/config.json\n",
      "Model weights saved in out/checkpoint-1400/pytorch_model.bin\n",
      "Feature extractor saved in out/checkpoint-1400/preprocessor_config.json\n",
      "Deleting older checkpoint [out/checkpoint-1100] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSequenceMultiClassification.forward` and have been ignored: duration, id. If duration, id are not expected by `Wav2Vec2ForSequenceMultiClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to out/checkpoint-1500\n",
      "Configuration saved in out/checkpoint-1500/config.json\n",
      "Model weights saved in out/checkpoint-1500/pytorch_model.bin\n",
      "Feature extractor saved in out/checkpoint-1500/preprocessor_config.json\n",
      "Deleting older checkpoint [out/checkpoint-1200] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSequenceMultiClassification.forward` and have been ignored: duration, id. If duration, id are not expected by `Wav2Vec2ForSequenceMultiClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to out/checkpoint-1600\n",
      "Configuration saved in out/checkpoint-1600/config.json\n",
      "Model weights saved in out/checkpoint-1600/pytorch_model.bin\n",
      "Feature extractor saved in out/checkpoint-1600/preprocessor_config.json\n",
      "Deleting older checkpoint [out/checkpoint-1400] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSequenceMultiClassification.forward` and have been ignored: duration, id. If duration, id are not expected by `Wav2Vec2ForSequenceMultiClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to out/checkpoint-1700\n",
      "Configuration saved in out/checkpoint-1700/config.json\n",
      "Model weights saved in out/checkpoint-1700/pytorch_model.bin\n",
      "Feature extractor saved in out/checkpoint-1700/preprocessor_config.json\n",
      "Deleting older checkpoint [out/checkpoint-1500] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSequenceMultiClassification.forward` and have been ignored: duration, id. If duration, id are not expected by `Wav2Vec2ForSequenceMultiClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to out/checkpoint-1800\n",
      "Configuration saved in out/checkpoint-1800/config.json\n",
      "Model weights saved in out/checkpoint-1800/pytorch_model.bin\n",
      "Feature extractor saved in out/checkpoint-1800/preprocessor_config.json\n",
      "Deleting older checkpoint [out/checkpoint-1600] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSequenceMultiClassification.forward` and have been ignored: duration, id. If duration, id are not expected by `Wav2Vec2ForSequenceMultiClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to out/checkpoint-1900\n",
      "Configuration saved in out/checkpoint-1900/config.json\n",
      "Model weights saved in out/checkpoint-1900/pytorch_model.bin\n",
      "Feature extractor saved in out/checkpoint-1900/preprocessor_config.json\n",
      "Deleting older checkpoint [out/checkpoint-1700] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSequenceMultiClassification.forward` and have been ignored: duration, id. If duration, id are not expected by `Wav2Vec2ForSequenceMultiClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to out/checkpoint-2000\n",
      "Configuration saved in out/checkpoint-2000/config.json\n",
      "Model weights saved in out/checkpoint-2000/pytorch_model.bin\n",
      "Feature extractor saved in out/checkpoint-2000/preprocessor_config.json\n",
      "Deleting older checkpoint [out/checkpoint-1800] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from out/checkpoint-1300 (score: 0.56).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9ee7c41af5f490c95e8eb8bf4f99bc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.002 MB of 0.028 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=0.085927…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>▇▄▃▁▁▂▇██▄▄▆█▇▅▆▆▆▆▆</td></tr><tr><td>eval/loss</td><td>▅█▇▇▇▆▄▄▄▇▆▄▁▄▆▅▅▄▃▃</td></tr><tr><td>eval/runtime</td><td>▄▁▃█▁▁▁▁██▁▄▁▁▁▁████</td></tr><tr><td>eval/samples_per_second</td><td>▄█▅▁████▁▁█▄████▁▁▁▁</td></tr><tr><td>eval/steps_per_second</td><td>▄█▅▁████▁▁█▄████▁▁▁▁</td></tr><tr><td>train/epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>train/global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>train/learning_rate</td><td>███▇▇▇▇▇▇▆▆▆▆▆▆▅▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▁▁</td></tr><tr><td>train/loss</td><td>█▇▇▅▆▆▇▆█▇██▇▆▅▅▅▄▄▃▂▇▆▆▄▁▃▅▄▄▆▇▄▅▇▅▇▅▇▆</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>0.5</td></tr><tr><td>eval/loss</td><td>0.97249</td></tr><tr><td>eval/runtime</td><td>5.8737</td></tr><tr><td>eval/samples_per_second</td><td>17.025</td></tr><tr><td>eval/steps_per_second</td><td>1.192</td></tr><tr><td>train/epoch</td><td>20.0</td></tr><tr><td>train/global_step</td><td>2000</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>1.0683</td></tr><tr><td>train/total_flos</td><td>1.2461570323039926e+18</td></tr><tr><td>train/train_loss</td><td>1.05787</td></tr><tr><td>train/train_runtime</td><td>1022.5948</td></tr><tr><td>train/train_samples_per_second</td><td>15.627</td></tr><tr><td>train/train_steps_per_second</td><td>1.956</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">wav2vec2-fnt-c256_256-d0-20230223-212551</strong> at: <a href='https://wandb.ai/astockman/music-classification-aii/runs/qx4d2zow' target=\"_blank\">https://wandb.ai/astockman/music-classification-aii/runs/qx4d2zow</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230223_212554-qx4d2zow/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to res/models/wav2vec2-fnt-c256_256-d0-20230223-212551\n",
      "Configuration saved in res/models/wav2vec2-fnt-c256_256-d0-20230223-212551/config.json\n",
      "Model weights saved in res/models/wav2vec2-fnt-c256_256-d0-20230223-212551/pytorch_model.bin\n",
      "Feature extractor saved in res/models/wav2vec2-fnt-c256_256-d0-20230223-212551/preprocessor_config.json\n"
     ]
    }
   ],
   "source": [
    "run_name = get_run_name(TRAINING_CONFIG)\n",
    "model = get_model(TRAINING_CONFIG, prepared_ds[\"train\"])\n",
    "\n",
    "trainer = get_trainer(\n",
    "    run_name=run_name,\n",
    "    model=model,\n",
    "    train_ds=prepared_ds[\"train\"],\n",
    "    eval_ds=prepared_ds[\"valid\"],\n",
    "    training_config=TRAINING_CONFIG,\n",
    "    output_dir=\"out\",\n",
    "    debug=False,\n",
    "    env=NOTEBOOK_ENV,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "end_training(run_name, trainer, MODELS_DIR_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5Z6ixINFyTA7"
   },
   "source": [
    "## Whisper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_CONFIG[\"feature_encoder\"] = \"whisper\"\n",
    "TRAINING_CONFIG[\"freeze_encoder\"] = True\n",
    "TRAINING_CONFIG[\"classifier_layers\"] = [256]\n",
    "TRAINING_CONFIG[\"classifier_dropout\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file preprocessor_config.json from cache at /home/alesssandros/.cache/huggingface/hub/models--openai--whisper-tiny/snapshots/302560528ac75a251232980ebcc68bad9668f664/preprocessor_config.json\n",
      "Feature extractor WhisperFeatureExtractor {\n",
      "  \"chunk_length\": 30,\n",
      "  \"feature_extractor_type\": \"WhisperFeatureExtractor\",\n",
      "  \"feature_size\": 80,\n",
      "  \"hop_length\": 160,\n",
      "  \"mel_filters\": [\n",
      "    [\n",
      "      -0.0,\n",
      "      0.02486259490251541,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0\n",
      "    ],\n",
      "    [\n",
      "      0.0,\n",
      "      0.001990821911022067,\n",
      "      0.022871771827340126,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0\n",
      "    ],\n",
      "    [\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.003981643822044134,\n",
      "      0.02088095061480999,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0\n",
      "    ],\n",
      "    [\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0059724655002355576,\n",
      "      0.018890129402279854,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0\n",
      "    ],\n",
      "    [\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.007963287644088268,\n",
      "      0.01689930632710457,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0\n",
      "    ],\n",
      "    [\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.009954108856618404,\n",
      "      0.014908484183251858,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0\n",
      "    ],\n",
      "    [\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.011944931000471115,\n",
      "      0.012917662039399147,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0\n",
      "    ],\n",
      "    [\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.013935752213001251,\n",
      "      0.010926840826869011,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0\n",
      "    ],\n",
      "    [\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.015926575288176537,\n",
      "      0.0089360186830163,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0\n",
      "    ],\n",
      "    [\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.017917396500706673,\n",
      "      0.006945197004824877,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0\n",
      "    ],\n",
      "    [\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.01990821771323681,\n",
      "      0.004954374860972166,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0\n",
      "    ],\n",
      "    [\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.021899040788412094,\n",
      "      0.0029635531827807426,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0\n",
      "    ],\n",
      "    [\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.02388986200094223,\n",
      "      0.0009727313299663365,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0\n",
      "    ],\n",
      "    [\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.025880683213472366,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0\n",
      "    ],\n",
      "    [\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.025835324078798294,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0\n",
      "    ],\n",
      "    [\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0010180906392633915,\n",
      "      0.023844502866268158,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0\n",
      "    ],\n",
      "    [\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.003008912317454815,\n",
      "      0.021853681653738022,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0\n",
      "    ],\n",
      "    [\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.004999734461307526,\n",
      "      0.019862858578562737,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0\n",
      "    ],\n",
      "    [\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.006990555673837662,\n",
      "      0.0178720373660326,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0\n",
      "    ],\n",
      "    [\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.008981377817690372,\n",
      "      0.015881216153502464,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0\n",
      "    ],\n",
      "    [\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.010972199961543083,\n",
      "      0.013890394009649754,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0\n",
      "    ],\n",
      "    [\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.01296302117407322,\n",
      "      0.011899571865797043,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0\n",
      "    ],\n",
      "    [\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.01495384331792593,\n",
      "      0.009908749721944332,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0\n",
      "    ],\n",
      "    [\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.01694466546177864,\n",
      "      0.007917927578091621,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0\n",
      "    ],\n",
      "    [\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.018935488536953926,\n",
      "      0.005927106365561485,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0\n",
      "    ],\n",
      "    [\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.020874010398983955,\n",
      "      0.004040425643324852,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0\n",
      "    ],\n",
      "    [\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.022114217281341553,\n",
      "      0.0033186059445142746,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0\n",
      "    ],\n",
      "    [\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.02173672430217266,\n",
      "      0.0036109676584601402,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0\n",
      "    ],\n",
      "    [\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.020497702062129974,\n",
      "      0.004762193653732538,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0\n",
      "    ],\n",
      "    [\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.018486659973859787,\n",
      "      0.006592618301510811,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0\n",
      "    ],\n",
      "    [\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.01585603691637516,\n",
      "      0.00896277092397213,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0\n",
      "    ],\n",
      "    [\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.012738768011331558,\n",
      "      0.011751330457627773,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0\n",
      "    ],\n",
      "    [\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.009250369854271412,\n",
      "      0.014853144995868206,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0\n",
      "    ],\n",
      "    [\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.005490840878337622,\n",
      "      0.018177473917603493,\n",
      "      0.0028155462350696325,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0\n",
      "    ],\n",
      "    [\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0015463664894923568,\n",
      "      0.01632951945066452,\n",
      "      0.007420188747346401,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0\n",
      "    ],\n",
      "    [\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.011181050911545753,\n",
      "      0.012018864043056965,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0\n",
      "    ],\n",
      "    [\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.006065350491553545,\n",
      "      0.016561277210712433,\n",
      "      0.004360878840088844,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0\n",
      "    ],\n",
      "    [\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0010297985281795263,\n",
      "      0.012770536355674267,\n",
      "      0.009707189165055752,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0\n",
      "    ],\n",
      "    [\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.006986402906477451,\n",
      "      0.01485429983586073,\n",
      "      0.004391219466924667,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0\n",
      "    ],\n",
      "    [\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.001418047584593296,\n",
      "      0.011486922390758991,\n",
      "      0.010089744813740253,\n",
      "      0.00040022286702878773,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0\n",
      "    ],\n",
      "    [\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.005411104764789343,\n",
      "      0.014735566452145576,\n",
      "      0.006518189795315266,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0\n",
      "    ],\n",
      "    [\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.00827841367572546,\n",
      "      0.012277561239898205,\n",
      "      0.00396781275048852,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0\n",
      "    ],\n",
      "    [\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.002187808509916067,\n",
      "      0.010184479877352715,\n",
      "      0.00998187530785799,\n",
      "      0.0022864851634949446,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0\n",
      "    ],\n",
      "    [\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.00386943481862545,\n",
      "      0.011274894699454308,\n",
      "      0.008466221392154694,\n",
      "      0.0013397691072896123,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0\n",
      "    ],\n",
      "    [\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.004820294212549925,\n",
      "      0.011678251437842846,\n",
      "      0.007608682848513126,\n",
      "      0.0010091039584949613,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0\n",
      "    ],\n",
      "    [\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.005156961735337973,\n",
      "      0.011507894843816757,\n",
      "      0.007301822770386934,\n",
      "      0.0011901655234396458,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0\n",
      "    ],\n",
      "    [\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.004982104524970055,\n",
      "      0.010863498784601688,\n",
      "      0.007451189681887627,\n",
      "      0.001791381393559277,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0\n",
      "    ],\n",
      "    [\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.004385921638458967,\n",
      "      0.009832492098212242,\n",
      "      0.007973956875503063,\n",
      "      0.002732589840888977,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0\n",
      "    ],\n",
      "    [\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0034474546555429697,\n",
      "      0.008491347543895245,\n",
      "      0.008797688409686089,\n",
      "      0.00394382793456316,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0\n",
      "    ],\n",
      "    [\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0022357646375894547,\n",
      "      0.0069067515432834625,\n",
      "      0.009859241545200348,\n",
      "      0.005364237818866968,\n",
      "      0.0008692338014952838,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0\n",
      "    ],\n",
      "    [\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0008110002381727099,\n",
      "      0.005136650986969471,\n",
      "      0.00946230161935091,\n",
      "      0.0069410777650773525,\n",
      "      0.0027783995028585196,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0\n",
      "    ],\n",
      "    [\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.003231203882023692,\n",
      "      0.007237049750983715,\n",
      "      0.00862883497029543,\n",
      "      0.004773912951350212,\n",
      "      0.0009189908159896731,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0\n",
      "    ],\n",
      "    [\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.001233637798577547,\n",
      "      0.0049433219246566296,\n",
      "      0.008653006516397,\n",
      "      0.006818502210080624,\n",
      "      0.003248583758249879,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0\n",
      "    ],\n",
      "    [\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0026164355222135782,\n",
      "      0.006051854696124792,\n",
      "      0.008880467154085636,\n",
      "      0.005574479699134827,\n",
      "      0.002268492942675948,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0\n",
      "    ],\n",
      "    [\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0002863667905330658,\n",
      "      0.003467798000201583,\n",
      "      0.0066492292098701,\n",
      "      0.00787146482616663,\n",
      "      0.004809896927326918,\n",
      "      0.0017483289120718837,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0\n",
      "    ],\n",
      "    [\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0009245910914614797,\n",
      "      0.0038708120118826628,\n",
      "      0.00681703258305788,\n",
      "      0.007283343467861414,\n",
      "      0.004448124207556248,\n",
      "      0.0016129047144204378,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0\n",
      "    ],\n",
      "    [\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0011703289346769452,\n",
      "      0.003898728871718049,\n",
      "      0.006627128925174475,\n",
      "      0.0070473202504217625,\n",
      "      0.004421714693307877,\n",
      "      0.0017961094854399562,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0\n",
      "    ],\n",
      "    [\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0010892992140725255,\n",
      "      0.003615982597693801,\n",
      "      0.006142666097730398,\n",
      "      0.007102936040610075,\n",
      "      0.004671447444707155,\n",
      "      0.002239959081634879,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0\n",
      "    ],\n",
      "    [\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0007392280967906117,\n",
      "      0.0030791081953793764,\n",
      "      0.005418988410383463,\n",
      "      0.007397185545414686,\n",
      "      0.005145462695509195,\n",
      "      0.002893739379942417,\n",
      "      0.0006420162972062826,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0\n",
      "    ],\n",
      "    [\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.00017068670422304422,\n",
      "      0.0023375742603093386,\n",
      "      0.004504461772739887,\n",
      "      0.0066713495180010796,\n",
      "      0.005798479542136192,\n",
      "      0.003713231300935149,\n",
      "      0.0016279831761494279,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0\n",
      "    ],\n",
      "    [\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0014345343224704266,\n",
      "      0.0034412189852446318,\n",
      "      0.005447904113680124,\n",
      "      0.006591092795133591,\n",
      "      0.004660011734813452,\n",
      "      0.002728930441662669,\n",
      "      0.0007978491485118866,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0\n",
      "    ],\n",
      "    [\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0004075043834745884,\n",
      "      0.002265830524265766,\n",
      "      0.004124156199395657,\n",
      "      0.005982482805848122,\n",
      "      0.005700822453945875,\n",
      "      0.003912510350346565,\n",
      "      0.0021241982467472553,\n",
      "      0.0003358862304594368,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0\n",
      "    ],\n",
      "    [\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0010099108330905437,\n",
      "      0.002730846870690584,\n",
      "      0.004451782442629337,\n",
      "      0.006172718480229378,\n",
      "      0.005150905344635248,\n",
      "      0.0034948070533573627,\n",
      "      0.0018387088784947991,\n",
      "      0.0001826105872169137,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0\n",
      "    ],\n",
      "    [\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0012943691108375788,\n",
      "      0.002888072282075882,\n",
      "      0.004481775686144829,\n",
      "      0.006075479090213776,\n",
      "      0.0048866597935557365,\n",
      "      0.003353001084178686,\n",
      "      0.0018193417927250266,\n",
      "      0.00028568264679051936,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0\n",
      "    ],\n",
      "    [\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0013131388695910573,\n",
      "      0.0027890161145478487,\n",
      "      0.004264893010258675,\n",
      "      0.0057407706044614315,\n",
      "      0.004859979264438152,\n",
      "      0.0034397069830447435,\n",
      "      0.0020194342359900475,\n",
      "      0.0005991620710119605,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0\n",
      "    ],\n",
      "    [\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0011121684219688177,\n",
      "      0.002478930866345763,\n",
      "      0.0038456933107227087,\n",
      "      0.0052124555222690105,\n",
      "      0.005028639920055866,\n",
      "      0.0037133716978132725,\n",
      "      0.002398103242740035,\n",
      "      0.0010828346712514758,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0\n",
      "    ],\n",
      "    [\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0007317548734135926,\n",
      "      0.0019974694587290287,\n",
      "      0.003263183869421482,\n",
      "      0.004528898745775223,\n",
      "      0.005355686880648136,\n",
      "      0.004137659445405006,\n",
      "      0.0029196315445005894,\n",
      "      0.0017016039928421378,\n",
      "      0.0004835762665607035,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0\n",
      "    ],\n",
      "    [\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.00020713974663522094,\n",
      "      0.0013792773243039846,\n",
      "      0.0025514145381748676,\n",
      "      0.003723552217707038,\n",
      "      0.004895689897239208,\n",
      "      0.004680895246565342,\n",
      "      0.0035529187880456448,\n",
      "      0.0024249425623565912,\n",
      "      0.0012969663366675377,\n",
      "      0.00016899015463422984,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0\n",
      "    ],\n",
      "    [\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0006545265205204487,\n",
      "      0.0017400053329765797,\n",
      "      0.0028254841454327106,\n",
      "      0.003910962492227554,\n",
      "      0.004996441304683685,\n",
      "      0.0042709787376224995,\n",
      "      0.003226396394893527,\n",
      "      0.002181813819333911,\n",
      "      0.0011372314766049385,\n",
      "      9.264905384043232e-05,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0\n",
      "    ],\n",
      "    [\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.000854626705404371,\n",
      "      0.001859853626228869,\n",
      "      0.002865080488845706,\n",
      "      0.003870307235047221,\n",
      "      0.00487553421407938,\n",
      "      0.00408313749358058,\n",
      "      0.003115783678367734,\n",
      "      0.0021484296303242445,\n",
      "      0.001181075582280755,\n",
      "      0.0002137213887181133,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0\n",
      "    ],\n",
      "    [\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0008483415003865957,\n",
      "      0.0017792496364563704,\n",
      "      0.0027101580053567886,\n",
      "      0.0036410661414265633,\n",
      "      0.004571974277496338,\n",
      "      0.004079728852957487,\n",
      "      0.003183893393725157,\n",
      "      0.002288057701662183,\n",
      "      0.0013922222424298525,\n",
      "      0.0004963868414051831,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0\n",
      "    ],\n",
      "    [\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0006716204807162285,\n",
      "      0.0015337044605985284,\n",
      "      0.002395788673311472,\n",
      "      0.0032578727696090937,\n",
      "      0.004119956865906715,\n",
      "      0.004227725323289633,\n",
      "      0.0033981208689510822,\n",
      "      0.0025685166474431753,\n",
      "      0.0017389123095199466,\n",
      "      0.0009093079133890569,\n",
      "      7.970355363795534e-05,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0\n",
      "    ],\n",
      "    [\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0003559796023182571,\n",
      "      0.0011543278815224767,\n",
      "      0.0019526762189343572,\n",
      "      0.002751024439930916,\n",
      "      0.0035493727773427963,\n",
      "      0.004347721114754677,\n",
      "      0.0037299629766494036,\n",
      "      0.002961693098768592,\n",
      "      0.00219342322088778,\n",
      "      0.0014251532265916467,\n",
      "      0.0006568834069184959,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0\n",
      "    ],\n",
      "    [\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0006682946113869548,\n",
      "      0.0014076193328946829,\n",
      "      0.0021469437051564455,\n",
      "      0.002886268775910139,\n",
      "      0.0036255933810025454,\n",
      "      0.004154576454311609,\n",
      "      0.0034431067761033773,\n",
      "      0.0027316368650645018,\n",
      "      0.0020201667211949825,\n",
      "      0.0013086966937407851,\n",
      "      0.0005972267827019095,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0\n",
      "    ],\n",
      "    [\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      9.926508937496692e-05,\n",
      "      0.0007839298341423273,\n",
      "      0.001468594535253942,\n",
      "      0.0021532592363655567,\n",
      "      0.0028379240538924932,\n",
      "      0.0035225888714194298,\n",
      "      0.0039915177039802074,\n",
      "      0.0033326479606330395,\n",
      "      0.002673778682947159,\n",
      "      0.002014909405261278,\n",
      "      0.0013560398947447538,\n",
      "      0.0006971705006435513,\n",
      "      3.8301113818306476e-05,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0\n",
      "    ],\n",
      "    [\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.00010181095422012731,\n",
      "      0.0007358568836934865,\n",
      "      0.0013699028640985489,\n",
      "      0.0020039486698806286,\n",
      "      0.002637994708493352,\n",
      "      0.0032720407471060753,\n",
      "      0.003906086552888155,\n",
      "      0.0033682563807815313,\n",
      "      0.0027580985333770514,\n",
      "      0.002147940918803215,\n",
      "      0.0015377833042293787,\n",
      "      0.0009276255150325596,\n",
      "      0.000317467754939571,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0\n",
      "    ],\n",
      "    [\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0005530364578589797,\n",
      "      0.0011402058880776167,\n",
      "      0.0017273754347115755,\n",
      "      0.0023145449813455343,\n",
      "      0.002901714527979493,\n",
      "      0.003488884074613452,\n",
      "      0.003523340215906501,\n",
      "      0.002958292607218027,\n",
      "      0.002393245231360197,\n",
      "      0.0018281979719176888,\n",
      "      0.001263150479644537,\n",
      "      0.0006981031037867069,\n",
      "      0.0001330557424807921,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0\n",
      "    ],\n",
      "    [\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0002608386566862464,\n",
      "      0.0008045974536798894,\n",
      "      0.0013483562506735325,\n",
      "      0.0018921148730441928,\n",
      "      0.0024358737282454967,\n",
      "      0.002979632467031479,\n",
      "      0.003523391205817461,\n",
      "      0.003251380519941449,\n",
      "      0.0027281083166599274,\n",
      "      0.002204835880547762,\n",
      "      0.001681563793681562,\n",
      "      0.001158291706815362,\n",
      "      0.0006350195035338402,\n",
      "      0.00011174729297636077,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0\n",
      "    ],\n",
      "    [\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0003849811910185963,\n",
      "      0.0008885387214832008,\n",
      "      0.001392096164636314,\n",
      "      0.0018956535495817661,\n",
      "      0.00239921105094254,\n",
      "      0.002902768552303314,\n",
      "      0.0034063260536640882,\n",
      "      0.003132763085886836,\n",
      "      0.0026481777895241976,\n",
      "      0.0021635922603309155,\n",
      "      0.0016790067311376333,\n",
      "      0.0011944210855290294,\n",
      "      0.0007098356145434082,\n",
      "      0.00022525011445395648,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0\n",
      "    ],\n",
      "    [\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.0,\n",
      "      0.000366741674952209,\n",
      "      0.0008330700220540166,\n",
      "      0.0012993983691558242,\n",
      "      0.0017657268326729536,\n",
      "      0.0022320549469441175,\n",
      "      0.002698383294045925,\n",
      "      0.0031647118739783764,\n",
      "      0.003141313325613737,\n",
      "      0.002692554146051407,\n",
      "      0.0022437951993197203,\n",
      "      0.00179503601975739,\n",
      "      0.0013462770730257034,\n",
      "      0.000897518009878695,\n",
      "      0.0004487590049393475,\n",
      "      0.0\n",
      "    ]\n",
      "  ],\n",
      "  \"n_fft\": 400,\n",
      "  \"n_samples\": 480000,\n",
      "  \"nb_max_frames\": 3000,\n",
      "  \"padding_side\": \"right\",\n",
      "  \"padding_value\": 0.0,\n",
      "  \"processor_class\": \"WhisperProcessor\",\n",
      "  \"return_attention_mask\": false,\n",
      "  \"sampling_rate\": 16000\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing extra columns from dataset\n",
      "Mapping features clusters\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "610c55f4768c4b2589632b28efff5202",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/20636 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting train split\n",
      "Extracting valid split\n",
      "Extracting test split\n",
      "Create `ClassLabels` for target classes\n",
      "{'genre': ClassLabel(names=['Electronic', 'Rock/Blues', 'World/Ethnic'], id=None)}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2643de5a631444e4bc073aa4dd585937",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting the dataset:   0%|          | 0/799 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cd47d91165d4a74a7db66d2b625df99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting the dataset:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bba11ec6df1470dbc91324282344b9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting the dataset:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['label', 'id', 'duration', 'input_features'],\n",
       "        num_rows: 799\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['label', 'id', 'duration', 'input_features'],\n",
       "        num_rows: 100\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['label', 'id', 'duration', 'input_features'],\n",
       "        num_rows: 100\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prepared_ds = load_and_prepare_ds(TRAINING_CONFIG, FEATURES_CONFIG_SUBSET, df)\n",
    "\n",
    "prepared_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frozen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/alesssandros/.cache/huggingface/hub/models--openai--whisper-tiny/snapshots/302560528ac75a251232980ebcc68bad9668f664/config.json\n",
      "Model config WhisperConfig {\n",
      "  \"_name_or_path\": \"openai/whisper-tiny\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"WhisperForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"begin_suppress_tokens\": [\n",
      "    220,\n",
      "    50257\n",
      "  ],\n",
      "  \"bos_token_id\": 50257,\n",
      "  \"d_model\": 384,\n",
      "  \"decoder_attention_heads\": 6,\n",
      "  \"decoder_ffn_dim\": 1536,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 4,\n",
      "  \"decoder_start_token_id\": 50258,\n",
      "  \"dropout\": 0.0,\n",
      "  \"encoder_attention_heads\": 6,\n",
      "  \"encoder_ffn_dim\": 1536,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 4,\n",
      "  \"eos_token_id\": 50257,\n",
      "  \"forced_decoder_ids\": [\n",
      "    [\n",
      "      1,\n",
      "      50259\n",
      "    ],\n",
      "    [\n",
      "      2,\n",
      "      50359\n",
      "    ],\n",
      "    [\n",
      "      3,\n",
      "      50363\n",
      "    ]\n",
      "  ],\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"max_length\": 448,\n",
      "  \"max_source_positions\": 1500,\n",
      "  \"max_target_positions\": 448,\n",
      "  \"model_type\": \"whisper\",\n",
      "  \"num_hidden_layers\": 4,\n",
      "  \"num_mel_bins\": 80,\n",
      "  \"pad_token_id\": 50257,\n",
      "  \"scale_embedding\": false,\n",
      "  \"suppress_tokens\": [\n",
      "    1,\n",
      "    2,\n",
      "    7,\n",
      "    8,\n",
      "    9,\n",
      "    10,\n",
      "    14,\n",
      "    25,\n",
      "    26,\n",
      "    27,\n",
      "    28,\n",
      "    29,\n",
      "    31,\n",
      "    58,\n",
      "    59,\n",
      "    60,\n",
      "    61,\n",
      "    62,\n",
      "    63,\n",
      "    90,\n",
      "    91,\n",
      "    92,\n",
      "    93,\n",
      "    359,\n",
      "    503,\n",
      "    522,\n",
      "    542,\n",
      "    873,\n",
      "    893,\n",
      "    902,\n",
      "    918,\n",
      "    922,\n",
      "    931,\n",
      "    1350,\n",
      "    1853,\n",
      "    1982,\n",
      "    2460,\n",
      "    2627,\n",
      "    3246,\n",
      "    3253,\n",
      "    3268,\n",
      "    3536,\n",
      "    3846,\n",
      "    3961,\n",
      "    4183,\n",
      "    4667,\n",
      "    6585,\n",
      "    6647,\n",
      "    7273,\n",
      "    9061,\n",
      "    9383,\n",
      "    10428,\n",
      "    10929,\n",
      "    11938,\n",
      "    12033,\n",
      "    12331,\n",
      "    12562,\n",
      "    13793,\n",
      "    14157,\n",
      "    14635,\n",
      "    15265,\n",
      "    15618,\n",
      "    16553,\n",
      "    16604,\n",
      "    18362,\n",
      "    18956,\n",
      "    20075,\n",
      "    21675,\n",
      "    22520,\n",
      "    26130,\n",
      "    26161,\n",
      "    26435,\n",
      "    28279,\n",
      "    29464,\n",
      "    31650,\n",
      "    32302,\n",
      "    32470,\n",
      "    36865,\n",
      "    42863,\n",
      "    47425,\n",
      "    49870,\n",
      "    50254,\n",
      "    50258,\n",
      "    50360,\n",
      "    50361,\n",
      "    50362\n",
      "  ],\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 51865\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/alesssandros/.cache/huggingface/hub/models--openai--whisper-tiny/snapshots/302560528ac75a251232980ebcc68bad9668f664/pytorch_model.bin\n",
      "Some weights of the model checkpoint at openai/whisper-tiny were not used when initializing WhisperForSequenceClassification: ['model.decoder.layers.3.fc2.weight', 'model.decoder.layers.3.self_attn.out_proj.weight', 'model.decoder.layers.0.fc1.bias', 'model.decoder.embed_positions.weight', 'model.decoder.layers.0.self_attn.q_proj.weight', 'model.decoder.layers.0.self_attn.out_proj.weight', 'model.decoder.layers.2.encoder_attn_layer_norm.weight', 'model.decoder.layers.1.self_attn.v_proj.weight', 'model.decoder.layers.2.self_attn_layer_norm.bias', 'model.decoder.layers.0.encoder_attn_layer_norm.bias', 'model.decoder.layers.2.self_attn.out_proj.weight', 'model.decoder.layers.1.self_attn.out_proj.bias', 'model.decoder.layers.1.encoder_attn_layer_norm.weight', 'model.decoder.layers.2.encoder_attn.v_proj.bias', 'model.decoder.layers.0.self_attn_layer_norm.weight', 'model.decoder.layers.3.final_layer_norm.weight', 'model.decoder.layers.3.fc1.weight', 'model.decoder.layers.0.self_attn.v_proj.weight', 'model.decoder.layers.0.encoder_attn.out_proj.bias', 'model.decoder.layers.3.encoder_attn.k_proj.weight', 'model.decoder.layers.3.encoder_attn_layer_norm.weight', 'model.decoder.layers.2.fc2.weight', 'model.decoder.layers.0.final_layer_norm.bias', 'model.decoder.layers.3.self_attn_layer_norm.weight', 'model.decoder.layers.2.fc2.bias', 'model.decoder.layers.3.encoder_attn.v_proj.bias', 'model.decoder.layers.1.final_layer_norm.weight', 'model.decoder.layers.2.self_attn.v_proj.bias', 'model.decoder.layers.0.encoder_attn.out_proj.weight', 'model.decoder.layers.0.encoder_attn.v_proj.bias', 'model.decoder.layers.1.encoder_attn.q_proj.weight', 'model.decoder.layers.1.encoder_attn_layer_norm.bias', 'model.decoder.layers.2.self_attn.v_proj.weight', 'model.decoder.layers.1.encoder_attn.out_proj.bias', 'model.decoder.layers.0.fc1.weight', 'model.decoder.layers.3.self_attn_layer_norm.bias', 'model.decoder.layers.1.self_attn_layer_norm.bias', 'model.decoder.layers.2.self_attn.k_proj.weight', 'model.decoder.layers.2.self_attn.q_proj.bias', 'model.decoder.layers.2.final_layer_norm.weight', 'model.decoder.layers.0.self_attn.out_proj.bias', 'model.decoder.layers.0.encoder_attn.v_proj.weight', 'model.decoder.layers.1.final_layer_norm.bias', 'model.decoder.layers.1.fc2.bias', 'model.decoder.layer_norm.weight', 'model.decoder.layers.2.encoder_attn_layer_norm.bias', 'model.decoder.layers.2.encoder_attn.out_proj.weight', 'model.decoder.layers.0.self_attn.k_proj.weight', 'model.decoder.layers.2.encoder_attn.q_proj.bias', 'model.decoder.layers.0.fc2.bias', 'model.decoder.layers.1.fc2.weight', 'model.decoder.layers.1.fc1.weight', 'model.decoder.layers.3.self_attn.v_proj.bias', 'model.decoder.layers.3.self_attn.k_proj.weight', 'model.decoder.layers.3.encoder_attn.q_proj.weight', 'model.decoder.layers.0.fc2.weight', 'model.decoder.layers.0.encoder_attn.q_proj.bias', 'model.decoder.layers.2.fc1.weight', 'model.decoder.layers.1.encoder_attn.q_proj.bias', 'model.decoder.layers.1.self_attn_layer_norm.weight', 'model.decoder.layers.3.fc1.bias', 'model.decoder.layers.2.self_attn_layer_norm.weight', 'model.decoder.layers.3.self_attn.q_proj.bias', 'model.decoder.layers.2.fc1.bias', 'model.decoder.layers.2.encoder_attn.v_proj.weight', 'model.decoder.embed_tokens.weight', 'model.decoder.layers.0.encoder_attn.q_proj.weight', 'model.decoder.layers.1.self_attn.q_proj.bias', 'model.decoder.layers.1.self_attn.q_proj.weight', 'model.decoder.layers.1.encoder_attn.v_proj.bias', 'model.decoder.layers.1.self_attn.v_proj.bias', 'model.decoder.layers.0.self_attn.v_proj.bias', 'model.decoder.layers.3.self_attn.out_proj.bias', 'model.decoder.layers.3.self_attn.q_proj.weight', 'model.decoder.layers.3.encoder_attn.v_proj.weight', 'model.decoder.layers.1.encoder_attn.v_proj.weight', 'model.decoder.layers.3.encoder_attn_layer_norm.bias', 'model.decoder.layers.3.final_layer_norm.bias', 'model.decoder.layers.1.fc1.bias', 'model.decoder.layers.3.fc2.bias', 'model.decoder.layers.1.self_attn.k_proj.weight', 'model.decoder.layers.3.self_attn.v_proj.weight', 'model.decoder.layers.1.self_attn.out_proj.weight', 'model.decoder.layers.0.self_attn_layer_norm.bias', 'model.decoder.layers.2.encoder_attn.out_proj.bias', 'model.decoder.layers.0.encoder_attn_layer_norm.weight', 'model.decoder.layers.0.self_attn.q_proj.bias', 'model.decoder.layers.1.encoder_attn.out_proj.weight', 'model.decoder.layers.2.encoder_attn.k_proj.weight', 'model.decoder.layers.2.self_attn.out_proj.bias', 'model.decoder.layers.3.encoder_attn.q_proj.bias', 'model.decoder.layers.2.self_attn.q_proj.weight', 'model.decoder.layers.3.encoder_attn.out_proj.bias', 'model.decoder.layer_norm.bias', 'model.decoder.layers.2.final_layer_norm.bias', 'model.decoder.layers.2.encoder_attn.q_proj.weight', 'model.decoder.layers.0.final_layer_norm.weight', 'model.decoder.layers.3.encoder_attn.out_proj.weight', 'model.decoder.layers.1.encoder_attn.k_proj.weight', 'model.decoder.layers.0.encoder_attn.k_proj.weight']\n",
      "- This IS expected if you are initializing WhisperForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing WhisperForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of WhisperForSequenceClassification were not initialized from the model checkpoint at openai/whisper-tiny and are newly initialized: ['model.head.layers.0.bias', 'model.classifier.weight', 'model.head.layers.0.weight', 'model.classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/alesssandros/dev/FCN_Newspaper/aii/wandb/run-20230223_214405-8xnhru4f</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/astockman/music-classification-aii/runs/8xnhru4f' target=\"_blank\">whisper-frz-c256-d0-20230223-214404</a></strong> to <a href='https://wandb.ai/astockman/music-classification-aii' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/astockman/music-classification-aii' target=\"_blank\">https://wandb.ai/astockman/music-classification-aii</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/astockman/music-classification-aii/runs/8xnhru4f' target=\"_blank\">https://wandb.ai/astockman/music-classification-aii/runs/8xnhru4f</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The following columns in the training set don't have a corresponding argument in `WhisperForSequenceClassification.forward` and have been ignored: duration, id. If duration, id are not expected by `WhisperForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/home/alesssandros/.local/share/virtualenvs/aii-EtAbm6lw/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 799\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 2000\n",
      "  Number of trainable parameters = 99331\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2000' max='2000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2000/2000 13:24, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.094000</td>\n",
       "      <td>1.087617</td>\n",
       "      <td>0.550000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.086300</td>\n",
       "      <td>1.084868</td>\n",
       "      <td>0.410000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.075400</td>\n",
       "      <td>1.073304</td>\n",
       "      <td>0.550000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.068600</td>\n",
       "      <td>1.060855</td>\n",
       "      <td>0.530000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.056000</td>\n",
       "      <td>1.053237</td>\n",
       "      <td>0.520000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.046300</td>\n",
       "      <td>1.038378</td>\n",
       "      <td>0.540000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.041100</td>\n",
       "      <td>1.024477</td>\n",
       "      <td>0.580000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.030300</td>\n",
       "      <td>1.016008</td>\n",
       "      <td>0.580000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.020500</td>\n",
       "      <td>1.010600</td>\n",
       "      <td>0.520000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.021100</td>\n",
       "      <td>0.995507</td>\n",
       "      <td>0.580000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>1.009900</td>\n",
       "      <td>0.993104</td>\n",
       "      <td>0.560000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>1.004500</td>\n",
       "      <td>0.986266</td>\n",
       "      <td>0.540000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.981500</td>\n",
       "      <td>0.981570</td>\n",
       "      <td>0.520000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>1.005000</td>\n",
       "      <td>0.976028</td>\n",
       "      <td>0.540000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.989600</td>\n",
       "      <td>0.973068</td>\n",
       "      <td>0.560000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.971800</td>\n",
       "      <td>0.968248</td>\n",
       "      <td>0.560000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.989500</td>\n",
       "      <td>0.966099</td>\n",
       "      <td>0.560000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.982200</td>\n",
       "      <td>0.964476</td>\n",
       "      <td>0.560000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.973100</td>\n",
       "      <td>0.963878</td>\n",
       "      <td>0.570000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.986800</td>\n",
       "      <td>0.963726</td>\n",
       "      <td>0.580000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `WhisperForSequenceClassification.forward` and have been ignored: duration, id. If duration, id are not expected by `WhisperForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to out/checkpoint-100\n",
      "Configuration saved in out/checkpoint-100/config.json\n",
      "Model weights saved in out/checkpoint-100/pytorch_model.bin\n",
      "Feature extractor saved in out/checkpoint-100/preprocessor_config.json\n",
      "Deleting older checkpoint [out/checkpoint-1300] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `WhisperForSequenceClassification.forward` and have been ignored: duration, id. If duration, id are not expected by `WhisperForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to out/checkpoint-200\n",
      "Configuration saved in out/checkpoint-200/config.json\n",
      "Model weights saved in out/checkpoint-200/pytorch_model.bin\n",
      "Feature extractor saved in out/checkpoint-200/preprocessor_config.json\n",
      "Deleting older checkpoint [out/checkpoint-1900] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `WhisperForSequenceClassification.forward` and have been ignored: duration, id. If duration, id are not expected by `WhisperForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to out/checkpoint-300\n",
      "Configuration saved in out/checkpoint-300/config.json\n",
      "Model weights saved in out/checkpoint-300/pytorch_model.bin\n",
      "Feature extractor saved in out/checkpoint-300/preprocessor_config.json\n",
      "Deleting older checkpoint [out/checkpoint-2000] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `WhisperForSequenceClassification.forward` and have been ignored: duration, id. If duration, id are not expected by `WhisperForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to out/checkpoint-400\n",
      "Configuration saved in out/checkpoint-400/config.json\n",
      "Model weights saved in out/checkpoint-400/pytorch_model.bin\n",
      "Feature extractor saved in out/checkpoint-400/preprocessor_config.json\n",
      "Deleting older checkpoint [out/checkpoint-200] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `WhisperForSequenceClassification.forward` and have been ignored: duration, id. If duration, id are not expected by `WhisperForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to out/checkpoint-500\n",
      "Configuration saved in out/checkpoint-500/config.json\n",
      "Model weights saved in out/checkpoint-500/pytorch_model.bin\n",
      "Feature extractor saved in out/checkpoint-500/preprocessor_config.json\n",
      "Deleting older checkpoint [out/checkpoint-300] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `WhisperForSequenceClassification.forward` and have been ignored: duration, id. If duration, id are not expected by `WhisperForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to out/checkpoint-600\n",
      "Configuration saved in out/checkpoint-600/config.json\n",
      "Model weights saved in out/checkpoint-600/pytorch_model.bin\n",
      "Feature extractor saved in out/checkpoint-600/preprocessor_config.json\n",
      "Deleting older checkpoint [out/checkpoint-400] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `WhisperForSequenceClassification.forward` and have been ignored: duration, id. If duration, id are not expected by `WhisperForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to out/checkpoint-700\n",
      "Configuration saved in out/checkpoint-700/config.json\n",
      "Model weights saved in out/checkpoint-700/pytorch_model.bin\n",
      "Feature extractor saved in out/checkpoint-700/preprocessor_config.json\n",
      "Deleting older checkpoint [out/checkpoint-100] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `WhisperForSequenceClassification.forward` and have been ignored: duration, id. If duration, id are not expected by `WhisperForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to out/checkpoint-800\n",
      "Configuration saved in out/checkpoint-800/config.json\n",
      "Model weights saved in out/checkpoint-800/pytorch_model.bin\n",
      "Feature extractor saved in out/checkpoint-800/preprocessor_config.json\n",
      "Deleting older checkpoint [out/checkpoint-500] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `WhisperForSequenceClassification.forward` and have been ignored: duration, id. If duration, id are not expected by `WhisperForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to out/checkpoint-900\n",
      "Configuration saved in out/checkpoint-900/config.json\n",
      "Model weights saved in out/checkpoint-900/pytorch_model.bin\n",
      "Feature extractor saved in out/checkpoint-900/preprocessor_config.json\n",
      "Deleting older checkpoint [out/checkpoint-600] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `WhisperForSequenceClassification.forward` and have been ignored: duration, id. If duration, id are not expected by `WhisperForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to out/checkpoint-1000\n",
      "Configuration saved in out/checkpoint-1000/config.json\n",
      "Model weights saved in out/checkpoint-1000/pytorch_model.bin\n",
      "Feature extractor saved in out/checkpoint-1000/preprocessor_config.json\n",
      "Deleting older checkpoint [out/checkpoint-800] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `WhisperForSequenceClassification.forward` and have been ignored: duration, id. If duration, id are not expected by `WhisperForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to out/checkpoint-1100\n",
      "Configuration saved in out/checkpoint-1100/config.json\n",
      "Model weights saved in out/checkpoint-1100/pytorch_model.bin\n",
      "Feature extractor saved in out/checkpoint-1100/preprocessor_config.json\n",
      "Deleting older checkpoint [out/checkpoint-900] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `WhisperForSequenceClassification.forward` and have been ignored: duration, id. If duration, id are not expected by `WhisperForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to out/checkpoint-1200\n",
      "Configuration saved in out/checkpoint-1200/config.json\n",
      "Model weights saved in out/checkpoint-1200/pytorch_model.bin\n",
      "Feature extractor saved in out/checkpoint-1200/preprocessor_config.json\n",
      "Deleting older checkpoint [out/checkpoint-1000] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `WhisperForSequenceClassification.forward` and have been ignored: duration, id. If duration, id are not expected by `WhisperForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to out/checkpoint-1300\n",
      "Configuration saved in out/checkpoint-1300/config.json\n",
      "Model weights saved in out/checkpoint-1300/pytorch_model.bin\n",
      "Feature extractor saved in out/checkpoint-1300/preprocessor_config.json\n",
      "Deleting older checkpoint [out/checkpoint-1100] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `WhisperForSequenceClassification.forward` and have been ignored: duration, id. If duration, id are not expected by `WhisperForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to out/checkpoint-1400\n",
      "Configuration saved in out/checkpoint-1400/config.json\n",
      "Model weights saved in out/checkpoint-1400/pytorch_model.bin\n",
      "Feature extractor saved in out/checkpoint-1400/preprocessor_config.json\n",
      "Deleting older checkpoint [out/checkpoint-1200] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `WhisperForSequenceClassification.forward` and have been ignored: duration, id. If duration, id are not expected by `WhisperForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to out/checkpoint-1500\n",
      "Configuration saved in out/checkpoint-1500/config.json\n",
      "Model weights saved in out/checkpoint-1500/pytorch_model.bin\n",
      "Feature extractor saved in out/checkpoint-1500/preprocessor_config.json\n",
      "Deleting older checkpoint [out/checkpoint-1300] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `WhisperForSequenceClassification.forward` and have been ignored: duration, id. If duration, id are not expected by `WhisperForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to out/checkpoint-1600\n",
      "Configuration saved in out/checkpoint-1600/config.json\n",
      "Model weights saved in out/checkpoint-1600/pytorch_model.bin\n",
      "Feature extractor saved in out/checkpoint-1600/preprocessor_config.json\n",
      "Deleting older checkpoint [out/checkpoint-1400] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `WhisperForSequenceClassification.forward` and have been ignored: duration, id. If duration, id are not expected by `WhisperForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to out/checkpoint-1700\n",
      "Configuration saved in out/checkpoint-1700/config.json\n",
      "Model weights saved in out/checkpoint-1700/pytorch_model.bin\n",
      "Feature extractor saved in out/checkpoint-1700/preprocessor_config.json\n",
      "Deleting older checkpoint [out/checkpoint-1500] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `WhisperForSequenceClassification.forward` and have been ignored: duration, id. If duration, id are not expected by `WhisperForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to out/checkpoint-1800\n",
      "Configuration saved in out/checkpoint-1800/config.json\n",
      "Model weights saved in out/checkpoint-1800/pytorch_model.bin\n",
      "Feature extractor saved in out/checkpoint-1800/preprocessor_config.json\n",
      "Deleting older checkpoint [out/checkpoint-1600] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `WhisperForSequenceClassification.forward` and have been ignored: duration, id. If duration, id are not expected by `WhisperForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to out/checkpoint-1900\n",
      "Configuration saved in out/checkpoint-1900/config.json\n",
      "Model weights saved in out/checkpoint-1900/pytorch_model.bin\n",
      "Feature extractor saved in out/checkpoint-1900/preprocessor_config.json\n",
      "Deleting older checkpoint [out/checkpoint-1700] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `WhisperForSequenceClassification.forward` and have been ignored: duration, id. If duration, id are not expected by `WhisperForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to out/checkpoint-2000\n",
      "Configuration saved in out/checkpoint-2000/config.json\n",
      "Model weights saved in out/checkpoint-2000/pytorch_model.bin\n",
      "Feature extractor saved in out/checkpoint-2000/preprocessor_config.json\n",
      "Deleting older checkpoint [out/checkpoint-1800] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from out/checkpoint-700 (score: 0.58).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60190480525c457b992ffd59d2fc2d8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>▇▁▇▆▆▆██▆█▇▆▆▆▇▇▇▇██</td></tr><tr><td>eval/loss</td><td>██▇▆▆▅▄▄▄▃▃▂▂▂▂▁▁▁▁▁</td></tr><tr><td>eval/runtime</td><td>█████████▁██▁███████</td></tr><tr><td>eval/samples_per_second</td><td>▁▁▁▁▁▁▁▁▁█▁▁█▁▁▁▁▁▁▁</td></tr><tr><td>eval/steps_per_second</td><td>▁▁▁▁▁▁▁▁▁█▁▁█▁▁▁▁▁▁▁</td></tr><tr><td>train/epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>train/global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>train/learning_rate</td><td>███▇▇▇▇▇▇▆▆▆▆▆▆▅▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▁▁</td></tr><tr><td>train/loss</td><td>███▇▇▇▇▆▆▆▆▅▅▅▅▄▄▄▄▃▄▃▃▂▃▃▂▁▃▂▂▂▁▁▂▁▂▂▁▂</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>0.58</td></tr><tr><td>eval/loss</td><td>0.96373</td></tr><tr><td>eval/runtime</td><td>4.8435</td></tr><tr><td>eval/samples_per_second</td><td>20.646</td></tr><tr><td>eval/steps_per_second</td><td>1.445</td></tr><tr><td>train/epoch</td><td>20.0</td></tr><tr><td>train/global_step</td><td>2000</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.9868</td></tr><tr><td>train/total_flos</td><td>5.9305346736e+16</td></tr><tr><td>train/train_loss</td><td>1.02172</td></tr><tr><td>train/train_runtime</td><td>804.7173</td></tr><tr><td>train/train_samples_per_second</td><td>19.858</td></tr><tr><td>train/train_steps_per_second</td><td>2.485</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">whisper-frz-c256-d0-20230223-214404</strong> at: <a href='https://wandb.ai/astockman/music-classification-aii/runs/8xnhru4f' target=\"_blank\">https://wandb.ai/astockman/music-classification-aii/runs/8xnhru4f</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230223_214405-8xnhru4f/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to res/models/whisper-frz-c256-d0-20230223-214404\n",
      "Configuration saved in res/models/whisper-frz-c256-d0-20230223-214404/config.json\n",
      "Model weights saved in res/models/whisper-frz-c256-d0-20230223-214404/pytorch_model.bin\n",
      "Feature extractor saved in res/models/whisper-frz-c256-d0-20230223-214404/preprocessor_config.json\n"
     ]
    }
   ],
   "source": [
    "run_name = get_run_name(TRAINING_CONFIG)\n",
    "model = get_model(TRAINING_CONFIG, prepared_ds[\"train\"])\n",
    "\n",
    "trainer = get_trainer(\n",
    "    run_name=run_name,\n",
    "    model=model,\n",
    "    train_ds=prepared_ds[\"train\"],\n",
    "    eval_ds=prepared_ds[\"valid\"],\n",
    "    training_config=TRAINING_CONFIG,\n",
    "    output_dir=\"out\",\n",
    "    debug=False,\n",
    "    env=NOTEBOOK_ENV,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "end_training(run_name, trainer, MODELS_DIR_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_CONFIG[\"freeze_encoder\"] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/alesssandros/.cache/huggingface/hub/models--openai--whisper-tiny/snapshots/302560528ac75a251232980ebcc68bad9668f664/config.json\n",
      "Model config WhisperConfig {\n",
      "  \"_name_or_path\": \"openai/whisper-tiny\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"WhisperForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"begin_suppress_tokens\": [\n",
      "    220,\n",
      "    50257\n",
      "  ],\n",
      "  \"bos_token_id\": 50257,\n",
      "  \"d_model\": 384,\n",
      "  \"decoder_attention_heads\": 6,\n",
      "  \"decoder_ffn_dim\": 1536,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 4,\n",
      "  \"decoder_start_token_id\": 50258,\n",
      "  \"dropout\": 0.0,\n",
      "  \"encoder_attention_heads\": 6,\n",
      "  \"encoder_ffn_dim\": 1536,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 4,\n",
      "  \"eos_token_id\": 50257,\n",
      "  \"forced_decoder_ids\": [\n",
      "    [\n",
      "      1,\n",
      "      50259\n",
      "    ],\n",
      "    [\n",
      "      2,\n",
      "      50359\n",
      "    ],\n",
      "    [\n",
      "      3,\n",
      "      50363\n",
      "    ]\n",
      "  ],\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"max_length\": 448,\n",
      "  \"max_source_positions\": 1500,\n",
      "  \"max_target_positions\": 448,\n",
      "  \"model_type\": \"whisper\",\n",
      "  \"num_hidden_layers\": 4,\n",
      "  \"num_mel_bins\": 80,\n",
      "  \"pad_token_id\": 50257,\n",
      "  \"scale_embedding\": false,\n",
      "  \"suppress_tokens\": [\n",
      "    1,\n",
      "    2,\n",
      "    7,\n",
      "    8,\n",
      "    9,\n",
      "    10,\n",
      "    14,\n",
      "    25,\n",
      "    26,\n",
      "    27,\n",
      "    28,\n",
      "    29,\n",
      "    31,\n",
      "    58,\n",
      "    59,\n",
      "    60,\n",
      "    61,\n",
      "    62,\n",
      "    63,\n",
      "    90,\n",
      "    91,\n",
      "    92,\n",
      "    93,\n",
      "    359,\n",
      "    503,\n",
      "    522,\n",
      "    542,\n",
      "    873,\n",
      "    893,\n",
      "    902,\n",
      "    918,\n",
      "    922,\n",
      "    931,\n",
      "    1350,\n",
      "    1853,\n",
      "    1982,\n",
      "    2460,\n",
      "    2627,\n",
      "    3246,\n",
      "    3253,\n",
      "    3268,\n",
      "    3536,\n",
      "    3846,\n",
      "    3961,\n",
      "    4183,\n",
      "    4667,\n",
      "    6585,\n",
      "    6647,\n",
      "    7273,\n",
      "    9061,\n",
      "    9383,\n",
      "    10428,\n",
      "    10929,\n",
      "    11938,\n",
      "    12033,\n",
      "    12331,\n",
      "    12562,\n",
      "    13793,\n",
      "    14157,\n",
      "    14635,\n",
      "    15265,\n",
      "    15618,\n",
      "    16553,\n",
      "    16604,\n",
      "    18362,\n",
      "    18956,\n",
      "    20075,\n",
      "    21675,\n",
      "    22520,\n",
      "    26130,\n",
      "    26161,\n",
      "    26435,\n",
      "    28279,\n",
      "    29464,\n",
      "    31650,\n",
      "    32302,\n",
      "    32470,\n",
      "    36865,\n",
      "    42863,\n",
      "    47425,\n",
      "    49870,\n",
      "    50254,\n",
      "    50258,\n",
      "    50360,\n",
      "    50361,\n",
      "    50362\n",
      "  ],\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 51865\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/alesssandros/.cache/huggingface/hub/models--openai--whisper-tiny/snapshots/302560528ac75a251232980ebcc68bad9668f664/pytorch_model.bin\n",
      "Some weights of the model checkpoint at openai/whisper-tiny were not used when initializing WhisperForSequenceClassification: ['model.decoder.layers.3.fc2.weight', 'model.decoder.layers.3.self_attn.out_proj.weight', 'model.decoder.layers.0.fc1.bias', 'model.decoder.embed_positions.weight', 'model.decoder.layers.0.self_attn.q_proj.weight', 'model.decoder.layers.0.self_attn.out_proj.weight', 'model.decoder.layers.2.encoder_attn_layer_norm.weight', 'model.decoder.layers.1.self_attn.v_proj.weight', 'model.decoder.layers.2.self_attn_layer_norm.bias', 'model.decoder.layers.0.encoder_attn_layer_norm.bias', 'model.decoder.layers.2.self_attn.out_proj.weight', 'model.decoder.layers.1.self_attn.out_proj.bias', 'model.decoder.layers.1.encoder_attn_layer_norm.weight', 'model.decoder.layers.2.encoder_attn.v_proj.bias', 'model.decoder.layers.0.self_attn_layer_norm.weight', 'model.decoder.layers.3.final_layer_norm.weight', 'model.decoder.layers.3.fc1.weight', 'model.decoder.layers.0.self_attn.v_proj.weight', 'model.decoder.layers.0.encoder_attn.out_proj.bias', 'model.decoder.layers.3.encoder_attn.k_proj.weight', 'model.decoder.layers.3.encoder_attn_layer_norm.weight', 'model.decoder.layers.2.fc2.weight', 'model.decoder.layers.0.final_layer_norm.bias', 'model.decoder.layers.3.self_attn_layer_norm.weight', 'model.decoder.layers.2.fc2.bias', 'model.decoder.layers.3.encoder_attn.v_proj.bias', 'model.decoder.layers.1.final_layer_norm.weight', 'model.decoder.layers.2.self_attn.v_proj.bias', 'model.decoder.layers.0.encoder_attn.out_proj.weight', 'model.decoder.layers.0.encoder_attn.v_proj.bias', 'model.decoder.layers.1.encoder_attn.q_proj.weight', 'model.decoder.layers.1.encoder_attn_layer_norm.bias', 'model.decoder.layers.2.self_attn.v_proj.weight', 'model.decoder.layers.1.encoder_attn.out_proj.bias', 'model.decoder.layers.0.fc1.weight', 'model.decoder.layers.3.self_attn_layer_norm.bias', 'model.decoder.layers.1.self_attn_layer_norm.bias', 'model.decoder.layers.2.self_attn.k_proj.weight', 'model.decoder.layers.2.self_attn.q_proj.bias', 'model.decoder.layers.2.final_layer_norm.weight', 'model.decoder.layers.0.self_attn.out_proj.bias', 'model.decoder.layers.0.encoder_attn.v_proj.weight', 'model.decoder.layers.1.final_layer_norm.bias', 'model.decoder.layers.1.fc2.bias', 'model.decoder.layer_norm.weight', 'model.decoder.layers.2.encoder_attn_layer_norm.bias', 'model.decoder.layers.2.encoder_attn.out_proj.weight', 'model.decoder.layers.0.self_attn.k_proj.weight', 'model.decoder.layers.2.encoder_attn.q_proj.bias', 'model.decoder.layers.0.fc2.bias', 'model.decoder.layers.1.fc2.weight', 'model.decoder.layers.1.fc1.weight', 'model.decoder.layers.3.self_attn.v_proj.bias', 'model.decoder.layers.3.self_attn.k_proj.weight', 'model.decoder.layers.3.encoder_attn.q_proj.weight', 'model.decoder.layers.0.fc2.weight', 'model.decoder.layers.0.encoder_attn.q_proj.bias', 'model.decoder.layers.2.fc1.weight', 'model.decoder.layers.1.encoder_attn.q_proj.bias', 'model.decoder.layers.1.self_attn_layer_norm.weight', 'model.decoder.layers.3.fc1.bias', 'model.decoder.layers.2.self_attn_layer_norm.weight', 'model.decoder.layers.3.self_attn.q_proj.bias', 'model.decoder.layers.2.fc1.bias', 'model.decoder.layers.2.encoder_attn.v_proj.weight', 'model.decoder.embed_tokens.weight', 'model.decoder.layers.0.encoder_attn.q_proj.weight', 'model.decoder.layers.1.self_attn.q_proj.bias', 'model.decoder.layers.1.self_attn.q_proj.weight', 'model.decoder.layers.1.encoder_attn.v_proj.bias', 'model.decoder.layers.1.self_attn.v_proj.bias', 'model.decoder.layers.0.self_attn.v_proj.bias', 'model.decoder.layers.3.self_attn.out_proj.bias', 'model.decoder.layers.3.self_attn.q_proj.weight', 'model.decoder.layers.3.encoder_attn.v_proj.weight', 'model.decoder.layers.1.encoder_attn.v_proj.weight', 'model.decoder.layers.3.encoder_attn_layer_norm.bias', 'model.decoder.layers.3.final_layer_norm.bias', 'model.decoder.layers.1.fc1.bias', 'model.decoder.layers.3.fc2.bias', 'model.decoder.layers.1.self_attn.k_proj.weight', 'model.decoder.layers.3.self_attn.v_proj.weight', 'model.decoder.layers.1.self_attn.out_proj.weight', 'model.decoder.layers.0.self_attn_layer_norm.bias', 'model.decoder.layers.2.encoder_attn.out_proj.bias', 'model.decoder.layers.0.encoder_attn_layer_norm.weight', 'model.decoder.layers.0.self_attn.q_proj.bias', 'model.decoder.layers.1.encoder_attn.out_proj.weight', 'model.decoder.layers.2.encoder_attn.k_proj.weight', 'model.decoder.layers.2.self_attn.out_proj.bias', 'model.decoder.layers.3.encoder_attn.q_proj.bias', 'model.decoder.layers.2.self_attn.q_proj.weight', 'model.decoder.layers.3.encoder_attn.out_proj.bias', 'model.decoder.layer_norm.bias', 'model.decoder.layers.2.final_layer_norm.bias', 'model.decoder.layers.2.encoder_attn.q_proj.weight', 'model.decoder.layers.0.final_layer_norm.weight', 'model.decoder.layers.3.encoder_attn.out_proj.weight', 'model.decoder.layers.1.encoder_attn.k_proj.weight', 'model.decoder.layers.0.encoder_attn.k_proj.weight']\n",
      "- This IS expected if you are initializing WhisperForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing WhisperForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of WhisperForSequenceClassification were not initialized from the model checkpoint at openai/whisper-tiny and are newly initialized: ['model.head.layers.0.bias', 'model.classifier.weight', 'model.head.layers.0.weight', 'model.classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/alesssandros/dev/FCN_Newspaper/aii/wandb/run-20230223_215739-yfjcwy4a</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/astockman/music-classification-aii/runs/yfjcwy4a' target=\"_blank\">whisper-fnt-c256-d0-20230223-215738</a></strong> to <a href='https://wandb.ai/astockman/music-classification-aii' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/astockman/music-classification-aii' target=\"_blank\">https://wandb.ai/astockman/music-classification-aii</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/astockman/music-classification-aii/runs/yfjcwy4a' target=\"_blank\">https://wandb.ai/astockman/music-classification-aii/runs/yfjcwy4a</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The following columns in the training set don't have a corresponding argument in `WhisperForSequenceClassification.forward` and have been ignored: duration, id. If duration, id are not expected by `WhisperForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/home/alesssandros/.local/share/virtualenvs/aii-EtAbm6lw/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 799\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 2000\n",
      "  Number of trainable parameters = 8307715\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2000' max='2000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2000/2000 14:46, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.985900</td>\n",
       "      <td>0.805263</td>\n",
       "      <td>0.640000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.734200</td>\n",
       "      <td>0.646112</td>\n",
       "      <td>0.730000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.581800</td>\n",
       "      <td>0.567446</td>\n",
       "      <td>0.740000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.566800</td>\n",
       "      <td>0.515999</td>\n",
       "      <td>0.760000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.372200</td>\n",
       "      <td>0.703742</td>\n",
       "      <td>0.780000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.308300</td>\n",
       "      <td>0.835029</td>\n",
       "      <td>0.790000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.255800</td>\n",
       "      <td>0.758668</td>\n",
       "      <td>0.830000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.109000</td>\n",
       "      <td>0.839265</td>\n",
       "      <td>0.820000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.089200</td>\n",
       "      <td>0.939754</td>\n",
       "      <td>0.810000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.044100</td>\n",
       "      <td>1.021315</td>\n",
       "      <td>0.840000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.028500</td>\n",
       "      <td>1.023541</td>\n",
       "      <td>0.830000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>0.974650</td>\n",
       "      <td>0.840000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>1.103982</td>\n",
       "      <td>0.830000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>1.225332</td>\n",
       "      <td>0.820000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>1.226566</td>\n",
       "      <td>0.810000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>1.229570</td>\n",
       "      <td>0.810000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>1.235679</td>\n",
       "      <td>0.810000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>1.239864</td>\n",
       "      <td>0.810000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>1.242262</td>\n",
       "      <td>0.810000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>1.243139</td>\n",
       "      <td>0.810000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `WhisperForSequenceClassification.forward` and have been ignored: duration, id. If duration, id are not expected by `WhisperForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to out/checkpoint-100\n",
      "Configuration saved in out/checkpoint-100/config.json\n",
      "Model weights saved in out/checkpoint-100/pytorch_model.bin\n",
      "Feature extractor saved in out/checkpoint-100/preprocessor_config.json\n",
      "Deleting older checkpoint [out/checkpoint-700] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `WhisperForSequenceClassification.forward` and have been ignored: duration, id. If duration, id are not expected by `WhisperForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to out/checkpoint-200\n",
      "Configuration saved in out/checkpoint-200/config.json\n",
      "Model weights saved in out/checkpoint-200/pytorch_model.bin\n",
      "Feature extractor saved in out/checkpoint-200/preprocessor_config.json\n",
      "Deleting older checkpoint [out/checkpoint-1900] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `WhisperForSequenceClassification.forward` and have been ignored: duration, id. If duration, id are not expected by `WhisperForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to out/checkpoint-300\n",
      "Configuration saved in out/checkpoint-300/config.json\n",
      "Model weights saved in out/checkpoint-300/pytorch_model.bin\n",
      "Feature extractor saved in out/checkpoint-300/preprocessor_config.json\n",
      "Deleting older checkpoint [out/checkpoint-2000] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `WhisperForSequenceClassification.forward` and have been ignored: duration, id. If duration, id are not expected by `WhisperForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to out/checkpoint-400\n",
      "Configuration saved in out/checkpoint-400/config.json\n",
      "Model weights saved in out/checkpoint-400/pytorch_model.bin\n",
      "Feature extractor saved in out/checkpoint-400/preprocessor_config.json\n",
      "Deleting older checkpoint [out/checkpoint-100] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `WhisperForSequenceClassification.forward` and have been ignored: duration, id. If duration, id are not expected by `WhisperForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to out/checkpoint-500\n",
      "Configuration saved in out/checkpoint-500/config.json\n",
      "Model weights saved in out/checkpoint-500/pytorch_model.bin\n",
      "Feature extractor saved in out/checkpoint-500/preprocessor_config.json\n",
      "Deleting older checkpoint [out/checkpoint-200] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `WhisperForSequenceClassification.forward` and have been ignored: duration, id. If duration, id are not expected by `WhisperForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to out/checkpoint-600\n",
      "Configuration saved in out/checkpoint-600/config.json\n",
      "Model weights saved in out/checkpoint-600/pytorch_model.bin\n",
      "Feature extractor saved in out/checkpoint-600/preprocessor_config.json\n",
      "Deleting older checkpoint [out/checkpoint-300] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `WhisperForSequenceClassification.forward` and have been ignored: duration, id. If duration, id are not expected by `WhisperForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to out/checkpoint-700\n",
      "Configuration saved in out/checkpoint-700/config.json\n",
      "Model weights saved in out/checkpoint-700/pytorch_model.bin\n",
      "Feature extractor saved in out/checkpoint-700/preprocessor_config.json\n",
      "Deleting older checkpoint [out/checkpoint-400] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `WhisperForSequenceClassification.forward` and have been ignored: duration, id. If duration, id are not expected by `WhisperForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to out/checkpoint-800\n",
      "Configuration saved in out/checkpoint-800/config.json\n",
      "Model weights saved in out/checkpoint-800/pytorch_model.bin\n",
      "Feature extractor saved in out/checkpoint-800/preprocessor_config.json\n",
      "Deleting older checkpoint [out/checkpoint-500] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `WhisperForSequenceClassification.forward` and have been ignored: duration, id. If duration, id are not expected by `WhisperForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to out/checkpoint-900\n",
      "Configuration saved in out/checkpoint-900/config.json\n",
      "Model weights saved in out/checkpoint-900/pytorch_model.bin\n",
      "Feature extractor saved in out/checkpoint-900/preprocessor_config.json\n",
      "Deleting older checkpoint [out/checkpoint-600] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `WhisperForSequenceClassification.forward` and have been ignored: duration, id. If duration, id are not expected by `WhisperForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to out/checkpoint-1000\n",
      "Configuration saved in out/checkpoint-1000/config.json\n",
      "Model weights saved in out/checkpoint-1000/pytorch_model.bin\n",
      "Feature extractor saved in out/checkpoint-1000/preprocessor_config.json\n",
      "Deleting older checkpoint [out/checkpoint-700] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `WhisperForSequenceClassification.forward` and have been ignored: duration, id. If duration, id are not expected by `WhisperForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to out/checkpoint-1100\n",
      "Configuration saved in out/checkpoint-1100/config.json\n",
      "Model weights saved in out/checkpoint-1100/pytorch_model.bin\n",
      "Feature extractor saved in out/checkpoint-1100/preprocessor_config.json\n",
      "Deleting older checkpoint [out/checkpoint-800] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `WhisperForSequenceClassification.forward` and have been ignored: duration, id. If duration, id are not expected by `WhisperForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to out/checkpoint-1200\n",
      "Configuration saved in out/checkpoint-1200/config.json\n",
      "Model weights saved in out/checkpoint-1200/pytorch_model.bin\n",
      "Feature extractor saved in out/checkpoint-1200/preprocessor_config.json\n",
      "Deleting older checkpoint [out/checkpoint-900] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `WhisperForSequenceClassification.forward` and have been ignored: duration, id. If duration, id are not expected by `WhisperForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to out/checkpoint-1300\n",
      "Configuration saved in out/checkpoint-1300/config.json\n",
      "Model weights saved in out/checkpoint-1300/pytorch_model.bin\n",
      "Feature extractor saved in out/checkpoint-1300/preprocessor_config.json\n",
      "Deleting older checkpoint [out/checkpoint-1100] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `WhisperForSequenceClassification.forward` and have been ignored: duration, id. If duration, id are not expected by `WhisperForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to out/checkpoint-1400\n",
      "Configuration saved in out/checkpoint-1400/config.json\n",
      "Model weights saved in out/checkpoint-1400/pytorch_model.bin\n",
      "Feature extractor saved in out/checkpoint-1400/preprocessor_config.json\n",
      "Deleting older checkpoint [out/checkpoint-1200] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `WhisperForSequenceClassification.forward` and have been ignored: duration, id. If duration, id are not expected by `WhisperForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to out/checkpoint-1500\n",
      "Configuration saved in out/checkpoint-1500/config.json\n",
      "Model weights saved in out/checkpoint-1500/pytorch_model.bin\n",
      "Feature extractor saved in out/checkpoint-1500/preprocessor_config.json\n",
      "Deleting older checkpoint [out/checkpoint-1300] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `WhisperForSequenceClassification.forward` and have been ignored: duration, id. If duration, id are not expected by `WhisperForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to out/checkpoint-1600\n",
      "Configuration saved in out/checkpoint-1600/config.json\n",
      "Model weights saved in out/checkpoint-1600/pytorch_model.bin\n",
      "Feature extractor saved in out/checkpoint-1600/preprocessor_config.json\n",
      "Deleting older checkpoint [out/checkpoint-1400] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `WhisperForSequenceClassification.forward` and have been ignored: duration, id. If duration, id are not expected by `WhisperForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to out/checkpoint-1700\n",
      "Configuration saved in out/checkpoint-1700/config.json\n",
      "Model weights saved in out/checkpoint-1700/pytorch_model.bin\n",
      "Feature extractor saved in out/checkpoint-1700/preprocessor_config.json\n",
      "Deleting older checkpoint [out/checkpoint-1500] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `WhisperForSequenceClassification.forward` and have been ignored: duration, id. If duration, id are not expected by `WhisperForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to out/checkpoint-1800\n",
      "Configuration saved in out/checkpoint-1800/config.json\n",
      "Model weights saved in out/checkpoint-1800/pytorch_model.bin\n",
      "Feature extractor saved in out/checkpoint-1800/preprocessor_config.json\n",
      "Deleting older checkpoint [out/checkpoint-1600] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `WhisperForSequenceClassification.forward` and have been ignored: duration, id. If duration, id are not expected by `WhisperForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to out/checkpoint-1900\n",
      "Configuration saved in out/checkpoint-1900/config.json\n",
      "Model weights saved in out/checkpoint-1900/pytorch_model.bin\n",
      "Feature extractor saved in out/checkpoint-1900/preprocessor_config.json\n",
      "Deleting older checkpoint [out/checkpoint-1700] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `WhisperForSequenceClassification.forward` and have been ignored: duration, id. If duration, id are not expected by `WhisperForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to out/checkpoint-2000\n",
      "Configuration saved in out/checkpoint-2000/config.json\n",
      "Model weights saved in out/checkpoint-2000/pytorch_model.bin\n",
      "Feature extractor saved in out/checkpoint-2000/preprocessor_config.json\n",
      "Deleting older checkpoint [out/checkpoint-1800] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from out/checkpoint-1000 (score: 0.84).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3cd23cb8740416aa8c4737e92158a17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.002 MB of 0.013 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=0.190458…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>▁▄▅▅▆▆█▇▇████▇▇▇▇▇▇▇</td></tr><tr><td>eval/loss</td><td>▄▂▁▁▃▄▃▄▅▆▆▅▇███████</td></tr><tr><td>eval/runtime</td><td>██▁██▁██▁▁████▁███▁█</td></tr><tr><td>eval/samples_per_second</td><td>▁▁█▁▁█▁▁██▁▁▁▁█▁▁▁█▁</td></tr><tr><td>eval/steps_per_second</td><td>▁▁█▁▁█▁▁██▁▁▁▁█▁▁▁█▁</td></tr><tr><td>train/epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>train/global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>train/learning_rate</td><td>███▇▇▇▇▇▇▆▆▆▆▆▆▅▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▁▁</td></tr><tr><td>train/loss</td><td>██▇▆▆▅▅▄▅▃▃▃▃▂▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>0.81</td></tr><tr><td>eval/loss</td><td>1.24314</td></tr><tr><td>eval/runtime</td><td>4.8442</td></tr><tr><td>eval/samples_per_second</td><td>20.643</td></tr><tr><td>eval/steps_per_second</td><td>1.445</td></tr><tr><td>train/epoch</td><td>20.0</td></tr><tr><td>train/global_step</td><td>2000</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.0003</td></tr><tr><td>train/total_flos</td><td>5.9305346736e+16</td></tr><tr><td>train/train_loss</td><td>0.2087</td></tr><tr><td>train/train_runtime</td><td>887.1617</td></tr><tr><td>train/train_samples_per_second</td><td>18.013</td></tr><tr><td>train/train_steps_per_second</td><td>2.254</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">whisper-fnt-c256-d0-20230223-215738</strong> at: <a href='https://wandb.ai/astockman/music-classification-aii/runs/yfjcwy4a' target=\"_blank\">https://wandb.ai/astockman/music-classification-aii/runs/yfjcwy4a</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230223_215739-yfjcwy4a/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to res/models/whisper-fnt-c256-d0-20230223-215738\n",
      "Configuration saved in res/models/whisper-fnt-c256-d0-20230223-215738/config.json\n",
      "Model weights saved in res/models/whisper-fnt-c256-d0-20230223-215738/pytorch_model.bin\n",
      "Feature extractor saved in res/models/whisper-fnt-c256-d0-20230223-215738/preprocessor_config.json\n"
     ]
    }
   ],
   "source": [
    "run_name = get_run_name(TRAINING_CONFIG)\n",
    "model = get_model(TRAINING_CONFIG, prepared_ds[\"train\"])\n",
    "\n",
    "trainer = get_trainer(\n",
    "    run_name=run_name,\n",
    "    model=model,\n",
    "    train_ds=prepared_ds[\"train\"],\n",
    "    eval_ds=prepared_ds[\"valid\"],\n",
    "    training_config=TRAINING_CONFIG,\n",
    "    output_dir=\"out\",\n",
    "    debug=False,\n",
    "    env=NOTEBOOK_ENV,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "end_training(run_name, trainer, MODELS_DIR_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_CONFIG[\"classifier_layers\"] = [256, 256]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/alesssandros/.cache/huggingface/hub/models--openai--whisper-tiny/snapshots/302560528ac75a251232980ebcc68bad9668f664/config.json\n",
      "Model config WhisperConfig {\n",
      "  \"_name_or_path\": \"openai/whisper-tiny\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"WhisperForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"begin_suppress_tokens\": [\n",
      "    220,\n",
      "    50257\n",
      "  ],\n",
      "  \"bos_token_id\": 50257,\n",
      "  \"d_model\": 384,\n",
      "  \"decoder_attention_heads\": 6,\n",
      "  \"decoder_ffn_dim\": 1536,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 4,\n",
      "  \"decoder_start_token_id\": 50258,\n",
      "  \"dropout\": 0.0,\n",
      "  \"encoder_attention_heads\": 6,\n",
      "  \"encoder_ffn_dim\": 1536,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 4,\n",
      "  \"eos_token_id\": 50257,\n",
      "  \"forced_decoder_ids\": [\n",
      "    [\n",
      "      1,\n",
      "      50259\n",
      "    ],\n",
      "    [\n",
      "      2,\n",
      "      50359\n",
      "    ],\n",
      "    [\n",
      "      3,\n",
      "      50363\n",
      "    ]\n",
      "  ],\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"max_length\": 448,\n",
      "  \"max_source_positions\": 1500,\n",
      "  \"max_target_positions\": 448,\n",
      "  \"model_type\": \"whisper\",\n",
      "  \"num_hidden_layers\": 4,\n",
      "  \"num_mel_bins\": 80,\n",
      "  \"pad_token_id\": 50257,\n",
      "  \"scale_embedding\": false,\n",
      "  \"suppress_tokens\": [\n",
      "    1,\n",
      "    2,\n",
      "    7,\n",
      "    8,\n",
      "    9,\n",
      "    10,\n",
      "    14,\n",
      "    25,\n",
      "    26,\n",
      "    27,\n",
      "    28,\n",
      "    29,\n",
      "    31,\n",
      "    58,\n",
      "    59,\n",
      "    60,\n",
      "    61,\n",
      "    62,\n",
      "    63,\n",
      "    90,\n",
      "    91,\n",
      "    92,\n",
      "    93,\n",
      "    359,\n",
      "    503,\n",
      "    522,\n",
      "    542,\n",
      "    873,\n",
      "    893,\n",
      "    902,\n",
      "    918,\n",
      "    922,\n",
      "    931,\n",
      "    1350,\n",
      "    1853,\n",
      "    1982,\n",
      "    2460,\n",
      "    2627,\n",
      "    3246,\n",
      "    3253,\n",
      "    3268,\n",
      "    3536,\n",
      "    3846,\n",
      "    3961,\n",
      "    4183,\n",
      "    4667,\n",
      "    6585,\n",
      "    6647,\n",
      "    7273,\n",
      "    9061,\n",
      "    9383,\n",
      "    10428,\n",
      "    10929,\n",
      "    11938,\n",
      "    12033,\n",
      "    12331,\n",
      "    12562,\n",
      "    13793,\n",
      "    14157,\n",
      "    14635,\n",
      "    15265,\n",
      "    15618,\n",
      "    16553,\n",
      "    16604,\n",
      "    18362,\n",
      "    18956,\n",
      "    20075,\n",
      "    21675,\n",
      "    22520,\n",
      "    26130,\n",
      "    26161,\n",
      "    26435,\n",
      "    28279,\n",
      "    29464,\n",
      "    31650,\n",
      "    32302,\n",
      "    32470,\n",
      "    36865,\n",
      "    42863,\n",
      "    47425,\n",
      "    49870,\n",
      "    50254,\n",
      "    50258,\n",
      "    50360,\n",
      "    50361,\n",
      "    50362\n",
      "  ],\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 51865\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/alesssandros/.cache/huggingface/hub/models--openai--whisper-tiny/snapshots/302560528ac75a251232980ebcc68bad9668f664/pytorch_model.bin\n",
      "Some weights of the model checkpoint at openai/whisper-tiny were not used when initializing WhisperForSequenceClassification: ['model.decoder.layers.3.fc2.weight', 'model.decoder.layers.3.self_attn.out_proj.weight', 'model.decoder.layers.0.fc1.bias', 'model.decoder.embed_positions.weight', 'model.decoder.layers.0.self_attn.q_proj.weight', 'model.decoder.layers.0.self_attn.out_proj.weight', 'model.decoder.layers.2.encoder_attn_layer_norm.weight', 'model.decoder.layers.1.self_attn.v_proj.weight', 'model.decoder.layers.2.self_attn_layer_norm.bias', 'model.decoder.layers.0.encoder_attn_layer_norm.bias', 'model.decoder.layers.2.self_attn.out_proj.weight', 'model.decoder.layers.1.self_attn.out_proj.bias', 'model.decoder.layers.1.encoder_attn_layer_norm.weight', 'model.decoder.layers.2.encoder_attn.v_proj.bias', 'model.decoder.layers.0.self_attn_layer_norm.weight', 'model.decoder.layers.3.final_layer_norm.weight', 'model.decoder.layers.3.fc1.weight', 'model.decoder.layers.0.self_attn.v_proj.weight', 'model.decoder.layers.0.encoder_attn.out_proj.bias', 'model.decoder.layers.3.encoder_attn.k_proj.weight', 'model.decoder.layers.3.encoder_attn_layer_norm.weight', 'model.decoder.layers.2.fc2.weight', 'model.decoder.layers.0.final_layer_norm.bias', 'model.decoder.layers.3.self_attn_layer_norm.weight', 'model.decoder.layers.2.fc2.bias', 'model.decoder.layers.3.encoder_attn.v_proj.bias', 'model.decoder.layers.1.final_layer_norm.weight', 'model.decoder.layers.2.self_attn.v_proj.bias', 'model.decoder.layers.0.encoder_attn.out_proj.weight', 'model.decoder.layers.0.encoder_attn.v_proj.bias', 'model.decoder.layers.1.encoder_attn.q_proj.weight', 'model.decoder.layers.1.encoder_attn_layer_norm.bias', 'model.decoder.layers.2.self_attn.v_proj.weight', 'model.decoder.layers.1.encoder_attn.out_proj.bias', 'model.decoder.layers.0.fc1.weight', 'model.decoder.layers.3.self_attn_layer_norm.bias', 'model.decoder.layers.1.self_attn_layer_norm.bias', 'model.decoder.layers.2.self_attn.k_proj.weight', 'model.decoder.layers.2.self_attn.q_proj.bias', 'model.decoder.layers.2.final_layer_norm.weight', 'model.decoder.layers.0.self_attn.out_proj.bias', 'model.decoder.layers.0.encoder_attn.v_proj.weight', 'model.decoder.layers.1.final_layer_norm.bias', 'model.decoder.layers.1.fc2.bias', 'model.decoder.layer_norm.weight', 'model.decoder.layers.2.encoder_attn_layer_norm.bias', 'model.decoder.layers.2.encoder_attn.out_proj.weight', 'model.decoder.layers.0.self_attn.k_proj.weight', 'model.decoder.layers.2.encoder_attn.q_proj.bias', 'model.decoder.layers.0.fc2.bias', 'model.decoder.layers.1.fc2.weight', 'model.decoder.layers.1.fc1.weight', 'model.decoder.layers.3.self_attn.v_proj.bias', 'model.decoder.layers.3.self_attn.k_proj.weight', 'model.decoder.layers.3.encoder_attn.q_proj.weight', 'model.decoder.layers.0.fc2.weight', 'model.decoder.layers.0.encoder_attn.q_proj.bias', 'model.decoder.layers.2.fc1.weight', 'model.decoder.layers.1.encoder_attn.q_proj.bias', 'model.decoder.layers.1.self_attn_layer_norm.weight', 'model.decoder.layers.3.fc1.bias', 'model.decoder.layers.2.self_attn_layer_norm.weight', 'model.decoder.layers.3.self_attn.q_proj.bias', 'model.decoder.layers.2.fc1.bias', 'model.decoder.layers.2.encoder_attn.v_proj.weight', 'model.decoder.embed_tokens.weight', 'model.decoder.layers.0.encoder_attn.q_proj.weight', 'model.decoder.layers.1.self_attn.q_proj.bias', 'model.decoder.layers.1.self_attn.q_proj.weight', 'model.decoder.layers.1.encoder_attn.v_proj.bias', 'model.decoder.layers.1.self_attn.v_proj.bias', 'model.decoder.layers.0.self_attn.v_proj.bias', 'model.decoder.layers.3.self_attn.out_proj.bias', 'model.decoder.layers.3.self_attn.q_proj.weight', 'model.decoder.layers.3.encoder_attn.v_proj.weight', 'model.decoder.layers.1.encoder_attn.v_proj.weight', 'model.decoder.layers.3.encoder_attn_layer_norm.bias', 'model.decoder.layers.3.final_layer_norm.bias', 'model.decoder.layers.1.fc1.bias', 'model.decoder.layers.3.fc2.bias', 'model.decoder.layers.1.self_attn.k_proj.weight', 'model.decoder.layers.3.self_attn.v_proj.weight', 'model.decoder.layers.1.self_attn.out_proj.weight', 'model.decoder.layers.0.self_attn_layer_norm.bias', 'model.decoder.layers.2.encoder_attn.out_proj.bias', 'model.decoder.layers.0.encoder_attn_layer_norm.weight', 'model.decoder.layers.0.self_attn.q_proj.bias', 'model.decoder.layers.1.encoder_attn.out_proj.weight', 'model.decoder.layers.2.encoder_attn.k_proj.weight', 'model.decoder.layers.2.self_attn.out_proj.bias', 'model.decoder.layers.3.encoder_attn.q_proj.bias', 'model.decoder.layers.2.self_attn.q_proj.weight', 'model.decoder.layers.3.encoder_attn.out_proj.bias', 'model.decoder.layer_norm.bias', 'model.decoder.layers.2.final_layer_norm.bias', 'model.decoder.layers.2.encoder_attn.q_proj.weight', 'model.decoder.layers.0.final_layer_norm.weight', 'model.decoder.layers.3.encoder_attn.out_proj.weight', 'model.decoder.layers.1.encoder_attn.k_proj.weight', 'model.decoder.layers.0.encoder_attn.k_proj.weight']\n",
      "- This IS expected if you are initializing WhisperForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing WhisperForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of WhisperForSequenceClassification were not initialized from the model checkpoint at openai/whisper-tiny and are newly initialized: ['model.head.layers.0.bias', 'model.head.layers.0.weight', 'model.head.layers.1.bias', 'model.classifier.weight', 'model.head.layers.1.weight', 'model.classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/alesssandros/dev/FCN_Newspaper/aii/wandb/run-20230223_221235-03q7nf4v</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/astockman/music-classification-aii/runs/03q7nf4v' target=\"_blank\">whisper-fnt-c256_256-d0-20230223-221234</a></strong> to <a href='https://wandb.ai/astockman/music-classification-aii' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/astockman/music-classification-aii' target=\"_blank\">https://wandb.ai/astockman/music-classification-aii</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/astockman/music-classification-aii/runs/03q7nf4v' target=\"_blank\">https://wandb.ai/astockman/music-classification-aii/runs/03q7nf4v</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The following columns in the training set don't have a corresponding argument in `WhisperForSequenceClassification.forward` and have been ignored: duration, id. If duration, id are not expected by `WhisperForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/home/alesssandros/.local/share/virtualenvs/aii-EtAbm6lw/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 799\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 2000\n",
      "  Number of trainable parameters = 8373507\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2000' max='2000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2000/2000 16:03, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.087200</td>\n",
       "      <td>1.038829</td>\n",
       "      <td>0.650000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.801800</td>\n",
       "      <td>0.695724</td>\n",
       "      <td>0.760000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.633500</td>\n",
       "      <td>0.565698</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.426100</td>\n",
       "      <td>0.483982</td>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.340600</td>\n",
       "      <td>0.614469</td>\n",
       "      <td>0.780000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.209200</td>\n",
       "      <td>0.984450</td>\n",
       "      <td>0.740000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.213300</td>\n",
       "      <td>0.642029</td>\n",
       "      <td>0.850000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.096200</td>\n",
       "      <td>0.906091</td>\n",
       "      <td>0.830000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.068500</td>\n",
       "      <td>0.940986</td>\n",
       "      <td>0.780000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.021400</td>\n",
       "      <td>0.601790</td>\n",
       "      <td>0.890000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.017900</td>\n",
       "      <td>0.797289</td>\n",
       "      <td>0.870000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.005000</td>\n",
       "      <td>0.914959</td>\n",
       "      <td>0.830000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.870887</td>\n",
       "      <td>0.860000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.849484</td>\n",
       "      <td>0.850000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.840353</td>\n",
       "      <td>0.850000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.843681</td>\n",
       "      <td>0.850000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.849220</td>\n",
       "      <td>0.850000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.853240</td>\n",
       "      <td>0.850000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.856087</td>\n",
       "      <td>0.850000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.856908</td>\n",
       "      <td>0.850000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `WhisperForSequenceClassification.forward` and have been ignored: duration, id. If duration, id are not expected by `WhisperForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to out/checkpoint-100\n",
      "Configuration saved in out/checkpoint-100/config.json\n",
      "Model weights saved in out/checkpoint-100/pytorch_model.bin\n",
      "Feature extractor saved in out/checkpoint-100/preprocessor_config.json\n",
      "Deleting older checkpoint [out/checkpoint-1000] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `WhisperForSequenceClassification.forward` and have been ignored: duration, id. If duration, id are not expected by `WhisperForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to out/checkpoint-200\n",
      "Configuration saved in out/checkpoint-200/config.json\n",
      "Model weights saved in out/checkpoint-200/pytorch_model.bin\n",
      "Feature extractor saved in out/checkpoint-200/preprocessor_config.json\n",
      "Deleting older checkpoint [out/checkpoint-1900] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `WhisperForSequenceClassification.forward` and have been ignored: duration, id. If duration, id are not expected by `WhisperForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to out/checkpoint-300\n",
      "Configuration saved in out/checkpoint-300/config.json\n",
      "Model weights saved in out/checkpoint-300/pytorch_model.bin\n",
      "Feature extractor saved in out/checkpoint-300/preprocessor_config.json\n",
      "Deleting older checkpoint [out/checkpoint-2000] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `WhisperForSequenceClassification.forward` and have been ignored: duration, id. If duration, id are not expected by `WhisperForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to out/checkpoint-400\n",
      "Configuration saved in out/checkpoint-400/config.json\n",
      "Model weights saved in out/checkpoint-400/pytorch_model.bin\n",
      "Feature extractor saved in out/checkpoint-400/preprocessor_config.json\n",
      "Deleting older checkpoint [out/checkpoint-100] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `WhisperForSequenceClassification.forward` and have been ignored: duration, id. If duration, id are not expected by `WhisperForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to out/checkpoint-500\n",
      "Configuration saved in out/checkpoint-500/config.json\n",
      "Model weights saved in out/checkpoint-500/pytorch_model.bin\n",
      "Feature extractor saved in out/checkpoint-500/preprocessor_config.json\n",
      "Deleting older checkpoint [out/checkpoint-200] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `WhisperForSequenceClassification.forward` and have been ignored: duration, id. If duration, id are not expected by `WhisperForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to out/checkpoint-600\n",
      "Configuration saved in out/checkpoint-600/config.json\n",
      "Model weights saved in out/checkpoint-600/pytorch_model.bin\n",
      "Feature extractor saved in out/checkpoint-600/preprocessor_config.json\n",
      "Deleting older checkpoint [out/checkpoint-300] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `WhisperForSequenceClassification.forward` and have been ignored: duration, id. If duration, id are not expected by `WhisperForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to out/checkpoint-700\n",
      "Configuration saved in out/checkpoint-700/config.json\n",
      "Model weights saved in out/checkpoint-700/pytorch_model.bin\n",
      "Feature extractor saved in out/checkpoint-700/preprocessor_config.json\n",
      "Deleting older checkpoint [out/checkpoint-400] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `WhisperForSequenceClassification.forward` and have been ignored: duration, id. If duration, id are not expected by `WhisperForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to out/checkpoint-800\n",
      "Configuration saved in out/checkpoint-800/config.json\n",
      "Model weights saved in out/checkpoint-800/pytorch_model.bin\n",
      "Feature extractor saved in out/checkpoint-800/preprocessor_config.json\n",
      "Deleting older checkpoint [out/checkpoint-500] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `WhisperForSequenceClassification.forward` and have been ignored: duration, id. If duration, id are not expected by `WhisperForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to out/checkpoint-900\n",
      "Configuration saved in out/checkpoint-900/config.json\n",
      "Model weights saved in out/checkpoint-900/pytorch_model.bin\n",
      "Feature extractor saved in out/checkpoint-900/preprocessor_config.json\n",
      "Deleting older checkpoint [out/checkpoint-600] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `WhisperForSequenceClassification.forward` and have been ignored: duration, id. If duration, id are not expected by `WhisperForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to out/checkpoint-1000\n",
      "Configuration saved in out/checkpoint-1000/config.json\n",
      "Model weights saved in out/checkpoint-1000/pytorch_model.bin\n",
      "Feature extractor saved in out/checkpoint-1000/preprocessor_config.json\n",
      "Deleting older checkpoint [out/checkpoint-700] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `WhisperForSequenceClassification.forward` and have been ignored: duration, id. If duration, id are not expected by `WhisperForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to out/checkpoint-1100\n",
      "Configuration saved in out/checkpoint-1100/config.json\n",
      "Model weights saved in out/checkpoint-1100/pytorch_model.bin\n",
      "Feature extractor saved in out/checkpoint-1100/preprocessor_config.json\n",
      "Deleting older checkpoint [out/checkpoint-800] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `WhisperForSequenceClassification.forward` and have been ignored: duration, id. If duration, id are not expected by `WhisperForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to out/checkpoint-1200\n",
      "Configuration saved in out/checkpoint-1200/config.json\n",
      "Model weights saved in out/checkpoint-1200/pytorch_model.bin\n",
      "Feature extractor saved in out/checkpoint-1200/preprocessor_config.json\n",
      "Deleting older checkpoint [out/checkpoint-900] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `WhisperForSequenceClassification.forward` and have been ignored: duration, id. If duration, id are not expected by `WhisperForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to out/checkpoint-1300\n",
      "Configuration saved in out/checkpoint-1300/config.json\n",
      "Model weights saved in out/checkpoint-1300/pytorch_model.bin\n",
      "Feature extractor saved in out/checkpoint-1300/preprocessor_config.json\n",
      "Deleting older checkpoint [out/checkpoint-1100] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `WhisperForSequenceClassification.forward` and have been ignored: duration, id. If duration, id are not expected by `WhisperForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to out/checkpoint-1400\n",
      "Configuration saved in out/checkpoint-1400/config.json\n",
      "Model weights saved in out/checkpoint-1400/pytorch_model.bin\n",
      "Feature extractor saved in out/checkpoint-1400/preprocessor_config.json\n",
      "Deleting older checkpoint [out/checkpoint-1200] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `WhisperForSequenceClassification.forward` and have been ignored: duration, id. If duration, id are not expected by `WhisperForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to out/checkpoint-1500\n",
      "Configuration saved in out/checkpoint-1500/config.json\n",
      "Model weights saved in out/checkpoint-1500/pytorch_model.bin\n",
      "Feature extractor saved in out/checkpoint-1500/preprocessor_config.json\n",
      "Deleting older checkpoint [out/checkpoint-1300] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `WhisperForSequenceClassification.forward` and have been ignored: duration, id. If duration, id are not expected by `WhisperForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to out/checkpoint-1600\n",
      "Configuration saved in out/checkpoint-1600/config.json\n",
      "Model weights saved in out/checkpoint-1600/pytorch_model.bin\n",
      "Feature extractor saved in out/checkpoint-1600/preprocessor_config.json\n",
      "Deleting older checkpoint [out/checkpoint-1400] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `WhisperForSequenceClassification.forward` and have been ignored: duration, id. If duration, id are not expected by `WhisperForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to out/checkpoint-1700\n",
      "Configuration saved in out/checkpoint-1700/config.json\n",
      "Model weights saved in out/checkpoint-1700/pytorch_model.bin\n",
      "Feature extractor saved in out/checkpoint-1700/preprocessor_config.json\n",
      "Deleting older checkpoint [out/checkpoint-1500] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `WhisperForSequenceClassification.forward` and have been ignored: duration, id. If duration, id are not expected by `WhisperForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to out/checkpoint-1800\n",
      "Configuration saved in out/checkpoint-1800/config.json\n",
      "Model weights saved in out/checkpoint-1800/pytorch_model.bin\n",
      "Feature extractor saved in out/checkpoint-1800/preprocessor_config.json\n",
      "Deleting older checkpoint [out/checkpoint-1600] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `WhisperForSequenceClassification.forward` and have been ignored: duration, id. If duration, id are not expected by `WhisperForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to out/checkpoint-1900\n",
      "Configuration saved in out/checkpoint-1900/config.json\n",
      "Model weights saved in out/checkpoint-1900/pytorch_model.bin\n",
      "Feature extractor saved in out/checkpoint-1900/preprocessor_config.json\n",
      "Deleting older checkpoint [out/checkpoint-1700] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `WhisperForSequenceClassification.forward` and have been ignored: duration, id. If duration, id are not expected by `WhisperForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to out/checkpoint-2000\n",
      "Configuration saved in out/checkpoint-2000/config.json\n",
      "Model weights saved in out/checkpoint-2000/pytorch_model.bin\n",
      "Feature extractor saved in out/checkpoint-2000/preprocessor_config.json\n",
      "Deleting older checkpoint [out/checkpoint-1800] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from out/checkpoint-1000 (score: 0.89).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a18e7bc924c1449f8fb23bbe59fc0553",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.002 MB of 0.027 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=0.088952…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>▁▄▄▅▅▄▇▆▅█▇▆▇▇▇▇▇▇▇▇</td></tr><tr><td>eval/loss</td><td>█▄▂▁▃▇▃▆▇▂▅▆▆▆▅▆▆▆▆▆</td></tr><tr><td>eval/runtime</td><td>██▇███▁▇█▇███▅▇▇▇▁▇█</td></tr><tr><td>eval/samples_per_second</td><td>▁▁▁▁▁▁█▁▁▁▁▁▁▂▁▁▁█▁▁</td></tr><tr><td>eval/steps_per_second</td><td>▁▁▁▁▁▁█▁▁▁▁▁▁▂▁▁▁█▁▁</td></tr><tr><td>train/epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>train/global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>train/learning_rate</td><td>███▇▇▇▇▇▇▆▆▆▆▆▆▅▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▁▁</td></tr><tr><td>train/loss</td><td>███▇▆▅▅▄▄▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>0.85</td></tr><tr><td>eval/loss</td><td>0.85691</td></tr><tr><td>eval/runtime</td><td>4.8441</td></tr><tr><td>eval/samples_per_second</td><td>20.644</td></tr><tr><td>eval/steps_per_second</td><td>1.445</td></tr><tr><td>train/epoch</td><td>20.0</td></tr><tr><td>train/global_step</td><td>2000</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.0002</td></tr><tr><td>train/total_flos</td><td>5.98099976928e+16</td></tr><tr><td>train/train_loss</td><td>0.19665</td></tr><tr><td>train/train_runtime</td><td>963.5325</td></tr><tr><td>train/train_samples_per_second</td><td>16.585</td></tr><tr><td>train/train_steps_per_second</td><td>2.076</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">whisper-fnt-c256_256-d0-20230223-221234</strong> at: <a href='https://wandb.ai/astockman/music-classification-aii/runs/03q7nf4v' target=\"_blank\">https://wandb.ai/astockman/music-classification-aii/runs/03q7nf4v</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230223_221235-03q7nf4v/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to res/models/whisper-fnt-c256_256-d0-20230223-221234\n",
      "Configuration saved in res/models/whisper-fnt-c256_256-d0-20230223-221234/config.json\n",
      "Model weights saved in res/models/whisper-fnt-c256_256-d0-20230223-221234/pytorch_model.bin\n",
      "Feature extractor saved in res/models/whisper-fnt-c256_256-d0-20230223-221234/preprocessor_config.json\n"
     ]
    }
   ],
   "source": [
    "run_name = get_run_name(TRAINING_CONFIG)\n",
    "model = get_model(TRAINING_CONFIG, prepared_ds[\"train\"])\n",
    "\n",
    "trainer = get_trainer(\n",
    "    run_name=run_name,\n",
    "    model=model,\n",
    "    train_ds=prepared_ds[\"train\"],\n",
    "    eval_ds=prepared_ds[\"valid\"],\n",
    "    training_config=TRAINING_CONFIG,\n",
    "    output_dir=\"out\",\n",
    "    debug=False,\n",
    "    env=NOTEBOOK_ENV,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "end_training(run_name, trainer, MODELS_DIR_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_CONFIG = {\n",
    "    \"epochs\": 20,\n",
    "    \"learning_rate\": 5e-5,\n",
    "    \"warmup\": 0.0,\n",
    "    \"train_batch_size\": 8,\n",
    "    \"eval_batch_size\": 16,\n",
    "    \"feature_encoder\": \"whisper\",\n",
    "    \"freeze_encoder\": False,\n",
    "    \"classifier_layers\": [256], \n",
    "    \"classifier_dropout\": 0.0,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Genre Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading res/samples_clustered_genre6.csv\n",
      "16932 examples in DataFrame\n",
      "split\n",
      "train    13545\n",
      "test      1694\n",
      "valid     1693\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>duration</th>\n",
       "      <th>id</th>\n",
       "      <th>genre</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>01 Hip Hop/Abandoned Brass Stabs.mp3</td>\n",
       "      <td>7.262041</td>\n",
       "      <td>01_Hip_Hop_Abandoned_Brass_Stabs</td>\n",
       "      <td>Hip Hop/RnB</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>01 Hip Hop/Against Time Keys.mp3</td>\n",
       "      <td>6.948571</td>\n",
       "      <td>01_Hip_Hop_Against_Time_Keys</td>\n",
       "      <td>Hip Hop/RnB</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>01 Hip Hop/Against Time Piano.mp3</td>\n",
       "      <td>6.948571</td>\n",
       "      <td>01_Hip_Hop_Against_Time_Piano</td>\n",
       "      <td>Hip Hop/RnB</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>01 Hip Hop/Against Time Sax Sample.mp3</td>\n",
       "      <td>6.948571</td>\n",
       "      <td>01_Hip_Hop_Against_Time_Sax_Sample</td>\n",
       "      <td>Hip Hop/RnB</td>\n",
       "      <td>valid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>01 Hip Hop/Against Time Staccato Strings.mp3</td>\n",
       "      <td>6.948571</td>\n",
       "      <td>01_Hip_Hop_Against_Time_Staccato_Strings</td>\n",
       "      <td>Hip Hop/RnB</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           path  duration  \\\n",
       "0          01 Hip Hop/Abandoned Brass Stabs.mp3  7.262041   \n",
       "1              01 Hip Hop/Against Time Keys.mp3  6.948571   \n",
       "2             01 Hip Hop/Against Time Piano.mp3  6.948571   \n",
       "3        01 Hip Hop/Against Time Sax Sample.mp3  6.948571   \n",
       "4  01 Hip Hop/Against Time Staccato Strings.mp3  6.948571   \n",
       "\n",
       "                                         id        genre  split  \n",
       "0          01_Hip_Hop_Abandoned_Brass_Stabs  Hip Hop/RnB   test  \n",
       "1              01_Hip_Hop_Against_Time_Keys  Hip Hop/RnB  train  \n",
       "2             01_Hip_Hop_Against_Time_Piano  Hip Hop/RnB  train  \n",
       "3        01_Hip_Hop_Against_Time_Sax_Sample  Hip Hop/RnB  valid  \n",
       "4  01_Hip_Hop_Against_Time_Staccato_Strings  Hip Hop/RnB  train  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build the filename indicating the subset of the whole dataset with the specific configurations\n",
    "df = create_or_load_df(FEATURES_CONFIG_GEN)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/alesssandros/dev/FCN_Newspaper/aii/res/datasets/ds-whisper-full-encoded/cache-6571d4b477ed53ce.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing extra columns from dataset\n",
      "Mapping features clusters\n",
      "Extracting test split\n",
      "Extracting train split\n",
      "Extracting valid split\n",
      "Create `ClassLabels` for target classes\n",
      "{'genre': ClassLabel(names=['Electronic', 'Hip Hop/RnB', 'House', 'Orchestral', 'Rock/Blues', 'World/Ethnic'], id=None)}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29ce5bd3d17f4c8e88ffeec9ea06a4ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting the dataset:   0%|          | 0/1694 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20bac56a21ae42e1bb825b651f873f1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting the dataset:   0%|          | 0/13545 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5748b445277442cfb37a3cb74f32c43e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting the dataset:   0%|          | 0/1693 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    test: Dataset({\n",
       "        features: ['label', 'id', 'duration', 'input_features'],\n",
       "        num_rows: 1694\n",
       "    })\n",
       "    train: Dataset({\n",
       "        features: ['label', 'id', 'duration', 'input_features'],\n",
       "        num_rows: 13545\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['label', 'id', 'duration', 'input_features'],\n",
       "        num_rows: 1693\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prepared_ds = load_and_prepare_ds(TRAINING_CONFIG, FEATURES_CONFIG_GEN, df)\n",
    "\n",
    "prepared_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/alesssandros/.cache/huggingface/hub/models--openai--whisper-tiny/snapshots/302560528ac75a251232980ebcc68bad9668f664/config.json\n",
      "Model config WhisperConfig {\n",
      "  \"_name_or_path\": \"openai/whisper-tiny\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"WhisperForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"begin_suppress_tokens\": [\n",
      "    220,\n",
      "    50257\n",
      "  ],\n",
      "  \"bos_token_id\": 50257,\n",
      "  \"d_model\": 384,\n",
      "  \"decoder_attention_heads\": 6,\n",
      "  \"decoder_ffn_dim\": 1536,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 4,\n",
      "  \"decoder_start_token_id\": 50258,\n",
      "  \"dropout\": 0.0,\n",
      "  \"encoder_attention_heads\": 6,\n",
      "  \"encoder_ffn_dim\": 1536,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 4,\n",
      "  \"eos_token_id\": 50257,\n",
      "  \"forced_decoder_ids\": [\n",
      "    [\n",
      "      1,\n",
      "      50259\n",
      "    ],\n",
      "    [\n",
      "      2,\n",
      "      50359\n",
      "    ],\n",
      "    [\n",
      "      3,\n",
      "      50363\n",
      "    ]\n",
      "  ],\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"max_length\": 448,\n",
      "  \"max_source_positions\": 1500,\n",
      "  \"max_target_positions\": 448,\n",
      "  \"model_type\": \"whisper\",\n",
      "  \"num_hidden_layers\": 4,\n",
      "  \"num_mel_bins\": 80,\n",
      "  \"pad_token_id\": 50257,\n",
      "  \"scale_embedding\": false,\n",
      "  \"suppress_tokens\": [\n",
      "    1,\n",
      "    2,\n",
      "    7,\n",
      "    8,\n",
      "    9,\n",
      "    10,\n",
      "    14,\n",
      "    25,\n",
      "    26,\n",
      "    27,\n",
      "    28,\n",
      "    29,\n",
      "    31,\n",
      "    58,\n",
      "    59,\n",
      "    60,\n",
      "    61,\n",
      "    62,\n",
      "    63,\n",
      "    90,\n",
      "    91,\n",
      "    92,\n",
      "    93,\n",
      "    359,\n",
      "    503,\n",
      "    522,\n",
      "    542,\n",
      "    873,\n",
      "    893,\n",
      "    902,\n",
      "    918,\n",
      "    922,\n",
      "    931,\n",
      "    1350,\n",
      "    1853,\n",
      "    1982,\n",
      "    2460,\n",
      "    2627,\n",
      "    3246,\n",
      "    3253,\n",
      "    3268,\n",
      "    3536,\n",
      "    3846,\n",
      "    3961,\n",
      "    4183,\n",
      "    4667,\n",
      "    6585,\n",
      "    6647,\n",
      "    7273,\n",
      "    9061,\n",
      "    9383,\n",
      "    10428,\n",
      "    10929,\n",
      "    11938,\n",
      "    12033,\n",
      "    12331,\n",
      "    12562,\n",
      "    13793,\n",
      "    14157,\n",
      "    14635,\n",
      "    15265,\n",
      "    15618,\n",
      "    16553,\n",
      "    16604,\n",
      "    18362,\n",
      "    18956,\n",
      "    20075,\n",
      "    21675,\n",
      "    22520,\n",
      "    26130,\n",
      "    26161,\n",
      "    26435,\n",
      "    28279,\n",
      "    29464,\n",
      "    31650,\n",
      "    32302,\n",
      "    32470,\n",
      "    36865,\n",
      "    42863,\n",
      "    47425,\n",
      "    49870,\n",
      "    50254,\n",
      "    50258,\n",
      "    50360,\n",
      "    50361,\n",
      "    50362\n",
      "  ],\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 51865\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/alesssandros/.cache/huggingface/hub/models--openai--whisper-tiny/snapshots/302560528ac75a251232980ebcc68bad9668f664/pytorch_model.bin\n",
      "Some weights of the model checkpoint at openai/whisper-tiny were not used when initializing WhisperForSequenceClassification: ['model.decoder.layers.3.fc2.weight', 'model.decoder.layers.3.self_attn.out_proj.weight', 'model.decoder.layers.0.fc1.bias', 'model.decoder.embed_positions.weight', 'model.decoder.layers.0.self_attn.q_proj.weight', 'model.decoder.layers.0.self_attn.out_proj.weight', 'model.decoder.layers.2.encoder_attn_layer_norm.weight', 'model.decoder.layers.1.self_attn.v_proj.weight', 'model.decoder.layers.2.self_attn_layer_norm.bias', 'model.decoder.layers.0.encoder_attn_layer_norm.bias', 'model.decoder.layers.2.self_attn.out_proj.weight', 'model.decoder.layers.1.self_attn.out_proj.bias', 'model.decoder.layers.1.encoder_attn_layer_norm.weight', 'model.decoder.layers.2.encoder_attn.v_proj.bias', 'model.decoder.layers.0.self_attn_layer_norm.weight', 'model.decoder.layers.3.final_layer_norm.weight', 'model.decoder.layers.3.fc1.weight', 'model.decoder.layers.0.self_attn.v_proj.weight', 'model.decoder.layers.0.encoder_attn.out_proj.bias', 'model.decoder.layers.3.encoder_attn.k_proj.weight', 'model.decoder.layers.3.encoder_attn_layer_norm.weight', 'model.decoder.layers.2.fc2.weight', 'model.decoder.layers.0.final_layer_norm.bias', 'model.decoder.layers.3.self_attn_layer_norm.weight', 'model.decoder.layers.2.fc2.bias', 'model.decoder.layers.3.encoder_attn.v_proj.bias', 'model.decoder.layers.1.final_layer_norm.weight', 'model.decoder.layers.2.self_attn.v_proj.bias', 'model.decoder.layers.0.encoder_attn.out_proj.weight', 'model.decoder.layers.0.encoder_attn.v_proj.bias', 'model.decoder.layers.1.encoder_attn.q_proj.weight', 'model.decoder.layers.1.encoder_attn_layer_norm.bias', 'model.decoder.layers.2.self_attn.v_proj.weight', 'model.decoder.layers.1.encoder_attn.out_proj.bias', 'model.decoder.layers.0.fc1.weight', 'model.decoder.layers.3.self_attn_layer_norm.bias', 'model.decoder.layers.1.self_attn_layer_norm.bias', 'model.decoder.layers.2.self_attn.k_proj.weight', 'model.decoder.layers.2.self_attn.q_proj.bias', 'model.decoder.layers.2.final_layer_norm.weight', 'model.decoder.layers.0.self_attn.out_proj.bias', 'model.decoder.layers.0.encoder_attn.v_proj.weight', 'model.decoder.layers.1.final_layer_norm.bias', 'model.decoder.layers.1.fc2.bias', 'model.decoder.layer_norm.weight', 'model.decoder.layers.2.encoder_attn_layer_norm.bias', 'model.decoder.layers.2.encoder_attn.out_proj.weight', 'model.decoder.layers.0.self_attn.k_proj.weight', 'model.decoder.layers.2.encoder_attn.q_proj.bias', 'model.decoder.layers.0.fc2.bias', 'model.decoder.layers.1.fc2.weight', 'model.decoder.layers.1.fc1.weight', 'model.decoder.layers.3.self_attn.v_proj.bias', 'model.decoder.layers.3.self_attn.k_proj.weight', 'model.decoder.layers.3.encoder_attn.q_proj.weight', 'model.decoder.layers.0.fc2.weight', 'model.decoder.layers.0.encoder_attn.q_proj.bias', 'model.decoder.layers.2.fc1.weight', 'model.decoder.layers.1.encoder_attn.q_proj.bias', 'model.decoder.layers.1.self_attn_layer_norm.weight', 'model.decoder.layers.3.fc1.bias', 'model.decoder.layers.2.self_attn_layer_norm.weight', 'model.decoder.layers.3.self_attn.q_proj.bias', 'model.decoder.layers.2.fc1.bias', 'model.decoder.layers.2.encoder_attn.v_proj.weight', 'model.decoder.embed_tokens.weight', 'model.decoder.layers.0.encoder_attn.q_proj.weight', 'model.decoder.layers.1.self_attn.q_proj.bias', 'model.decoder.layers.1.self_attn.q_proj.weight', 'model.decoder.layers.1.encoder_attn.v_proj.bias', 'model.decoder.layers.1.self_attn.v_proj.bias', 'model.decoder.layers.0.self_attn.v_proj.bias', 'model.decoder.layers.3.self_attn.out_proj.bias', 'model.decoder.layers.3.self_attn.q_proj.weight', 'model.decoder.layers.3.encoder_attn.v_proj.weight', 'model.decoder.layers.1.encoder_attn.v_proj.weight', 'model.decoder.layers.3.encoder_attn_layer_norm.bias', 'model.decoder.layers.3.final_layer_norm.bias', 'model.decoder.layers.1.fc1.bias', 'model.decoder.layers.3.fc2.bias', 'model.decoder.layers.1.self_attn.k_proj.weight', 'model.decoder.layers.3.self_attn.v_proj.weight', 'model.decoder.layers.1.self_attn.out_proj.weight', 'model.decoder.layers.0.self_attn_layer_norm.bias', 'model.decoder.layers.2.encoder_attn.out_proj.bias', 'model.decoder.layers.0.encoder_attn_layer_norm.weight', 'model.decoder.layers.0.self_attn.q_proj.bias', 'model.decoder.layers.1.encoder_attn.out_proj.weight', 'model.decoder.layers.2.encoder_attn.k_proj.weight', 'model.decoder.layers.2.self_attn.out_proj.bias', 'model.decoder.layers.3.encoder_attn.q_proj.bias', 'model.decoder.layers.2.self_attn.q_proj.weight', 'model.decoder.layers.3.encoder_attn.out_proj.bias', 'model.decoder.layer_norm.bias', 'model.decoder.layers.2.final_layer_norm.bias', 'model.decoder.layers.2.encoder_attn.q_proj.weight', 'model.decoder.layers.0.final_layer_norm.weight', 'model.decoder.layers.3.encoder_attn.out_proj.weight', 'model.decoder.layers.1.encoder_attn.k_proj.weight', 'model.decoder.layers.0.encoder_attn.k_proj.weight']\n",
      "- This IS expected if you are initializing WhisperForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing WhisperForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of WhisperForSequenceClassification were not initialized from the model checkpoint at openai/whisper-tiny and are newly initialized: ['model.head.layers.0.bias', 'model.classifier.weight', 'model.head.layers.0.weight', 'model.classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/alesssandros/dev/FCN_Newspaper/aii/wandb/run-20230223_223044-xoipj75m</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/astockman/music-classification-aii/runs/xoipj75m' target=\"_blank\">whisper-fnt-c256-d0-20230223-223043</a></strong> to <a href='https://wandb.ai/astockman/music-classification-aii' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/astockman/music-classification-aii' target=\"_blank\">https://wandb.ai/astockman/music-classification-aii</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/astockman/music-classification-aii/runs/xoipj75m' target=\"_blank\">https://wandb.ai/astockman/music-classification-aii/runs/xoipj75m</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The following columns in the training set don't have a corresponding argument in `WhisperForSequenceClassification.forward` and have been ignored: duration, id. If duration, id are not expected by `WhisperForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/home/alesssandros/.local/share/virtualenvs/aii-EtAbm6lw/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 13545\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 33880\n",
      "  Number of trainable parameters = 8308486\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1492' max='33880' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 1492/33880 09:05 < 3:17:28, 2.73 it/s, Epoch 0.88/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 15\u001b[0m\n\u001b[1;32m      2\u001b[0m model \u001b[38;5;241m=\u001b[39m get_model(TRAINING_CONFIG, prepared_ds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m      4\u001b[0m trainer \u001b[38;5;241m=\u001b[39m get_trainer(\n\u001b[1;32m      5\u001b[0m     run_name\u001b[38;5;241m=\u001b[39mrun_name,\n\u001b[1;32m      6\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m     env\u001b[38;5;241m=\u001b[39mNOTEBOOK_ENV,\n\u001b[1;32m     13\u001b[0m )\n\u001b[0;32m---> 15\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m end_training(run_name, trainer, MODELS_DIR_PATH)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/aii-EtAbm6lw/lib/python3.10/site-packages/transformers/trainer.py:1543\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/alesssandros/.local/share/virtualenvs/aii-EtAbm6lw/lib/python3.10/site-packages/transformers/trainer.py?line=1537'>1538</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_wrapped \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\n\u001b[1;32m   <a href='file:///home/alesssandros/.local/share/virtualenvs/aii-EtAbm6lw/lib/python3.10/site-packages/transformers/trainer.py?line=1539'>1540</a>\u001b[0m inner_training_loop \u001b[39m=\u001b[39m find_executable_batch_size(\n\u001b[1;32m   <a href='file:///home/alesssandros/.local/share/virtualenvs/aii-EtAbm6lw/lib/python3.10/site-packages/transformers/trainer.py?line=1540'>1541</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inner_training_loop, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_batch_size, args\u001b[39m.\u001b[39mauto_find_batch_size\n\u001b[1;32m   <a href='file:///home/alesssandros/.local/share/virtualenvs/aii-EtAbm6lw/lib/python3.10/site-packages/transformers/trainer.py?line=1541'>1542</a>\u001b[0m )\n\u001b[0;32m-> <a href='file:///home/alesssandros/.local/share/virtualenvs/aii-EtAbm6lw/lib/python3.10/site-packages/transformers/trainer.py?line=1542'>1543</a>\u001b[0m \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   <a href='file:///home/alesssandros/.local/share/virtualenvs/aii-EtAbm6lw/lib/python3.10/site-packages/transformers/trainer.py?line=1543'>1544</a>\u001b[0m     args\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m   <a href='file:///home/alesssandros/.local/share/virtualenvs/aii-EtAbm6lw/lib/python3.10/site-packages/transformers/trainer.py?line=1544'>1545</a>\u001b[0m     resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[1;32m   <a href='file:///home/alesssandros/.local/share/virtualenvs/aii-EtAbm6lw/lib/python3.10/site-packages/transformers/trainer.py?line=1545'>1546</a>\u001b[0m     trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[1;32m   <a href='file:///home/alesssandros/.local/share/virtualenvs/aii-EtAbm6lw/lib/python3.10/site-packages/transformers/trainer.py?line=1546'>1547</a>\u001b[0m     ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[1;32m   <a href='file:///home/alesssandros/.local/share/virtualenvs/aii-EtAbm6lw/lib/python3.10/site-packages/transformers/trainer.py?line=1547'>1548</a>\u001b[0m )\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/aii-EtAbm6lw/lib/python3.10/site-packages/transformers/trainer.py:1765\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   <a href='file:///home/alesssandros/.local/share/virtualenvs/aii-EtAbm6lw/lib/python3.10/site-packages/transformers/trainer.py?line=1761'>1762</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_load_rng_state(resume_from_checkpoint)\n\u001b[1;32m   <a href='file:///home/alesssandros/.local/share/virtualenvs/aii-EtAbm6lw/lib/python3.10/site-packages/transformers/trainer.py?line=1763'>1764</a>\u001b[0m step \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m\n\u001b[0;32m-> <a href='file:///home/alesssandros/.local/share/virtualenvs/aii-EtAbm6lw/lib/python3.10/site-packages/transformers/trainer.py?line=1764'>1765</a>\u001b[0m \u001b[39mfor\u001b[39;00m step, inputs \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(epoch_iterator):\n\u001b[1;32m   <a href='file:///home/alesssandros/.local/share/virtualenvs/aii-EtAbm6lw/lib/python3.10/site-packages/transformers/trainer.py?line=1765'>1766</a>\u001b[0m \n\u001b[1;32m   <a href='file:///home/alesssandros/.local/share/virtualenvs/aii-EtAbm6lw/lib/python3.10/site-packages/transformers/trainer.py?line=1766'>1767</a>\u001b[0m     \u001b[39m# Skip past any already trained steps if resuming training\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/alesssandros/.local/share/virtualenvs/aii-EtAbm6lw/lib/python3.10/site-packages/transformers/trainer.py?line=1767'>1768</a>\u001b[0m     \u001b[39mif\u001b[39;00m steps_trained_in_current_epoch \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   <a href='file:///home/alesssandros/.local/share/virtualenvs/aii-EtAbm6lw/lib/python3.10/site-packages/transformers/trainer.py?line=1768'>1769</a>\u001b[0m         steps_trained_in_current_epoch \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/aii-EtAbm6lw/lib/python3.10/site-packages/torch/utils/data/dataloader.py:628\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    <a href='file:///home/alesssandros/.local/share/virtualenvs/aii-EtAbm6lw/lib/python3.10/site-packages/torch/utils/data/dataloader.py?line=624'>625</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    <a href='file:///home/alesssandros/.local/share/virtualenvs/aii-EtAbm6lw/lib/python3.10/site-packages/torch/utils/data/dataloader.py?line=625'>626</a>\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/alesssandros/.local/share/virtualenvs/aii-EtAbm6lw/lib/python3.10/site-packages/torch/utils/data/dataloader.py?line=626'>627</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> <a href='file:///home/alesssandros/.local/share/virtualenvs/aii-EtAbm6lw/lib/python3.10/site-packages/torch/utils/data/dataloader.py?line=627'>628</a>\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    <a href='file:///home/alesssandros/.local/share/virtualenvs/aii-EtAbm6lw/lib/python3.10/site-packages/torch/utils/data/dataloader.py?line=628'>629</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    <a href='file:///home/alesssandros/.local/share/virtualenvs/aii-EtAbm6lw/lib/python3.10/site-packages/torch/utils/data/dataloader.py?line=629'>630</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    <a href='file:///home/alesssandros/.local/share/virtualenvs/aii-EtAbm6lw/lib/python3.10/site-packages/torch/utils/data/dataloader.py?line=630'>631</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    <a href='file:///home/alesssandros/.local/share/virtualenvs/aii-EtAbm6lw/lib/python3.10/site-packages/torch/utils/data/dataloader.py?line=631'>632</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/aii-EtAbm6lw/lib/python3.10/site-packages/torch/utils/data/dataloader.py:671\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    <a href='file:///home/alesssandros/.local/share/virtualenvs/aii-EtAbm6lw/lib/python3.10/site-packages/torch/utils/data/dataloader.py?line=668'>669</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    <a href='file:///home/alesssandros/.local/share/virtualenvs/aii-EtAbm6lw/lib/python3.10/site-packages/torch/utils/data/dataloader.py?line=669'>670</a>\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> <a href='file:///home/alesssandros/.local/share/virtualenvs/aii-EtAbm6lw/lib/python3.10/site-packages/torch/utils/data/dataloader.py?line=670'>671</a>\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/alesssandros/.local/share/virtualenvs/aii-EtAbm6lw/lib/python3.10/site-packages/torch/utils/data/dataloader.py?line=671'>672</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    <a href='file:///home/alesssandros/.local/share/virtualenvs/aii-EtAbm6lw/lib/python3.10/site-packages/torch/utils/data/dataloader.py?line=672'>673</a>\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/aii-EtAbm6lw/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:56\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     <a href='file:///home/alesssandros/.local/share/virtualenvs/aii-EtAbm6lw/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py?line=53'>54</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_collation:\n\u001b[1;32m     <a href='file:///home/alesssandros/.local/share/virtualenvs/aii-EtAbm6lw/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py?line=54'>55</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset, \u001b[39m\"\u001b[39m\u001b[39m__getitems__\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__:\n\u001b[0;32m---> <a href='file:///home/alesssandros/.local/share/virtualenvs/aii-EtAbm6lw/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py?line=55'>56</a>\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset\u001b[39m.\u001b[39;49m__getitems__(possibly_batched_index)\n\u001b[1;32m     <a href='file:///home/alesssandros/.local/share/virtualenvs/aii-EtAbm6lw/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py?line=56'>57</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     <a href='file:///home/alesssandros/.local/share/virtualenvs/aii-EtAbm6lw/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py?line=57'>58</a>\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/aii-EtAbm6lw/lib/python3.10/site-packages/datasets/arrow_dataset.py:2662\u001b[0m, in \u001b[0;36mDataset.__getitems__\u001b[0;34m(self, keys)\u001b[0m\n\u001b[1;32m   <a href='file:///home/alesssandros/.local/share/virtualenvs/aii-EtAbm6lw/lib/python3.10/site-packages/datasets/arrow_dataset.py?line=2659'>2660</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitems__\u001b[39m(\u001b[39mself\u001b[39m, keys: List) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List:\n\u001b[1;32m   <a href='file:///home/alesssandros/.local/share/virtualenvs/aii-EtAbm6lw/lib/python3.10/site-packages/datasets/arrow_dataset.py?line=2660'>2661</a>\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Can be used to get a batch using a list of integers indices.\"\"\"\u001b[39;00m\n\u001b[0;32m-> <a href='file:///home/alesssandros/.local/share/virtualenvs/aii-EtAbm6lw/lib/python3.10/site-packages/datasets/arrow_dataset.py?line=2661'>2662</a>\u001b[0m     batch \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m__getitem__\u001b[39;49m(keys)\n\u001b[1;32m   <a href='file:///home/alesssandros/.local/share/virtualenvs/aii-EtAbm6lw/lib/python3.10/site-packages/datasets/arrow_dataset.py?line=2662'>2663</a>\u001b[0m     n_examples \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(batch[\u001b[39mnext\u001b[39m(\u001b[39miter\u001b[39m(batch))])\n\u001b[1;32m   <a href='file:///home/alesssandros/.local/share/virtualenvs/aii-EtAbm6lw/lib/python3.10/site-packages/datasets/arrow_dataset.py?line=2663'>2664</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m [{col: array[i] \u001b[39mfor\u001b[39;00m col, array \u001b[39min\u001b[39;00m batch\u001b[39m.\u001b[39mitems()} \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_examples)]\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/aii-EtAbm6lw/lib/python3.10/site-packages/datasets/arrow_dataset.py:2658\u001b[0m, in \u001b[0;36mDataset.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   <a href='file:///home/alesssandros/.local/share/virtualenvs/aii-EtAbm6lw/lib/python3.10/site-packages/datasets/arrow_dataset.py?line=2655'>2656</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, key):  \u001b[39m# noqa: F811\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/alesssandros/.local/share/virtualenvs/aii-EtAbm6lw/lib/python3.10/site-packages/datasets/arrow_dataset.py?line=2656'>2657</a>\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Can be used to index columns (by string names) or rows (by integer index or iterable of indices or bools).\"\"\"\u001b[39;00m\n\u001b[0;32m-> <a href='file:///home/alesssandros/.local/share/virtualenvs/aii-EtAbm6lw/lib/python3.10/site-packages/datasets/arrow_dataset.py?line=2657'>2658</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_getitem(key)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/aii-EtAbm6lw/lib/python3.10/site-packages/datasets/arrow_dataset.py:2643\u001b[0m, in \u001b[0;36mDataset._getitem\u001b[0;34m(self, key, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/alesssandros/.local/share/virtualenvs/aii-EtAbm6lw/lib/python3.10/site-packages/datasets/arrow_dataset.py?line=2640'>2641</a>\u001b[0m formatter \u001b[39m=\u001b[39m get_formatter(format_type, features\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeatures, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mformat_kwargs)\n\u001b[1;32m   <a href='file:///home/alesssandros/.local/share/virtualenvs/aii-EtAbm6lw/lib/python3.10/site-packages/datasets/arrow_dataset.py?line=2641'>2642</a>\u001b[0m pa_subtable \u001b[39m=\u001b[39m query_table(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data, key, indices\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_indices \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_indices \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m-> <a href='file:///home/alesssandros/.local/share/virtualenvs/aii-EtAbm6lw/lib/python3.10/site-packages/datasets/arrow_dataset.py?line=2642'>2643</a>\u001b[0m formatted_output \u001b[39m=\u001b[39m format_table(\n\u001b[1;32m   <a href='file:///home/alesssandros/.local/share/virtualenvs/aii-EtAbm6lw/lib/python3.10/site-packages/datasets/arrow_dataset.py?line=2643'>2644</a>\u001b[0m     pa_subtable, key, formatter\u001b[39m=\u001b[39;49mformatter, format_columns\u001b[39m=\u001b[39;49mformat_columns, output_all_columns\u001b[39m=\u001b[39;49moutput_all_columns\n\u001b[1;32m   <a href='file:///home/alesssandros/.local/share/virtualenvs/aii-EtAbm6lw/lib/python3.10/site-packages/datasets/arrow_dataset.py?line=2644'>2645</a>\u001b[0m )\n\u001b[1;32m   <a href='file:///home/alesssandros/.local/share/virtualenvs/aii-EtAbm6lw/lib/python3.10/site-packages/datasets/arrow_dataset.py?line=2645'>2646</a>\u001b[0m \u001b[39mreturn\u001b[39;00m formatted_output\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/aii-EtAbm6lw/lib/python3.10/site-packages/datasets/formatting/formatting.py:642\u001b[0m, in \u001b[0;36mformat_table\u001b[0;34m(table, key, formatter, format_columns, output_all_columns)\u001b[0m\n\u001b[1;32m    <a href='file:///home/alesssandros/.local/share/virtualenvs/aii-EtAbm6lw/lib/python3.10/site-packages/datasets/formatting/formatting.py?line=639'>640</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    <a href='file:///home/alesssandros/.local/share/virtualenvs/aii-EtAbm6lw/lib/python3.10/site-packages/datasets/formatting/formatting.py?line=640'>641</a>\u001b[0m     pa_table_to_format \u001b[39m=\u001b[39m pa_table\u001b[39m.\u001b[39mdrop(col \u001b[39mfor\u001b[39;00m col \u001b[39min\u001b[39;00m pa_table\u001b[39m.\u001b[39mcolumn_names \u001b[39mif\u001b[39;00m col \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m format_columns)\n\u001b[0;32m--> <a href='file:///home/alesssandros/.local/share/virtualenvs/aii-EtAbm6lw/lib/python3.10/site-packages/datasets/formatting/formatting.py?line=641'>642</a>\u001b[0m     formatted_output \u001b[39m=\u001b[39m formatter(pa_table_to_format, query_type\u001b[39m=\u001b[39;49mquery_type)\n\u001b[1;32m    <a href='file:///home/alesssandros/.local/share/virtualenvs/aii-EtAbm6lw/lib/python3.10/site-packages/datasets/formatting/formatting.py?line=642'>643</a>\u001b[0m     \u001b[39mif\u001b[39;00m output_all_columns:\n\u001b[1;32m    <a href='file:///home/alesssandros/.local/share/virtualenvs/aii-EtAbm6lw/lib/python3.10/site-packages/datasets/formatting/formatting.py?line=643'>644</a>\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(formatted_output, MutableMapping):\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/aii-EtAbm6lw/lib/python3.10/site-packages/datasets/formatting/formatting.py:410\u001b[0m, in \u001b[0;36mFormatter.__call__\u001b[0;34m(self, pa_table, query_type)\u001b[0m\n\u001b[1;32m    <a href='file:///home/alesssandros/.local/share/virtualenvs/aii-EtAbm6lw/lib/python3.10/site-packages/datasets/formatting/formatting.py?line=407'>408</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mformat_column(pa_table)\n\u001b[1;32m    <a href='file:///home/alesssandros/.local/share/virtualenvs/aii-EtAbm6lw/lib/python3.10/site-packages/datasets/formatting/formatting.py?line=408'>409</a>\u001b[0m \u001b[39melif\u001b[39;00m query_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mbatch\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> <a href='file:///home/alesssandros/.local/share/virtualenvs/aii-EtAbm6lw/lib/python3.10/site-packages/datasets/formatting/formatting.py?line=409'>410</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mformat_batch(pa_table)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/aii-EtAbm6lw/lib/python3.10/site-packages/datasets/formatting/formatting.py:453\u001b[0m, in \u001b[0;36mPythonFormatter.format_batch\u001b[0;34m(self, pa_table)\u001b[0m\n\u001b[1;32m    <a href='file:///home/alesssandros/.local/share/virtualenvs/aii-EtAbm6lw/lib/python3.10/site-packages/datasets/formatting/formatting.py?line=450'>451</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlazy:\n\u001b[1;32m    <a href='file:///home/alesssandros/.local/share/virtualenvs/aii-EtAbm6lw/lib/python3.10/site-packages/datasets/formatting/formatting.py?line=451'>452</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m LazyBatch(pa_table, \u001b[39mself\u001b[39m)\n\u001b[0;32m--> <a href='file:///home/alesssandros/.local/share/virtualenvs/aii-EtAbm6lw/lib/python3.10/site-packages/datasets/formatting/formatting.py?line=452'>453</a>\u001b[0m batch \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpython_arrow_extractor()\u001b[39m.\u001b[39;49mextract_batch(pa_table)\n\u001b[1;32m    <a href='file:///home/alesssandros/.local/share/virtualenvs/aii-EtAbm6lw/lib/python3.10/site-packages/datasets/formatting/formatting.py?line=453'>454</a>\u001b[0m batch \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpython_features_decoder\u001b[39m.\u001b[39mdecode_batch(batch)\n\u001b[1;32m    <a href='file:///home/alesssandros/.local/share/virtualenvs/aii-EtAbm6lw/lib/python3.10/site-packages/datasets/formatting/formatting.py?line=454'>455</a>\u001b[0m \u001b[39mreturn\u001b[39;00m batch\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/aii-EtAbm6lw/lib/python3.10/site-packages/datasets/formatting/formatting.py:150\u001b[0m, in \u001b[0;36mPythonArrowExtractor.extract_batch\u001b[0;34m(self, pa_table)\u001b[0m\n\u001b[1;32m    <a href='file:///home/alesssandros/.local/share/virtualenvs/aii-EtAbm6lw/lib/python3.10/site-packages/datasets/formatting/formatting.py?line=148'>149</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mextract_batch\u001b[39m(\u001b[39mself\u001b[39m, pa_table: pa\u001b[39m.\u001b[39mTable) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mdict\u001b[39m:\n\u001b[0;32m--> <a href='file:///home/alesssandros/.local/share/virtualenvs/aii-EtAbm6lw/lib/python3.10/site-packages/datasets/formatting/formatting.py?line=149'>150</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m pa_table\u001b[39m.\u001b[39;49mto_pydict()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "run_name = get_run_name(TRAINING_CONFIG)\n",
    "model = get_model(TRAINING_CONFIG, prepared_ds[\"train\"])\n",
    "\n",
    "trainer = get_trainer(\n",
    "    run_name=run_name,\n",
    "    model=model,\n",
    "    train_ds=prepared_ds[\"train\"],\n",
    "    eval_ds=prepared_ds[\"valid\"],\n",
    "    training_config=TRAINING_CONFIG,\n",
    "    output_dir=\"out\",\n",
    "    debug=False,\n",
    "    env=NOTEBOOK_ENV,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "end_training(run_name, trainer, MODELS_DIR_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Category Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the filename indicating the subset of the whole dataset with the specific configurations\n",
    "df = create_or_load_df(FEATURES_CONFIG_CAT)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepared_ds = load_and_prepare_ds(TRAINING_CONFIG, FEATURES_CONFIG_CAT, df)\n",
    "\n",
    "prepared_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_name = get_run_name(TRAINING_CONFIG)\n",
    "model = get_model(TRAINING_CONFIG, prepared_ds[\"train\"])\n",
    "\n",
    "trainer = get_trainer(\n",
    "    run_name=run_name,\n",
    "    model=model,\n",
    "    train_ds=prepared_ds[\"train\"],\n",
    "    eval_ds=prepared_ds[\"valid\"],\n",
    "    training_config=TRAINING_CONFIG,\n",
    "    output_dir=\"out\",\n",
    "    debug=False,\n",
    "    env=NOTEBOOK_ENV,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "end_training(run_name, trainer, MODELS_DIR_PATH)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "interpreter": {
   "hash": "20bdf70b7d345c6a48afd3cfa8e89df1057bd813a1bcca696776530906de370f"
  },
  "kernelspec": {
   "display_name": "Python 3.10.6 ('aii-EtAbm6lw')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0fee81913e764382bfd0946d1661be8d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_570e7fe98c364883a87014e220e5baf5",
      "placeholder": "​",
      "style": "IPY_MODEL_5d46f1f40ae24380a6b6d3304d1d1d36",
      "value": "Casting the dataset: 100%"
     }
    },
    "2febbdf5605149f494d9ac510883f262": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "35622cbe24684a7ea5382f1f536bd9ea": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_0fee81913e764382bfd0946d1661be8d",
       "IPY_MODEL_c9e2fff6b66e434e9806d869417f4eff",
       "IPY_MODEL_e3207490c4ba470f8e7cf61b97d84d54"
      ],
      "layout": "IPY_MODEL_2febbdf5605149f494d9ac510883f262"
     }
    },
    "4849127988f64e1e9558a88f220ad1de": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "570e7fe98c364883a87014e220e5baf5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5d46f1f40ae24380a6b6d3304d1d1d36": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "76877461d03e4018b8e5c5bc1721d2cb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "7df7757d71ea46d5a4616f431066aef1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a70ff0866d564c96ae19d5d6aa91a727": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c9e2fff6b66e434e9806d869417f4eff": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4849127988f64e1e9558a88f220ad1de",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_76877461d03e4018b8e5c5bc1721d2cb",
      "value": 1
     }
    },
    "e3207490c4ba470f8e7cf61b97d84d54": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a70ff0866d564c96ae19d5d6aa91a727",
      "placeholder": "​",
      "style": "IPY_MODEL_7df7757d71ea46d5a4616f431066aef1",
      "value": " 1/1 [00:12&lt;00:00, 12.42s/ba]"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
