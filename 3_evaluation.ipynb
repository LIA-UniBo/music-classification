{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import os\n",
    "\n",
    "import datasets\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from src.dataset import prepare_ds\n",
    "from src.train import get_evaluator, get_model\n",
    "from src.utils import get_csv_name, get_model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "RES_DIR_PATH = \"res\"\n",
    "NOTEBOOK_ENV = \"jupyter\"\n",
    "\n",
    "AUDIOS_DIR_PATH = os.path.join(RES_DIR_PATH, \"mp3_data\")\n",
    "MODELS_DIR_PATH = os.path.join(RES_DIR_PATH, \"models\")\n",
    "DATASETS_DIR_PATH = os.path.join(RES_DIR_PATH, \"datasets\")\n",
    "CSV_PATH = os.path.join(RES_DIR_PATH, \"samples.csv\")\n",
    "\n",
    "FEATURES_CONFIG = {\"genre\": {\"top_n\": 3, \"samples\": 1000}}\n",
    "\n",
    "VALID_SIZE = 0.1\n",
    "TEST_SIZE = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_CONFIG = {\n",
    "    \"epochs\": 20,\n",
    "    \"learning_rate\": 5e-5,\n",
    "    \"warmup\": 0.0,\n",
    "    \"train_batch_size\": 8,\n",
    "    \"eval_batch_size\": 16,\n",
    "    \"feature_encoder\": None,\n",
    "    \"freeze_encoder\": None,\n",
    "    \"classifier_layers\": None, \n",
    "    \"classifier_dropout\": 0.0,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>genre</th>\n",
       "      <th>id</th>\n",
       "      <th>duration</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Apple Loops for GarageBand/Spacey Electric Pia...</td>\n",
       "      <td>Electronic/Dance</td>\n",
       "      <td>Apple_Loops_for_GarageBand_Spacey_Electric_Pia...</td>\n",
       "      <td>8.097959</td>\n",
       "      <td>valid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Keyboard Collection/Comb The World Clav.mp3</td>\n",
       "      <td>Electronic/Dance</td>\n",
       "      <td>Keyboard_Collection_Comb_The_World_Clav</td>\n",
       "      <td>9.691429</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Jam Pack Remix Tools/Trance Dripper Beat 04.mp3</td>\n",
       "      <td>Electronic/Dance</td>\n",
       "      <td>Jam_Pack_Remix_Tools_Trance_Dripper_Beat_04</td>\n",
       "      <td>1.933061</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Jam Pack Remix Tools/Techno Spokes Beat 01.mp3</td>\n",
       "      <td>Electronic/Dance</td>\n",
       "      <td>Jam_Pack_Remix_Tools_Techno_Spokes_Beat_01</td>\n",
       "      <td>1.776327</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>25 Step Sequencer/Sneaky Cycles Bass.mp3</td>\n",
       "      <td>Electronic/Dance</td>\n",
       "      <td>25_Step_Sequencer_Sneaky_Cycles_Bass</td>\n",
       "      <td>7.497143</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                path             genre  \\\n",
       "0  Apple Loops for GarageBand/Spacey Electric Pia...  Electronic/Dance   \n",
       "1        Keyboard Collection/Comb The World Clav.mp3  Electronic/Dance   \n",
       "2    Jam Pack Remix Tools/Trance Dripper Beat 04.mp3  Electronic/Dance   \n",
       "3     Jam Pack Remix Tools/Techno Spokes Beat 01.mp3  Electronic/Dance   \n",
       "4           25 Step Sequencer/Sneaky Cycles Bass.mp3  Electronic/Dance   \n",
       "\n",
       "                                                  id  duration  split  \n",
       "0  Apple_Loops_for_GarageBand_Spacey_Electric_Pia...  8.097959  valid  \n",
       "1            Keyboard_Collection_Comb_The_World_Clav  9.691429  train  \n",
       "2        Jam_Pack_Remix_Tools_Trance_Dripper_Beat_04  1.933061  train  \n",
       "3         Jam_Pack_Remix_Tools_Techno_Spokes_Beat_01  1.776327  train  \n",
       "4               25_Step_Sequencer_Sneaky_Cycles_Bass  7.497143  train  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build the filename indicating the subset of the whole dataset with the specific configurations\n",
    "filtered_csv_path = get_csv_name(FEATURES_CONFIG, CSV_PATH)\n",
    "df = pd.read_csv(filtered_csv_path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function for loading the dataset for the requested model\n",
    "\n",
    "def load_and_prepare_ds(model, df):\n",
    "    encoded_dataset_path = os.path.join(DATASETS_DIR_PATH, f\"ds-{model}-full-encoded\")\n",
    "    ds = datasets.load_from_disk(encoded_dataset_path)\n",
    "    return prepare_ds(ds, df, FEATURES_CONFIG, fixed_mapping=None, save=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "combinations = {\n",
    "    \"feature_encoder\": [\"wav2vec2\", \"whisper\"],\n",
    "    \"freeze_encoder\": [True, False],\n",
    "    \"classifier_layers\": [[256], [256, 256]], \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/alessandro/Projects/Uni/AII/music-classification/res/datasets/ds-wav2vec2-full-encoded/cache-bca765b857065d3e.arrow\n",
      "Loading cached processed dataset at /home/alessandro/Projects/Uni/AII/music-classification/res/datasets/ds-wav2vec2-full-encoded/cache-2e979f1eba96989c.arrow\n",
      "Loading cached processed dataset at /home/alessandro/Projects/Uni/AII/music-classification/res/datasets/ds-wav2vec2-full-encoded/cache-126cb87b100a69fa.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing extra columns from dataset\n",
      "Extracting train split\n",
      "Extracting test split\n",
      "Extracting valid split\n",
      "Create `ClassLabels` for target classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/alessandro/Projects/Uni/AII/music-classification/res/datasets/ds-whisper-full-encoded/cache-38cb3a9232d39199.arrow\n",
      "Loading cached processed dataset at /home/alessandro/Projects/Uni/AII/music-classification/res/datasets/ds-whisper-full-encoded/cache-b67b100035545290.arrow\n",
      "Loading cached processed dataset at /home/alessandro/Projects/Uni/AII/music-classification/res/datasets/ds-whisper-full-encoded/cache-5c1d727b919b0b51.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing extra columns from dataset\n",
      "Extracting train split\n",
      "Extracting test split\n",
      "Extracting valid split\n",
      "Create `ClassLabels` for target classes\n",
      "Loading res/models/whisper-frz-c256-d0 weights\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/alessandro/.cache/huggingface/hub/models--openai--whisper-tiny/snapshots/ada5a5d516772e41f9aeb0f984df6ecc4620001f/config.json\n",
      "Model config WhisperConfig {\n",
      "  \"_name_or_path\": \"openai/whisper-tiny\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"WhisperForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"begin_suppress_tokens\": [\n",
      "    220,\n",
      "    50257\n",
      "  ],\n",
      "  \"bos_token_id\": 50257,\n",
      "  \"d_model\": 384,\n",
      "  \"decoder_attention_heads\": 6,\n",
      "  \"decoder_ffn_dim\": 1536,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 4,\n",
      "  \"decoder_start_token_id\": 50258,\n",
      "  \"dropout\": 0.0,\n",
      "  \"encoder_attention_heads\": 6,\n",
      "  \"encoder_ffn_dim\": 1536,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 4,\n",
      "  \"eos_token_id\": 50257,\n",
      "  \"forced_decoder_ids\": [\n",
      "    [\n",
      "      1,\n",
      "      50259\n",
      "    ],\n",
      "    [\n",
      "      2,\n",
      "      50359\n",
      "    ],\n",
      "    [\n",
      "      3,\n",
      "      50363\n",
      "    ]\n",
      "  ],\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"max_length\": 448,\n",
      "  \"max_source_positions\": 1500,\n",
      "  \"max_target_positions\": 448,\n",
      "  \"model_type\": \"whisper\",\n",
      "  \"num_hidden_layers\": 4,\n",
      "  \"num_mel_bins\": 80,\n",
      "  \"pad_token_id\": 50257,\n",
      "  \"scale_embedding\": false,\n",
      "  \"suppress_tokens\": [\n",
      "    1,\n",
      "    2,\n",
      "    7,\n",
      "    8,\n",
      "    9,\n",
      "    10,\n",
      "    14,\n",
      "    25,\n",
      "    26,\n",
      "    27,\n",
      "    28,\n",
      "    29,\n",
      "    31,\n",
      "    58,\n",
      "    59,\n",
      "    60,\n",
      "    61,\n",
      "    62,\n",
      "    63,\n",
      "    90,\n",
      "    91,\n",
      "    92,\n",
      "    93,\n",
      "    359,\n",
      "    503,\n",
      "    522,\n",
      "    542,\n",
      "    873,\n",
      "    893,\n",
      "    902,\n",
      "    918,\n",
      "    922,\n",
      "    931,\n",
      "    1350,\n",
      "    1853,\n",
      "    1982,\n",
      "    2460,\n",
      "    2627,\n",
      "    3246,\n",
      "    3253,\n",
      "    3268,\n",
      "    3536,\n",
      "    3846,\n",
      "    3961,\n",
      "    4183,\n",
      "    4667,\n",
      "    6585,\n",
      "    6647,\n",
      "    7273,\n",
      "    9061,\n",
      "    9383,\n",
      "    10428,\n",
      "    10929,\n",
      "    11938,\n",
      "    12033,\n",
      "    12331,\n",
      "    12562,\n",
      "    13793,\n",
      "    14157,\n",
      "    14635,\n",
      "    15265,\n",
      "    15618,\n",
      "    16553,\n",
      "    16604,\n",
      "    18362,\n",
      "    18956,\n",
      "    20075,\n",
      "    21675,\n",
      "    22520,\n",
      "    26130,\n",
      "    26161,\n",
      "    26435,\n",
      "    28279,\n",
      "    29464,\n",
      "    31650,\n",
      "    32302,\n",
      "    32470,\n",
      "    36865,\n",
      "    42863,\n",
      "    47425,\n",
      "    49870,\n",
      "    50254,\n",
      "    50258,\n",
      "    50360,\n",
      "    50361,\n",
      "    50362\n",
      "  ],\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 51865\n",
      "}\n",
      "\n",
      "loading weights file res/models/whisper-frz-c256-d0/pytorch_model.bin\n",
      "Some weights of the model checkpoint at res/models/whisper-frz-c256-d0 were not used when initializing WhisperForSequenceClassification: ['classifier.layernorm.bias', 'classifier.layernorm.weight', 'classifier.dense.weight', 'classifier.dense.bias']\n",
      "- This IS expected if you are initializing WhisperForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing WhisperForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of WhisperForSequenceClassification were not initialized from the model checkpoint at res/models/whisper-frz-c256-d0 and are newly initialized: ['head.layers.0.weight', 'classifier.weight', 'classifier.bias', 'head.layers.0.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the evaluation set don't have a corresponding argument in `WhisperForSequenceClassification.forward` and have been ignored: path, id, duration. If path, id, duration are not expected by `WhisperForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 8\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 00:12]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "The following columns in the evaluation set don't have a corresponding argument in `WhisperForSequenceClassification.forward` and have been ignored: path, id, duration. If path, id, duration are not expected by `WhisperForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 8\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set don't have a corresponding argument in `WhisperForSequenceClassification.forward` and have been ignored: path, id, duration. If path, id, duration are not expected by `WhisperForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 8\n",
      "  Batch size = 16\n"
     ]
    }
   ],
   "source": [
    "# Compute metrics for every network in every split of the dataset\n",
    "stats = {}\n",
    "ds_type = None\n",
    "prepared_ds = None\n",
    "\n",
    "for conf in itertools.product(*combinations.values()):\n",
    "    TRAINING_CONFIG[\"feature_encoder\"] = conf[0]\n",
    "    TRAINING_CONFIG[\"freeze_encoder\"] = conf[1]\n",
    "    TRAINING_CONFIG[\"classifier_layers\"] = conf[2]\n",
    "    model_name = get_model_name(TRAINING_CONFIG)\n",
    "    model_path = os.path.join(MODELS_DIR_PATH, model_name)\n",
    "    TRAINING_CONFIG[\"model_path\"] = model_path\n",
    "    \n",
    "    if ds_type != TRAINING_CONFIG[\"feature_encoder\"]:\n",
    "        ds_type = TRAINING_CONFIG[\"feature_encoder\"]\n",
    "        prepared_ds = load_and_prepare_ds(ds_type, df)\n",
    "    \n",
    "    if os.path.exists(model_path):\n",
    "        print(f\"Loading {model_path} weights\")\n",
    "        \n",
    "        stats[model_name] = {}\n",
    "        model = get_model(TRAINING_CONFIG, prepared_ds[\"train\"])\n",
    "        trainer = get_evaluator(\n",
    "            model=model,\n",
    "            training_config=TRAINING_CONFIG,\n",
    "        )\n",
    "\n",
    "        for split in [\"train\", \"valid\", \"test\"]:\n",
    "            outputs = trainer.evaluate(prepared_ds[split])\n",
    "            \n",
    "            preds_label = np.array([model.config.id2label[idx] for idx in outputs.label_ids])\n",
    "\n",
    "            stats[model_name][split] = {\n",
    "                \"loss\": outputs.metrics[\"eval_loss\"],\n",
    "                \"acc\": outputs.metrics[\"eval_accuracy\"],\n",
    "                \"preds_id\": outputs.label_ids,\n",
    "                \"preds_label\": preds_label,\n",
    "            }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrongly Classified Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    Electronic/Dance\n",
      "1        World/Ethnic\n",
      "Name: genre, dtype: object\n",
      "0        World/Ethnic\n",
      "1    Electronic/Dance\n",
      "2          Rock/Blues\n",
      "3    Electronic/Dance\n",
      "4          Rock/Blues\n",
      "Name: genre, dtype: object\n",
      "0          Rock/Blues\n",
      "1    Electronic/Dance\n",
      "2    Electronic/Dance\n",
      "3        World/Ethnic\n",
      "Name: genre, dtype: object\n"
     ]
    }
   ],
   "source": [
    "df_misclasses = []\n",
    "for model_name in stats.keys():\n",
    "    for split in stats[model_name].keys():\n",
    "        preds_label = stats[model_name][split][\"preds_label\"]\n",
    "        df_split = df[df[\"split\"] == split]\n",
    "        df_misclass = df_split[df_split[\"genre\"] != preds_label].reset_index(drop=True)\n",
    "        print(df_misclass[\"genre\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The conclusions drawn from the projects are the following:\n",
    "- Fine-tuning is key, probably due to\n",
    "- A more elaborated classifier head doesn't improve : the expresiveness of Transformers is fine by itself in \n",
    "- The Whisper architecture faster and more accurate\n",
    "\n",
    "Further analysis should regard more :\n",
    "- More hyperparameter tuning\n",
    "- By the ambiguous nature of genre of music\n",
    "- Heavier regularization\n",
    "- Data augmentation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aii",
   "language": "python",
   "name": "aii"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
